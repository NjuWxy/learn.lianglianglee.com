<!DOCTYPE html>
<!-- saved from url=(0046)https://kaiiiz.github.io/hexo-theme-book-demo/ -->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">
        <link rel="icon" href="/static/favicon.png">
        <title>CNCF X 阿里巴巴云原生技术公开课.html</title>
        <!-- Spectre.css framework -->
        <link rel="stylesheet" href="/static/index.css">
        <!-- theme css & js -->
        <meta name="generator" content="Hexo 4.2.0">
    </head>

<body>

<div class="book-container">

    <script>
        function add_inner() {
            let inner = document.querySelector('.sidebar-toggle-inner')
            inner.classList.add('show')
        }

        function remove_inner() {
            let inner = document.querySelector('.sidebar-toggle-inner')
            inner.classList.remove('show')
        }

        function sidebar_toggle() {
            let sidebar_toggle = document.querySelector('.sidebar-toggle')
            let sidebar = document.querySelector('.book-sidebar')
            let content = document.querySelector('.off-canvas-content')
            if (sidebar_toggle.classList.contains('extend')) { // show
                sidebar_toggle.classList.remove('extend')
                sidebar.classList.remove('hide')
                content.classList.remove('extend')
            } else { // hide
                sidebar_toggle.classList.add('extend')
                sidebar.classList.add('hide')
                content.classList.add('extend')
            }
        }


function open_sidebar() {
    let sidebar = document.querySelector('.book-sidebar')
    let overlay = document.querySelector('.off-canvas-overlay')
    sidebar.classList.add('show')
    overlay.classList.add('show')
}
function hide_canvas() {
    let sidebar = document.querySelector('.book-sidebar')
    let overlay = document.querySelector('.off-canvas-overlay')
    sidebar.classList.remove('show')
    overlay.classList.remove('show')
}

    </script>

    <div class="off-canvas-content">
        <div class="columns">
            <div class="column col-12 col-lg-12">
                <div class="book-navbar">
                    <!-- For Responsive Layout -->
                    <header class="navbar">
                        <section class="navbar-section">
                            <a onclick="open_sidebar()">
                                <i class="icon icon-menu"></i>
                            </a>
                        </section>
                    </header>
                </div>
                <div class="book-content" style="max-width: 960px; margin: 0 auto;
    overflow-x: auto;
    overflow-y: hidden;">
<div class="book-post">

                        <p id="tip" align="center"></p>

                        <div><h1>01 第一堂“云原生”课</h1>

<p><strong>本节课程要点</strong></p>

<ol>

<li>云原生技术发展历程（为什么要学习这门课）</li>

<li>课程简介与预备知识（这门课到底教什么）</li>

<li>云原生的定义与技术要点（本节正式内容）</li>

</ol>

<h2>为什么要开设云原生技术公开课？</h2>

<h3>云原生技术发展简史</h3>

<p>首先从第一个问题进行分享，那就是“为什么要开设云原生技术公开课？”云原生、CNCF 都是目前非常热门的关键词，但是这些技术并不是非常新鲜的内容。</p>

<ul>

<li>2004 年— 2007 年，Google 已在内部大规模地使用像 Cgroups 这样的容器技术；</li>

<li>2008 年，Google 将 Cgroups 合并进入了 Linux 内核主干；</li>

<li>2013 年，Docker 项目正式发布。</li>

<li>2014 年，Kubernetes 项目也正式发布。这样的原因也非常容易理解，因为有了容器和 Docker 之后，就需要有一种方式去帮助大家方便、快速、优雅地管理这些容器，这就是 Kubernetes 项目的初衷。在 Google 和 Redhat 发布了 Kubernetes 之后，这个项目的发展速度非常之快。</li>

<li>2015 年，由Google、Redhat 以及微软等大型云计算厂商以及一些开源公司共同牵头成立了 CNCF 云原生基金会。CNCF 成立之初，就有 22 个创始会员，而且 Kubernetes 也成为了 CNCF 托管的第一个开源项目。在这之后，CNCF 的发展速度非常迅猛；</li>

<li>2017 年，CNCF 达到 170 个成员和 14 个基金项目；</li>

<li>2018 年，CNCF 成立三周年有了 195 个成员，19 个基金会项目和 11 个孵化项目，如此之快的发展速度在整个云计算领域都是非常罕见的。</li>

</ul>

<h3>云原生技术生态现状</h3>

<p>因此，如今我们所讨论的云原生技术生态是一个庞大的技术集合。CNCF 有一张云原生全景图（<a href="https://github.com/cncf/landscape">https://github.com/cncf/landscape</a>），在这个全景图里已经有 200 多个项目和产品了，这些项目和产品也都是和 CNCF 的观点所契合的。所以如果以这张全景图作为背景，加以思考就会发现，我们今天所讨论的云原生其实主要谈论了以下几点：</p>

<ol>

<li>云原生基金会 —— CNCF；</li>

<li>云原生技术社区，比如像 CNCF 目前正式托管的 20 多个项目共同构成了现代云计算生态的基石，其中像 Kubernetes 这样的项目已经成为了世界第四活跃的开源项目；</li>

<li>除了前面两点之外，现在全球各大公有云厂商都已经支持了 Kubernetes。此外，还有 100 多家技术创业公司也在持续地进行投入。现在阿里巴巴也在谈全面上云，而且上云就要上云原生，这也是各大技术公司拥抱云原生的一个例子。</li>

</ol>

<h3>我们正处于时代的关键节点</h3>

<p>2019 年正是云原生时代的关键节点，为什么这么说？我们这里就为大家简单梳理一下。   从 2013 年 Docker 项目发布开始说起，Docker 项目的发布使得全操作系统语义的沙盒技术唾手可得，使得用户能够更好地、更完整地打包自己的应用，使得开发者可以轻而易举的获得了一个应用的最小可运行单位，而不需要依赖任何 PaaS 能力。这对经典 PaaS 产业其实是一个“降维打击”。   2014 年的时候，Kubernetes 项目发布，其意义在于 Google 将内部的 Borg/Omega 系统思想借助开源社区实现了“重生”，并且提出了“容器设计模式”的思想。而 Google 之所以选择间接开源 Kubernetes 而不是直接开源 Borg 项目，其实背后的原因也比较容易理解：Borg/Omega 这样的系统太复杂了，是没办法提供给 Google 之外的人使用，但是 Borg/Omega 这样的设计思想却可以借助 Kubernetes 让大家接触到，这也是开源 Kubernetes 的重要背景。   这样到了 2015 年到 2016 年，就到了容器编排“三国争霸”的时代，当时 Docker、Swarm、Mesos、Kubernetes 都在容器编排领域展开角逐，他们竞争的原因其实也比较容易理解， 那就是 Docker 或者容器本身的价值虽然大，但是如果想要让其产生商业价值或者说对云的价值，那么就一定需要在编排上面占据一个有利的位置。   Swarm 和 Mesos 的特点，那就是各自只在生态和技术方面比较强，其中，Swarm 更偏向于生态，而 Mesos 技术更强一些。相比之下， Kubernetes 则兼具了两者优势，最终在 2017 年“三国争霸”的局面中得以胜出，成为了当时直到现在的容器编排标准。这一过程的代表性事件就是 Docker 公司宣布在核心产品中内置了 Kubernetes 服务，并且 Swarm 项目逐渐停止维护。   到了 2018 年的时候，云原生技术理念开始逐渐萌芽，这是因为此时 Kubernetes 以及容器都成为了云厂商的既定标准，以“云”为核心的软件研发思想逐步形成。   而到了 2019 年，情况似乎又将发生一些变化。</p>

<h3>2019 年——云原生技术普及元年</h3>

<p>为什么说 2019 年很可能是一个关键节点呢？<strong>我们认为 2019 年是云原生技术的普及元年。</strong>   首先大家可以看到，在 2019 年，阿里巴巴宣布要全面上云，而且“上云就要上云原生”。我们还可以看到，以“云”为核心的软件研发思想，正逐步成为所有开发者的默认选项。像 Kubernetes 等云原生技术正在成为技术人员的必修课，大量的工作岗位正在涌现出来。</p>

<p>这种背景下，“会 Kubernetes”已经远远不够了，“懂 Kubernetes”、“会云原生架构”的重要性正日益凸显出来。 从 2019 年开始，云原生技术将会大规模普及，这也是为什么大家都要在这个时间点上学习和投资云原生技术的重要原因。</p>

<h2>“云原生技术公开课”是一门怎样的课程？</h2>

<p>基于上面所提到的技术趋势，所以阿里巴巴和 CNCF 联合开设了云原生技术公开课。   那么这样的公开课到底在讲什么内容呢？</p>

<h3>公开课教学大纲</h3>

<p>第一期云原生公开课的教学大纲，主要以应用容器和 Kubernetes 为核心，在后面几期将会陆续上线 Service Mesh、Serverless 等相关课程。   在第一期公开课中，我们首先将课程分为两部分——基础知识部分和进阶知识部分：</p>

<ul>

<li>首先，我们希望通过第一部分的课程讲解帮助大家夯实基础。然后，对于更高阶的内容展开更深入的代码级别的剖析。希望通过这样循序渐进的方式帮助大家学习云原生技术；</li>

<li>其次，在每个课程后面我们的讲师都会设置对应的课后自测考试题，这些考试题实际上是对本节课程最有效的归纳，我们希望能够通过课后评测的方式来帮助大家总结知识点，打造出属于自己的云原生知识体系；</li>

<li>最后，我们的讲师在每个知识点的背后都设计了云端实践，所谓“实践出真知”，学习计算机相关的知识还是需要上手来实际地进行操作才可以。 因此在云端实践部分，讲师会提供详细的实践步骤供大家课后自我联系。并且在这个环节，阿里云还会赠送了定量的阿里云代金券帮助大家更好地在云上进行实践。</li>

</ul>

<p>以上三个部分就构成了阿里云和 CNCF 联合推出的云原生技术公开课的教学内容。</p>

<h3>公开课授课计划</h3>

<p>在授课计划方面，初步这样安排：第一堂课在 2019 年 9 月上线，此后将会每周更新2节课，总共 29 个课时。每个知识点后面都提供了课后自测。   对于讲师阵容而言，也是本次公开课最引以为傲的部分。我们的公开课将会主要由 CNCF 社区资深成员与项目维护者为大家讲解，很多课程讲师都是阿里云容器平台团队的专家级工程师。同时，我们也会邀请云原生社区的资深专家和外部讲师为大家讲解部分内容。因此在课程进行过程中，我们会不定期地安排大咖直播、课程答疑和落地实践案例。   我们希望将这些内容都集成在一起，为大家呈现一个中国最完整、最权威、最具有影响力的云原生技术公开课。</p>

<h3>课程预备知识</h3>

<p>大家可能存在这样的疑惑，就是想要学习云原生基础知识之前需要哪些预备知识呢？其实大致需要三部分预备知识：</p>

<ol>

<li><strong>Linux 操作系统知识</strong>：主要是一些通识性的基础，最好具有一定的在 Linux 下开发的经验；</li>

<li>计算机和程序设计的基础：这一点到入门工程师或者高年级本科生水平就足够了；</li>

<li><strong>容器的使用基础</strong>：希望大家具有容器的简单使用经验，比如 docker run 以及 docker build 等，最好有一定 Docker 化应用开发的经验。当然，我们在课程中也会讲解相关的基础知识。</li>

</ol>

<h2>什么是“云原生”？云原生该怎么落地？</h2>

<p>在介绍完课程之后，我们再来详细的聊一聊“云原生”：什么是“云原生”？云原生该怎么落地？这两个问题也是整个课程的核心内容。</p>

<h3>云原生的定义</h3>

<p>很多人都会问“到底什么是云原生？”   实际上，云原生是一条最佳路径或者最佳实践。更详细的说，<strong>云原生为用户指定了一条低心智负担的、敏捷的、能够以可扩展、可复制的方式最大化地利用云的能力、发挥云的价值的最佳路径。</strong>   因此，云原生其实是一套指导进行软件架构设计的思想。按照这样的思想而设计出来的软件：首先，天然就“生在云上，长在云上”；其次，能够最大化地发挥云的能力，使得我们开发的软件和“云”能够天然地集成在一起，发挥出“云”的最大价值。   所以，云原生的最大价值和愿景，就是认为未来的软件，会从诞生起就生长在云上，并且遵循一种新的软件开发、发布和运维模式，从而使得软件能够最大化地发挥云的能力。说到了这里，大家可以思考一下为什么容器技术具有革命性？</p>

<p>其实，容器技术和集装箱技术的革命性非常类似，即：容器技术使得应用具有了一种“自包含”的定义方式。所以，这样的应用才能以敏捷的、以可扩展可复制的方式发布在云上，发挥出云的能力。这也就是容器技术对云发挥出的革命性影响所在，所以说，容器技术正是云原生技术的核心底盘。</p>

<h3>云原生的技术范畴</h3>

<p>云原生的技术范畴包括了以下几个方面：</p>

<ul>

<li>第一部分是云应用定义与开发流程。这包括应用定义与镜像制作、配置 CI/CD、消息和 Streaming 以及数据库等。</li>

<li>第二部分是云应用的编排与管理流程。这也是 Kubernetes 比较关注的一部分，包括了应用编排与调度、服务发现治理、远程调用、API 网关以及 Service Mesh。</li>

<li>第三部分是监控与可观测性。这部分所强调的是云上应用如何进行监控、日志收集、Tracing 以及在云上如何实现破坏性测试，也就是混沌工程的概念。</li>

<li>第四部分就是云原生的底层技术，比如容器运行时、云原生存储技术、云原生网络技术等。</li>

<li>第五部分是云原生工具集，在前面的这些核心技术点之上，还有很多配套的生态或者周边的工具需要使用，比如流程自动化与配置管理、容器镜像仓库、云原生安全技术以及云端密码管理等。</li>

<li>最后则是 Serverless。Serverless 是一种 PaaS 的特殊形态，它定义了一种更为“极端抽象”的应用编写方式，包含了 FaaS 和 BaaS 这样的概念。而无论是 FaaS 还是 BaaS，其最为典型的特点就是按实际使用计费（Pay as you go），因此 Serverless 计费也是重要的知识和概念。</li>

</ul>

<h3>云原生思想的两个理论</h3>

<p>在了解完云原生的技术范畴之后你就会发现，其所包含的技术内容还是很多的，但是这些内容的技术本质却是类似的。云原生技术的本质是两个理论基础。</p>

<ul>

<li><strong>第一个理论基础是：不可变基础设施</strong>。这一点目前是通过容器镜像来实现的，其含义就是应用的基础设施应该是不可变的，是一个自包含、自描述可以完全在不同环境中迁移的东西；</li>

<li><strong>第二个理论基础就是：云应用编排理论</strong>。当前的实现方式就是 Google 所提出来的“容器设计模式”，这也是本系列课程中的 Kubernetes 部分所需主要讲解的内容。</li>

</ul>

<h3>基础设施向云演进的过程</h3>

<p>首先为大家介绍一下“不可变基础设施”的概念。其实，应用所依赖的基础设施也在经历一个向云演进的过程，举例而言，对于传统的应用基础设施而言，其实往往是可变的。</p>

<p>大家可能经常会干这样一件事情，比如需要发布或者更新一个软件，那么流程大致是这样的，先通过 SSH 连到服务器，然后手动升级或者降级软件包，逐个调整服务器上的配置文件，并且将新代码直接都部署到现有服务器上。因此，这套基础设施会不断地被调整和修改。   但是在云上，对“云”友好的应用基础设施是不可变的。</p>

<p>这种场景下的上述更新过程会这么做：一旦应用部署完成之后，那么这套应用基础设施就不会再修改了。如果需要更新，那么需要现更改公共镜像来构建新服务直接替换旧服务。而我们之所以能够实现直接替换，就是因为容器提供了自包含的环境（包含应用运行所需的所有依赖）。所以对于应用而言，完全不需要关心容器发生了什么变化，只需要把容器镜像本身修改掉就可以了。因此，对于云友好的基础设施是随时可以替换和更换的，这就是因为容器具有敏捷和一致性的能力，也就是云时代的应用基础设施。   所以，总结而言，云时代的基础设施就像是可以替代的“牲口”，可以随时替换；而传统的基础设施则是独一无二的“宠物”，需要细心呵护，这就体现出了云时代不可变基础设施的优点。</p>

<h3>基础设施向云演进的意义</h3>

<p>所以，像这样的基础设施向“不可变”演进的过程，为我们提供了两个非常重要的优点。</p>

<ul>

<li>1、基础设施的一致性和可靠性。同样一个镜像，无论是在美国打开，在中国打开，还是在印度打开都是一样的。并且其中的 OS 环境对于应用而言都是一致的。而对于应用而言，它就不需要关心容器跑在哪里，这就是基础设施一致性非常重要的一个特征。</li>

<li>2、这样的镜像本身就是自包含的，其包含了应用运行所需要的所有依赖，因此也可以漂移到云上的任何一个位置。</li>

</ul>

<p>此外，云原生的基础设施还提供了简单、可预测的部署和运维能力。由于现在有了镜像，应用还是自描述的，通过镜像运行起来的整个容器其实可以像 Kubernetes 的 Operator 技术一样将其做成自运维的，所以整个应用本身都是自包含的行为，使得其能够迁移到云上任何一个位置。这也使得整个流程的自动化变得非常容易。</p>

<p>应用本身也可以更好地扩容，从 1 个实例变成 100 个实例，进而变成 1 万个实例，这个过程对于容器化后的应用没有任何特殊的。最后，我们这时也能够通过不可变的基础设施来地快速周围的管控系统和支撑组件。因为，这些组件本身也是容器化的，是符合不可变基础设施这样一套理论的组件。   以上就是不可变基础设施为用户带来的最大的优点。</p>

<h3>云原生关键技术点</h3>

<p>当我们回过头来看云原生关键技术点或者说它所依赖的技术理论的时候，可以看到主要有这样的四个方向：</p>

<ol>

<li>如何构建自包含、可定制的应用镜像；</li>

<li>能不能实现应用快速部署与隔离能力；</li>

<li>应用基础设施创建和销毁的自动化管理；</li>

<li>可复制的管控系统和支撑组件。</li>

</ol>

<p>这四个云原生关键技术点是落地实现云原生技术的四个主要途径，而这四个技术点也是本门课程的 17 个技术点所主要讲述的核心知识。</p>

<h2>本节总结</h2>

<ul>

<li>“云原生”具备着重要的意义，它是云时代技术人自我提升的必备路径；</li>

<li>“云原生”定义了一条云时代应用从开发到交付的最佳路径；</li>

<li>“云原生”应用生在云上，长在云上，希望能够将云的能力发挥到极致。</li>

</ul>

<h2>讲师点评</h2>

<p>“未来的软件一定是生长于云上的”这是云原生理念的最核心假设。而所谓“云原生”，实际上就是在定义一条能够让应用最大程度利用云的能力、发挥云的价值的最佳路径。在这条路径上，脱离了“应用”这个载体，“云原生”就无从谈起；容器技术，则是将这个理念落地、将软件交付的革命持续进行下去的重要手段之一。</p>

<p>而本期云原生公开课重点讲解的 Kubernetes 项目，则是整个“云原生”理念落地的核心与关键所在。它正在迅速成为连通“云”与“应用”的高速公路，以标准、高效的方式将“应用”快速交付到世界上任何一个位置。如今”云原生应用交付“，已经成为了 2019 年云计算市场上最热门的技术关键词之一。希望学习课程的同学们能够学以致用，持续关注以 K8s 为基础进行“云原生应用管理与交付”的技术趋势。</p>

</div>

                    </div><div class="book-post">

                        <p id="tip" align="center"></p>

                        <div><h1>02 容器基本概念</h1>

<h1>容器与镜像</h1>

<h3>什么是容器？</h3>

<p>在介绍容器的具体概念之前，先简单回顾一下操作系统是如何管理进程的。</p>

<p>首先，当我们登录到操作系统之后，可以通过 ps 等操作看到各式各样的进程，这些进程包括系统自带的服务和用户的应用进程。那么，这些进程都有什么样的特点？</p>

<ul>

<li>第一，这些进程可以相互看到、相互通信；</li>

<li>第二，它们使用的是同一个文件系统，可以对同一个文件进行读写操作；</li>

<li>第三，这些进程会使用相同的系统资源。</li>

</ul>

<p>这样的三个特点会带来什么问题呢？</p>

<ul>

<li>因为这些进程能够相互看到并且进行通信，高级权限的进程可以攻击其他进程；</li>

<li>因为它们使用的是同一个文件系统，因此会带来两个问题：这些进程可以对于已有的数据进行增删改查，具有高级权限的进程可能会将其他进程的数据删除掉，破坏掉其他进程的正常运行；此外，进程与进程之间的依赖可能会存在冲突，如此一来就会给运维带来很大的压力；</li>

<li>因为这些进程使用的是同一个宿主机的资源，应用之间可能会存在资源抢占的问题，当一个应用需要消耗大量 CPU 和内存资源的时候，就可能会破坏其他应用的运行，导致其他应用无法正常地提供服务。</li>

</ul>

<p>针对上述的三个问题，如何为进程提供一个独立的运行环境呢？</p>

<ul>

<li>

<p>针对不同进程使用同一个文件系统所造成的问题而言，Linux 和 Unix 操作系统可以通过 chroot 系统调用将子目录变成根目录，达到视图级别的隔离；进程在 chroot 的帮助下可以具有独立的文件系统，对于这样的文件系统进行增删改查不会影响到其他进程；</p>

</li>

<li>

<p>因为进程之间相互可见并且可以相互通信，使用 Namespace 技术来实现进程在资源的视图上进行隔离。在 chroot 和 Namespace 的帮助下，进程就能够运行在一个独立的环境下了；</p>

</li>

<li>

<p>但在独立的环境下，进程所使用的还是同一个操作系统的资源，一些进程可能会侵蚀掉整个系统的资源。为了减少进程彼此之间的影响，可以通过 Cgroup 来限制其资源使用率，设置其能够使用的 CPU 以及内存量。</p>

<p>那么，应该如何定义这样的进程集合呢？</p>

</li>

</ul>

<p>其实，**容器就是一个视图隔离、资源可限制、独立文件系统的进程集合。**所谓“视图隔离”就是能够看到部分进程以及具有独立的主机名等；控制资源使用率则是可以对于内存大小以及 CPU 使用个数等进行限制。容器就是一个进程集合，它将系统的其他资源隔离开来，具有自己独立的资源视图。</p>

<p>容器具有一个独立的文件系统，因为使用的是系统的资源，所以在独立的文件系统内不需要具备内核相关的代码或者工具，我们只需要提供容器所需的二进制文件、配置文件以及依赖即可。只要容器运行时所需的文件集合都能够具备，那么这个容器就能够运行起来。</p>

<h3>什么是镜像？</h3>

<p>综上所述，我们将这些容器运行时所需要的所有的文件集合称之为容器镜像。</p>

<p>那么，一般都是通过什么样的方式来构建镜像的呢？通常情况下，我们会采用 Dockerfile 来构建镜像，这是因为 Dockerfile 提供了非常便利的语法糖，能够帮助我们很好地描述构建的每个步骤。当然，每个构建步骤都会对已有的文件系统进行操作，这样就会带来文件系统内容的变化，我们将这些变化称之为 changeset。当我们把构建步骤所产生的变化依次作用到一个空文件夹上，就能够得到一个完整的镜像。   changeset 的分层以及复用特点能够带来几点优势：</p>

<ul>

<li>第一，能够提高分发效率，简单试想一下，对于大的镜像而言，如果将其拆分成各个小块就能够提高镜像的分发效率，这是因为镜像拆分之后就可以并行下载这些数据；</li>

<li>第二，因为这些数据是相互共享的，也就意味着当本地存储上包含了一些数据的时候，只需要下载本地没有的数据即可，举个简单的例子就是 golang 镜像是基于 alpine 镜像进行构建的，当本地已经具有了 alpine 镜像之后，在下载 golang 镜像的时候只需要下载本地 alpine 镜像中没有的部分即可；</li>

<li>第三，因为镜像数据是共享的，因此可以节约大量的磁盘空间，简单设想一下，当本地存储具有了 alpine 镜像和 golang 镜像，在没有复用的能力之前，alpine 镜像具有 5M 大小，golang 镜像有 300M 大小，因此就会占用 305M 空间；而当具有了复用能力之后，只需要 300M 空间即可。</li>

</ul>

<h3>如何构建镜像？</h3>

<p>如下图所示的 Dockerfile 适用于描述如何构建 golang 应用的。</p>

<p><img src="assets/FhqkA03v6VMMVi4tioe1Nzpu4QHI" alt="avatar"></p>

<p>如图所示：</p>

<ol>

<li>FROM 行表示以下的构建步骤基于什么镜像进行构建，正如前面所提到的，镜像是可以复用的；</li>

<li>WORKDIR 行表示会把接下来的构建步骤都在哪一个相应的具体目录下进行，其起到的作用类似于 Shell 里面的 cd；</li>

<li>COPY 行表示的是可以将宿主机上的文件拷贝到容器镜像内；</li>

<li>RUN 行表示在具体的文件系统内执行相应的动作。当我们运行完毕之后就可以得到一个应用了；</li>

<li>CMD 行表示使用镜像时的默认程序名字。</li>

</ol>

<p>当有了 Dockerfile 之后，就可以通过 docker build 命令构建出所需要的应用。构建出的结果存储在本地，一般情况下，镜像构建会在打包机或者其他的隔离环境下完成。</p>

<p>那么，这些镜像如何运行在生产环境或者测试环境上呢？这时候就需要一个中转站或者中心存储，我们称之为 docker registry，也就是镜像仓库，其负责存储所有产生的镜像数据。我们只需要通过 docker push 就能够将本地镜像推动到镜像仓库中，这样一来，就能够在生产环境上或者测试环境上将相应的数据下载下来并运行了。</p>

<h3>如何运行容器？</h3>

<p>运行一个容器一般情况下分为三步：</p>

<ul>

<li>第一步：从镜像仓库中将相应的镜像下载下来；</li>

<li>第二步：当镜像下载完成之后就可以通过 docker images 来查看本地镜像，这里会给出一个完整的列表，我们可以在列表中选中想要的镜像；</li>

<li>第三步：当选中镜像之后，就可以通过 docker run 来运行这个镜像得到想要的容器，当然可以通过多次运行得到多个容器。一个镜像就相当于是一个模板，一个容器就像是一个具体的运行实例，因此镜像就具有了一次构建、到处运行的特点。</li>

</ul>

<h3>小结</h3>

<p>简单回顾一下，容器就是和系统其它部分隔离开来的进程集合，这里的其他部分包括进程、网络资源以及文件系统等。而镜像就是容器所需要的所有文件集合，其具备一次构建、到处运行的特点。</p>

<h2>容器的生命周期</h2>

<h3>容器运行时的生命周期</h3>

<p>容器是一组具有隔离特性的进程集合，在使用 docker run 的时候会选择一个镜像来提供独立的文件系统并指定相应的运行程序。这里指定的运行程序称之为 initial 进程，这个 initial 进程启动的时候，容器也会随之启动，当 initial 进程退出的时候，容器也会随之退出。</p>

<p>因此，可以认为容器的生命周期和 initial 进程的生命周期是一致的。当然，因为容器内不只有这样的一个 initial 进程，initial 进程本身也可以产生其他的子进程或者通过 docker exec 产生出来的运维操作，也属于 initial 进程管理的范围内。当 initial 进程退出的时候，所有的子进程也会随之退出，这样也是为了防止资源的泄漏。   但是这样的做法也会存在一些问题，首先应用里面的程序往往是有状态的，其可能会产生一些重要的数据，当一个容器退出被删除之后，数据也就会丢失了，这对于应用方而言是不能接受的，所以需要将容器所产生出来的重要数据持久化下来。容器能够直接将数据持久化到指定的目录上，这个目录就称之为数据卷。</p>

<p>数据卷有一些特点，其中非常明显的就是数据卷的生命周期是独立于容器的生命周期的，也就是说容器的创建、运行、停止、删除等操作都和数据卷没有任何关系，因为它是一个特殊的目录，是用于帮助容器进行持久化的。简单而言，我们会将数据卷挂载到容器内，这样一来容器就能够将数据写入到相应的目录里面了，而且容器的退出并不会导致数据的丢失。</p>

<p>通常情况下，数据卷管理主要有两种方式：</p>

<ul>

<li>第一种是通过 bind 的方式，直接将宿主机的目录直接挂载到容器内；这种方式比较简单，但是会带来运维成本，因为其依赖于宿主机的目录，需要对于所有的宿主机进行统一管理。</li>

<li>第二种是将目录管理交给运行引擎。</li>

</ul>

<h2>容器项目架构</h2>

<h3>moby 容器引擎架构</h3>

<p>moby 是目前最流行的容器管理引擎，moby daemon 会对上提供有关于容器、镜像、网络以及 Volume的管理。moby daemon 所依赖的最重要的组件就是 containerd，containerd 是一个容器运行时管理引擎，其独立于 moby daemon ，可以对上提供容器、镜像的相关管理。</p>

<p>containerd 底层有 containerd shim 模块，其类似于一个守护进程，这样设计的原因有几点：</p>

<ul>

<li>首先，containerd 需要管理容器生命周期，而容器可能是由不同的容器运行时所创建出来的，因此需要提供一个灵活的插件化管理。而 shim 就是针对于不同的容器运行时所开发的，这样就能够从 containerd 中脱离出来，通过插件的形式进行管理。</li>

<li>其次，因为 shim 插件化的实现，使其能够被 containerd 动态接管。如果不具备这样的能力，当 moby daemon 或者 containerd daemon 意外退出的时候，容器就没人管理了，那么它也会随之消失、退出，这样就会影响到应用的运行。</li>

<li>最后，因为随时可能会对 moby 或者 containerd 进行升级，如果不提供 shim 机制，那么就无法做到原地升级，也无法做到不影响业务的升级，因此 containerd shim 非常重要，它实现了动态接管的能力。</li>

</ul>

<p>本节课程只是针对于 moby 进行一个大致的介绍，在后续的课程也会详细介绍。</p>

<h2>容器 VS VM</h2>

<h3>容器和 VM 之间的差异</h3>

<p>VM 利用 Hypervisor 虚拟化技术来模拟 CPU、内存等硬件资源，这样就可以在宿主机上建立一个 Guest OS，这是常说的安装一个虚拟机。</p>

<p>每一个 Guest OS 都有一个独立的内核，比如 Ubuntu、CentOS 甚至是 Windows 等，在这样的 Guest OS 之下，每个应用都是相互独立的，VM 可以提供一个更好的隔离效果。但这样的隔离效果需要付出一定的代价，因为需要把一部分的计算资源交给虚拟化，这样就很难充分利用现有的计算资源，并且每个 Guest OS 都需要占用大量的磁盘空间，比如 Windows 操作系统的安装需要 10~30G 的磁盘空间，Ubuntu 也需要 5~6G，同时这样的方式启动很慢。正是因为虚拟机技术的缺点，催生出了容器技术。   容器是针对于进程而言的，因此无需 Guest OS，只需要一个独立的文件系统提供其所需要文件集合即可。所有的文件隔离都是进程级别的，因此启动时间快于 VM，并且所需的磁盘空间也小于 VM。当然了，进程级别的隔离并没有想象中的那么好，隔离效果相比 VM 要差很多。</p>

<p>总体而言，容器和 VM 相比，各有优劣，因此容器技术也在向着强隔离方向发展。</p>

<h2>本节总结</h2>

<ul>

<li>容器是一个进程集合，具有自己独特的视图视角；</li>

<li>镜像是容器所需要的所有文件集合，其具备一次构建、到处运行的特点；</li>

<li>容器的生命周期和 initial 进程的生命周期是一样的；</li>

<li>容器和 VM 相比，各有优劣，容器技术在向着强隔离方向发展。</li>

</ul>

</div>

                    </div><div class="book-post">

                        <p id="tip" align="center"></p>

                        <div><h1>03 Kubernetes 核心概念</h1>

<h2>什么是 Kubernetes</h2>

<p>Kubernetes，从官方网站上可以看到，它是一个工业级的容器编排平台。Kubernetes 这个单词是希腊语，它的中文翻译是“舵手”或者“飞行员”。在一些常见的资料中也会看到“ks”这个词，也就是“k8s”，它是通过将8个字母“ubernete ”替换为“8”而导致的一个缩写。</p>

<p>Kubernetes 为什么要用“舵手”来命名呢？大家可以看一下这张图：</p>

<p><img src="assets/FsUnrCguMgMBVyC1ybspBDgGg5tz" alt="avatar"></p>

<p>这是一艘载着一堆集装箱的轮船，轮船在大海上运着集装箱奔波，把集装箱送到它们该去的地方。我们之前其实介绍过一个概念叫做 container，container 这个英文单词也有另外的一个意思就是“集装箱”。Kubernetes 也就借着这个寓意，希望成为运送集装箱的一个轮船，来帮助我们管理这些集装箱，也就是管理这些容器。</p>

<p>这个就是为什么会选用 Kubernetes 这个词来代表这个项目的原因。更具体一点地来说：Kubernetes 是一个自动化的容器编排平台，它负责应用的部署、应用的弹性以及应用的管理，这些都是基于容器的。</p>

<h2>Kubernetes 有如下几个核心的功能</h2>

<ul>

<li>服务的发现与负载的均衡；</li>

<li>容器的自动装箱，我们也会把它叫做 scheduling，就是“调度”，把一个容器放到一个集群的某一个机器上，Kubernetes 会帮助我们去做存储的编排，让存储的声明周期与容器的生命周期能有一个连接；</li>

<li>Kubernetes 会帮助我们去做自动化的容器的恢复。在一个集群中，经常会出现宿主机的问题或者说是 OS 的问题，导致容器本身的不可用，Kubernetes 会自动地对这些不可用的容器进行恢复；</li>

<li>Kubernetes 会帮助我们去做应用的自动发布与应用的回滚，以及与应用相关的配置密文的管理；</li>

<li>对于 job 类型任务，Kubernetes 可以去做批量的执行；</li>

<li>为了让这个集群、这个应用更富有弹性，Kubernetes 也支持水平的伸缩。</li>

</ul>

<p>下面，我们希望以三个例子跟大家更切实地介绍一下 Kubernetes 的能力。</p>

<h3>调度</h3>

<p>Kubernetes 可以把用户提交的容器放到 Kubernetes 管理的集群的某一台节点上去。Kubernetes 的调度器是执行这项能力的组件，它会观察正在被调度的这个容器的大小、规格。</p>

<p>比如说它所需要的 CPU以及它所需要的 memory，然后在集群中找一台相对比较空闲的机器来进行一次 placement，也就是一次放置的操作。在这个例子中，它可能会把红颜色的这个容器放置到第二个空闲的机器上，来完成一次调度的工作。</p>

<p><img src="assets/FtQwOqpvCjmT_9MCtqS2IA0FhkKx" alt="avatar"></p>

<h3>自动修复</h3>

<p>Kubernetes 有一个节点健康检查的功能，它会监测这个集群中所有的宿主机，当宿主机本身出现故障，或者软件出现故障的时候，这个节点健康检查会自动对它进行发现。</p>

<p>下面 Kubernetes 会把运行在这些失败节点上的容器进行自动迁移，迁移到一个正在健康运行的宿主机上，来完成集群内容器的一个自动恢复。</p>

<p><img src="assets/Ft9Rf5hiHOSF5h0P0dz-RUVpq2Fs" alt="avatar"> <img src="assets/Fju4lrcck9azmN8uV1jHsk2NYM5d" alt="avatar"></p>

<h3>水平伸缩</h3>

<p>Kubernetes 有业务负载检查的能力，它会监测业务上所承担的负载，如果这个业务本身的 CPU 利用率过高，或者响应时间过长，它可以对这个业务进行一次扩容。</p>

<p>比如说在下面的例子中，黄颜色的过度忙碌，Kubernetes 就可以把黄颜色负载从一份变为三份。接下来，它就可以通过负载均衡把原来打到第一个黄颜色上的负载平均分到三个黄颜色的负载上去，以此来提高响应的时间。</p>

<p><img src="assets/Flzm1OeSsGAqzkPS_l_SL8OWo99t" alt="avatar"> <img src="assets/FiyLMf6T0Q1Z-OAmAF7THSwsyIxG" alt="avatar"></p>

<p>以上就是 Kubernetes 三个核心能力的简单介绍。</p>

<h2>Kubernetes 的架构</h2>

<p>Kubernetes 架构是一个比较典型的二层架构和 server-client 架构。Master 作为中央的管控节点，会去与 Node 进行一个连接。</p>

<p>所有 UI 的、clients、这些 user 侧的组件，只会和 Master 进行连接，把希望的状态或者想执行的命令下发给 Master，Master 会把这些命令或者状态下发给相应的节点，进行最终的执行。</p>

<p><img src="assets/FlcotYqNLPbCvnibylrLt33enE1M" alt="avatar"></p>

<p>Kubernetes 的 Master 包含四个主要的组件：API Server、Controller、Scheduler 以及 etcd。如下图所示：</p>

<p><img src="assets/FrFh4R6uKA2VDeclmHMZPxqTWbl_" alt="avatar"></p>

<ul>

<li><strong>API Server：</strong> 顾名思义是用来处理 API 操作的，Kubernetes 中所有的组件都会和 API Server 进行连接，组件与组件之间一般不进行独立的连接，都依赖于 API Server 进行消息的传送；</li>

<li><strong>Controller：</strong> 是控制器，它用来完成对集群状态的一些管理。比如刚刚我们提到的两个例子之中，第一个自动对容器进行修复、第二个自动进行水平扩张，都是由 Kubernetes 中的 Controller 来进行完成的；</li>

<li><strong>Scheduler：</strong> 是调度器，“调度器”顾名思义就是完成调度的操作，就是我们刚才介绍的第一个例子中，把一个用户提交的 Container，依据它对 CPU、对 memory 请求大小，找一台合适的节点，进行放置；</li>

<li><strong>etcd：</strong> 是一个分布式的一个存储系统，API Server 中所需要的这些原信息都被放置在 etcd 中，etcd 本身是一个高可用系统，通过 etcd 保证整个 Kubernetes 的 Master 组件的高可用性。</li>

</ul>

<p>我们刚刚提到的 API Server，它本身在部署结构上是一个可以水平扩展的一个部署组件；Controller 是一个可以进行热备的一个部署组件，它只有一个 active，它的调度器也是相应的，虽然只有一个 active，但是可以进行热备。</p>

<h3>Kubernetes 的架构：Node</h3>

<p>Kubernetes 的 Node 是真正运行业务负载的，每个业务负载会以 Pod 的形式运行。等一下我会介绍一下 Pod 的概念。一个 Pod 中运行的一个或者多个容器，真正去运行这些 Pod 的组件的是叫做 <strong>kubelet</strong>，也就是 Node 上最为关键的组件，它通过 API Server 接收到所需要 Pod 运行的状态，然后提交到我们下面画的这个 Container Runtime 组件中。</p>

<p><img src="assets/Fp29--O3Bo8y2VsC3C3SkwLILJ4-" alt="avatar"></p>

<p>在 OS 上去创建容器所需要运行的环境，最终把容器或者 Pod 运行起来，也需要对存储跟网络进行管理。Kubernetes 并不会直接进行网络存储的操作，他们会靠 Storage Plugin 或者是网络的 Plugin 来进行操作。用户自己或者云厂商都会去写相应的 <strong>Storage Plugin</strong> 或者 <strong>Network Plugin</strong>，去完成存储操作或网络操作。</p>

<p>在 Kubernetes 自己的环境中，也会有 Kubernetes 的 Network，它是为了提供 Service network 来进行搭网组网的。（等一下我们也会去介绍“service”这个概念。）真正完成 service 组网的组件的是 <strong>Kube-proxy</strong>，它是利用了 iptable 的能力来进行组建 Kubernetes 的 Network，就是 cluster network，以上就是 Node 上面的四个组件。</p>

<p>Kubernetes 的 Node 并不会直接和 user 进行 interaction，它的 interaction 只会通过 Master。而 User 是通过 Master 向节点下发这些信息的。Kubernetes 每个 Node 上，都会运行我们刚才提到的这几个组件。</p>

<p>下面我们以一个例子再去看一下 Kubernetes 架构中的这些组件，是如何互相进行 interaction 的。</p>

<p><img src="assets/FsXK0GqTGIYAF9Vu5wndumZQlB7A" alt="avatar"></p>

<p>用户可以通过 UI 或者 CLI 提交一个 Pod 给 Kubernetes 进行部署，这个 Pod 请求首先会通过 CLI 或者 UI 提交给 Kubernetes API Server，下一步 API Server 会把这个信息写入到它的存储系统 etcd，之后 Scheduler 会通过 API Server 的 watch 或者叫做 notification 机制得到这个信息：有一个 Pod 需要被调度。</p>

<p>这个时候 Scheduler 会根据它的内存状态进行一次调度决策，在完成这次调度之后，它会向 API Server report 说：“OK！这个 Pod 需要被调度到某一个节点上。”</p>

<p>这个时候 API Server 接收到这次操作之后，会把这次的结果再次写到 etcd 中，然后 API Server 会通知相应的节点进行这次 Pod 真正的执行启动。相应节点的 kubelet 会得到这个通知，kubelet 就会去调 Container runtime 来真正去启动配置这个容器和这个容器的运行环境，去调度 Storage Plugin 来去配置存储，network Plugin 去配置网络。</p>

<p>这个例子我们可以看到：这些组件之间是如何相互沟通相互通信，协调来完成一次Pod的调度执行操作的。</p>

<h2>Kubernetes 的核心概念与它的 API</h2>

<h3>核心概念</h3>

<h4>第一个概念：Pod</h4>

<p>Pod 是 Kubernetes 的一个最小调度以及资源单元。用户可以通过 Kubernetes 的 Pod API 生产一个 Pod，让 Kubernetes 对这个 Pod 进行调度，也就是把它放在某一个 Kubernetes 管理的节点上运行起来。一个 Pod 简单来说是对一组容器的抽象，它里面会包含一个或多个容器。</p>

<p>比如像下面的这幅图里面，它包含了两个容器，每个容器可以指定它所需要资源大小。比如说，一个核一个 G，或者说 0.5 个核，0.5 个 G。</p>

<p>当然在这个 Pod 中也可以包含一些其他所需要的资源：比如说我们所看到的 Volume 卷这个存储资源；比如说我们需要 100 个 GB 的存储或者 20GB 的另外一个存储。</p>

<p><img src="assets/FmFsx1ss6Nc01jifUhwW9NHW9W3_" alt="avatar"></p>

<p>在 Pod 里面，我们也可以去定义容器所需要运行的方式。比如说运行容器的 Command，以及运行容器的环境变量等等。Pod 这个抽象也给这些容器提供了一个共享的运行环境，它们会共享同一个网络环境，这些容器可以用 localhost 来进行直接的连接。而 Pod 与 Pod 之间，是互相有 isolation 隔离的。</p>

<h4>第二个概念：Volume</h4>

<p>Volume 就是卷的概念，它是用来管理 Kubernetes 存储的，是用来声明在 Pod 中的容器可以访问文件目录的，一个卷可以被挂载在 Pod 中一个或者多个容器的指定路径下面。</p>

<p>而 Volume 本身是一个抽象的概念，一个 Volume 可以去支持多种的后端的存储。比如说 Kubernetes 的 Volume 就支持了很多存储插件，它可以支持本地的存储，可以支持分布式的存储，比如说像 ceph，GlusterFS ；它也可以支持云存储，比如说阿里云上的云盘、AWS 上的云盘、Google 上的云盘等等。</p>

<p><img src="assets/Fl18QUI1iine53dAy56-l4sErsLr" alt="avatar"></p>

<h4>第三个概念：Deployment</h4>

<p>Deployment 是在 Pod 这个抽象上更为上层的一个抽象，它可以定义一组 Pod 的副本数目、以及这个 Pod 的版本。一般大家用 Deployment 这个抽象来做应用的真正的管理，而 Pod 是组成 Deployment 最小的单元。</p>

<p>Kubernetes 是通过 Controller，也就是我们刚才提到的控制器去维护 Deployment 中 Pod 的数目，它也会去帮助 Deployment 自动恢复失败的 Pod。</p>

<p>比如说我可以定义一个 Deployment，这个 Deployment 里面需要两个 Pod，当一个 Pod 失败的时候，控制器就会监测到，它重新把 Deployment 中的 Pod 数目从一个恢复到两个，通过再去新生成一个 Pod。通过控制器，我们也会帮助完成发布的策略。比如说进行滚动升级，进行重新生成的升级，或者进行版本的回滚。</p>

<p><img src="assets/FnD98YqsPIFP_5wKbyqD3Z7tKfOH" alt="avatar"></p>

<h4>第四个概念：Service</h4>

<p>Service 提供了一个或者多个 Pod 实例的稳定访问地址。</p>

<p>比如在上面的例子中，我们看到：一个 Deployment 可能有两个甚至更多个完全相同的 Pod。对于一个外部的用户来讲，访问哪个 Pod 其实都是一样的，所以它希望做一次负载均衡，在做负载均衡的同时，我只想访问某一个固定的 VIP，也就是 Virtual IP 地址，而不希望得知每一个具体的 Pod 的 IP 地址。</p>

<p>我们刚才提到，这个 pod 本身可能 terminal go（终止），如果一个 Pod 失败了，可能会换成另外一个新的。</p>

<p>对一个外部用户来讲，提供了多个具体的 Pod 地址，这个用户要不停地去更新 Pod 地址，当这个 Pod 再失败重启之后，我们希望有一个抽象，把所有 Pod 的访问能力抽象成一个第三方的一个 IP 地址，实现这个的 Kubernetes 的抽象就叫 Service。</p>

<p>实现 Service 有多种方式，Kubernetes 支持 Cluster IP，上面我们讲过的 kuber-proxy 的组网，它也支持 nodePort、 LoadBalancer 等其他的一些访问的能力。</p>

<p><img src="assets/Fqw4YYXdiHBS7TI2oFyCy55WVzP9" alt="avatar"></p>

<h4>第五个概念：Namespace</h4>

<p>Namespace 是用来做一个集群内部的逻辑隔离的，它包括鉴权、资源管理等。Kubernetes 的每个资源，比如刚才讲的 Pod、Deployment、Service 都属于一个 Namespace，同一个 Namespace 中的资源需要命名的唯一性，不同的 Namespace 中的资源可以重名。</p>

<p>Namespace 一个用例，比如像在阿里巴巴，我们内部会有很多个 business units，在每一个 business units 之间，希望有一个视图上的隔离，并且在鉴权上也不一样，在 cuda 上面也不一样，我们就会用 Namespace 来去给每一个 BU 提供一个他所看到的这么一个看到的隔离的机制。</p>

<p><img src="assets/FomVvOOmGfThgv1lQQ3d30d8v4aa" alt="avatar"></p>

<h3>Kubernetes 的 API</h3>

<p>下面我们介绍一下 Kubernetes 的 API 的基础知识。从 high-level 上看，Kubernetes API 是由 <strong>HTTP+JSON</strong> 组成的：用户访问的方式是 HTTP，访问的 API 中 content 的内容是 JSON 格式的。</p>

<p>Kubernetes 的 kubectl 也就是 command tool，Kubernetes UI，或者有时候用 curl，直接与 Kubernetes 进行沟通，都是使用 HTTP + JSON 这种形式。</p>

<p>下面有个例子：比如说，对于这个 Pod 类型的资源，它的 HTTP 访问的路径，就是 API，然后是 apiVesion: V1, 之后是相应的 Namespaces，以及 Pods 资源，最终是 Podname，也就是 Pod 的名字。</p>

<p><img src="assets/Fp8roGITYZHjGDSiwrf3Q5zt7l1S" alt="avatar"></p>

<p>如果我们去提交一个 Pod，或者 get 一个 Pod 的时候，它的 content 内容都是用 JSON 或者是 YAML 表达的。上图中有个 yaml 的例子，在这个 yaml file 中，对 Pod 资源的描述也分为几个部分。</p>

<p>第一个部分，一般来讲会是 API 的 <strong>version</strong>。比如在这个例子中是 V1，它也会描述我在操作哪个资源；比如说我的 <strong>kind</strong> 如果是 pod，在 Metadata 中，就写上这个 Pod 的名字；比如说 nginx，我们也会给它打一些 <strong>label</strong>，我们等下会讲到 label 的概念。在 Metadata 中，有时候也会去写 <strong>annotation</strong>，也就是对资源的额外的一些用户层次的描述。</p>

<p>比较重要的一个部分叫做 <strong>Spec</strong>，Spec 也就是我们希望 Pod 达到的一个预期的状态。比如说它内部需要有哪些 container 被运行；比如说这里面有一个 nginx 的 container，它的 image 是什么？它暴露的 port 是什么？</p>

<p>当我们从 Kubernetes API 中去获取这个资源的时候，一般来讲在 Spec 下面会有一个项目叫 <strong>status</strong>，它表达了这个资源当前的状态；比如说一个 Pod 的状态可能是正在被调度、或者是已经 running、或者是已经被 terminates，就是被执行完毕了。</p>

<p>刚刚在 API 之中，我们讲了一个比较有意思的 metadata 叫做“<strong>label</strong>”，这个 label 可以是一组 KeyValuePair。</p>

<p>比如下图的第一个 pod 中，label 就可能是一个 color 等于 red，即它的颜色是红颜色。当然你也可以加其他 label，比如说 size: big 就是大小，定义为大的，它可以是一组 label。</p>

<p>这些 label 是可以被 selector，也就是选择器所查询的。这个能力实际上跟我们的 sql 类型的 select 语句是非常相似的，比如下图中的三个 Pod 资源中，我们就可以进行 select。name color 等于 red，就是它的颜色是红色的，我们也可以看到，只有两个被选中了，因为只有他们的 label 是红色的，另外一个 label 中写的 color 等于 yellow，也就是它的颜色是黄色，是不会被选中的。</p>

<p><img src="assets/FuUn6ZkhMD2b2UV6CY2jHYP8i450" alt="avatar"></p>

<p>通过 label，kubernetes 的 API 层就可以对这些资源进行一个筛选，那这些筛选也是 kubernetes 对资源的集合所表达默认的一种方式。</p>

<p>例如说，我们刚刚介绍的 Deployment，它可能是代表一组的 Pod，它是一组 Pod 的抽象，一组 Pod 就是通过 label selector 来表达的。当然我们刚才讲到说 service 对应的一组 Pod，就是一个 service 要对应一个或者多个的 Pod，来对它们进行统一的访问，这个描述也是通过 label selector 来进行 select 选取的一组 Pod。</p>

<p>所以可以看到 label 是一个非常核心的 kubernetes API 的概念，我们在接下来的课程中也会着重地去讲解和介绍 label 这个概念，以及如何更好地去使用它。</p>

<h2>以一个 demo 结尾</h2>

<p>最后一部分，我想以一个例子来结束，让大家跟我一起来尝试一个 kubernetes，在尝试 Kubernetes 之前，我希望大家能在本机上安装一下 Kubernetes，安装一个 Kubernetes 沙箱环境。</p>

<p>安装这个沙箱环境，主要有三个步骤：</p>

<ul>

<li>

<p>首先需要安装一个虚拟机，来在虚拟机中启动 Kubernetes。我们会推荐大家利用 virtualbox 来作为虚拟机的运行环境；</p>

<p>安装 VirtualBox： <a href="https://www.virtualbox.org/wiki/Downloads">https://www.virtualbox.org/wiki/Downloads</a></p>

</li>

<li>

<p>其次我们需要在虚拟机中启动 Kubernetes，Kubernetes 有一个非常有意思的项目，叫 minikube，也就是启动一个最小的 local 的 Kubernetes 的一个环境。</p>

<p>minikube 我们推荐使用下面写到的阿里云的版本，它和官方 minikube 的主要区别就是把 minikube 中所需要的 Google 上的依赖换成国内访问比较快的一些镜像，这样就方便了大家的安装工作；</p>

<p>安装 MiniKube（中国版）: <a href="https://yq.aliyun.com/articles/221687">https://yq.aliyun.com/articles/221687</a></p>

</li>

<li>

<p>最后在安装完 virtualbox 和 minikube 之后，大家可以对 minikube 进行启动，也就是下面这个命令。</p>

</li>

</ul>

<p>启动命令：minikube start —vm-driver virtualbox</p>

<p>如果大家不是 Mac 系统，其他操作系统请访问下面这个链接，查看其它操作系统如何安装 minikube 沙箱环境。</p>

<p><a href="https://kubernetes.io/docs/tasks/tools/install-minikube，">https://kubernetes.io/docs/tasks/tools/install-minikube/</a></p>

<p>当大家安装好之后，我会跟大家一起做一个例子，来做三件事情：</p>

<ul>

<li>

<p>提交一个 nginx deployment；</p>

<p>kubectl apply  -f  <a href="https://k8s.io/examples/application/deployment.yaml">https://k8s.io/examples/application/deployment.yaml</a></p>

</li>

<li>

<p>升级 nginx deployment；</p>

<p>kubectl apply -f  <a href="https://k8s.io/examples/application/deployment-update.yaml">https://k8s.io/examples/application/deployment-update.yaml</a></p>

</li>

<li>

<p>扩容 nginx deployment。</p>

<p>kubectl apply -f  <a href="https://k8s.io/examples/application/deployment-update.yaml">https://k8s.io/examples/application/deployment-update.yaml</a></p>

</li>

</ul>

<p>第一步，我们提交一个 nginx 的 Deployment，然后对这个 Deployment 进行一次版本升级，也就是改变它中间 Pod 的版本。最后我们也会尝试对 nginx 进行一次扩容，进行一次水平的伸缩，下面就让大家一起跟我来尝试这三个操作吧。</p>

<p>首先，我们先看一下 minikube 的 status，可以看到 kubelet master 和 kubectl 都是配置好的。</p>

<p><img src="assets/FlOUt7JuwbE0UBUz8NwA9vVYCuUl" alt="avatar"></p>

<p>下一步我们利用 kubectl 来看一下这个集群中节选的状态，可以看到这个master 的节点已经是 running 状态：</p>

<p><img src="assets/FlT6wcXrdtsvhAnoiNWVFRVx1iiM" alt="avatar"></p>

<p>我们就以这个为节点，下面我们尝试去看一下现在集群中 Deployment 这个资源：</p>

<p><img src="assets/FggAaOvw-U4UXQS7NMrcO3ew02X_" alt="avatar"></p>

<p>可以看到集群中没有任何的 Deployment，我们可以利用 watch 这个语义去看集群中 Deployment 这个资源的变化情况。</p>

<p>下面我们去做刚才想要的三个操作：第一个操作是去创建一个 Deployment。可以看到下面第一个图，这是一个 API 的 content，它的 kind 是 Deployment，name 是 nginx-deployment, 有图中它的 replicas 数目是2，它的镜像版本是 1.7.9。</p>

<p><img src="assets/FrqXKjDLgyIqN_vTsIlWt1ImrvxZ" alt="avatar"> <img src="assets/FpIjS0iNnlmx5GRpxdJFTcWlT1vo" alt="avatar"></p>

<p>我们下面还是回到 kubectl 这个 commnd 来执行这次 Deployment 的真正的操作。我们可以看到一个简单的操作，就会去让 Deployment 不停地生成副本。</p>

<p><img src="assets/FlI67d3ZPrD37qNRdaNSrFE4xjfQ" alt="avatar"></p>

<p>Deployment 副本数目是 2 个，下面也可以 describe 一下现在的 Deployment 的状态。我们知道之前是没有这个 Deployment 的，现在我们去 describe 这个 nginx-deployment。</p>

<p>下图中可以看到：有一个 nginx-deployment 已经被生成了，它的 replicas 数目也是我们想要的、selector 也是我们想要的、它的 image 的版本也是 1.7.9。还可以看到，里面的 deployment-controller 这种版本控制器也是在管理它的生成。</p>

<p><img src="assets/FkZlCITJk0N8siIaXG1QDncJYsD4" alt="avatar"></p>

<p>下面我们去升级这个 Deployment 版本，首先下载另外一个 yaml 文件 deployment-update.yaml，可以看到这里面的 image 本身的版本号从 1.7.9 升级到 1.8。</p>

<p><img src="assets/FugYlYr8kTTTjDsmjNxsL181ezUy" alt="avatar"></p>

<p>接下来我们重新 apply 新的 deployment-update 这个 yaml 文件。</p>

<p>可以看到，在另一边的屏幕上显示出了这个 Deployment 升级的一些操作，最终它的 up-to-date 值从 0 变成了 2，也就是说所有的容器都是最新版本的，所有的 Pod 都是最新版本的。我们也可以 discribe 具体去看一下是不是所有 Pod 的版本都被更新了，可以看到这个 image 的版本由 1.7.9 真正更新到了 1.8。</p>

<p>最后，我们也可以看到  controller 又执行了几次新的操作，这个控制器维护了整个 Deployment 和 Pod 状态。</p>

<p><img src="assets/FjpIzqw7CKmfaIGlK8BhkfKhN4E3" alt="avatar"></p>

<p>最后我们演示一下给 Deployment 做水平扩张，下载另一个 yaml 文件 deployment-scale.yaml，这里面的 replicas 数目已经从 2 改成了 4。</p>

<p><img src="assets/FmVI-oaSb0TJLObDGahsgdLZaOjV" alt="avatar"></p>

<p>回到最开始的窗口，用 kubectl 去 apply 这个新的 deployment-scale.yaml 文件，在另外一个窗口上可以看到，当我们执行了 deployment-scale 操作之后，它的容器 Pod 数目从 2 变成了 4。我们可以再一次 describ 一下当前集群中的 deployment 的情况，可以看到它的 replicas 的数目从 2 变到了 4，同时也可以看到 controller 又做了几次新的操作，这个 scale up 成功了。</p>

<p><img src="assets/FuN37XsKiIT3WOYU5QNNIigMm1Ii" alt="avatar"></p>

<p>最后，让我们利用 delete 操作把我们刚才生成的 Deployment 给删除掉。kubectl delete deployment，也是刚才我们本身的 deployment name，当我们把它删除掉之后，我们今天所有的操作就完成了。</p>

<p>我们再去重新 get 这个 Deployment，也会显示这个资源不再存在，这个集群又回到了最开始干净的状态。</p>

<p><img src="assets/Ft0eDGSed6lpc2TROivAJNV4wKpm" alt="avatar"></p>

<p>以上这就是这堂课中所有的内容了，我们关注了 kubernetes 的核心概念以及 kubernetes 的架构设计，希望大家能在这节课中有所收获，也希望大家能关注云原生技术课堂中的其他内容，谢谢大家的观看！</p>

<h2>本节总结</h2>

<ul>

<li>Kubernetes 是一个自动化的容器编排平台，它负责应用的部署、应用的弹性以及应用的管理，这些都是基于容器的；</li>

<li>Kubernetes 架构是一个比较典型的二层架构和 server-client 架构。</li>

</ul>

</div>

                    </div><div class="book-post">

                        <p id="tip" align="center"></p>

                        <div><h1>04 理解 Pod 和容器设计模式</h1>

<h2>为什么需要 Pod</h2>

<h3>容器的基本概念</h3>

<p>现在来看第一个问题：为什么需要 Pod？我们知道 Pod 是 Kubernetes 项目里面一个非常重要的概念，也是非常重要的一个原子调度单位，但是为什么我们会需要这样一个概念呢？我们在使用容器 Docker 的时候，也没有这个说法。其实如果要理解 Pod，我们首先要理解容器，所以首先来回顾一下容器的概念：</p>

<p><strong>容器的本质实际上是一个进程，是一个视图被隔离，资源受限的进程。</strong></p>

<p>容器里面 PID=1 的进程就是应用本身，这意味着管理虚拟机等于管理基础设施，因为我们是在管理机器，但管理容器却等于直接管理应用本身。这也是之前说过的不可变基础设施的一个最佳体现，这个时候，你的应用就等于你的基础设施，它一定是不可变的。</p>

<p>在以上面的例子为前提的情况下，Kubernetes 又是什么呢？我们知道，很多人都说 Kubernetes 是云时代的操作系统，这个非常有意思，因为如果以此类推，容器镜像就是这个操作系统的软件安装包，它们之间是这样的一个类比关系。</p>

<p><img src="assets/FtCLIkCogVpWbPlVEuyw-Qi1OgBV" alt="avatar"></p>

<h3>真实操作系统里的例子</h3>

<p>如果说 Kubernetes 就是操作系统的话，那么我们不妨看一下真实的操作系统的例子。</p>

<p>例子里面有一个程序叫做 Helloworld，这个 Helloworld 程序实际上是由一组进程组成的，需要注意一下，这里说的进程实际上等同于 Linux 中的线程。</p>

<p>因为 Linux 中的线程是轻量级进程，所以如果从 Linux 系统中去查看 Helloworld 中的 pstree，将会看到这个 Helloworld 实际上是由四个线程组成的，分别是 <strong>{api、main、log、compute}</strong>。也就是说，四个这样的线程共同协作，共享 Helloworld 程序的资源，组成了 Helloworld 程序的真实工作情况。</p>

<p>这是操作系统里面进程组或者线程组中一个非常真实的例子，以上就是进程组的一个概念。</p>

<p><img src="assets/FnFxtuelVNca-K42uGiFCVShsJKp" alt="avatar"></p>

<p>那么大家不妨思考一下，在真实的操作系统里面，一个程序往往是根据进程组来进行管理的。Kubernetes 把它类比为一个操作系统，比如说 Linux。针对于容器我们前面提到可以类比为进程，就是前面的 Linux 线程。那么 Pod 又是什么呢？实际上 Pod 就是我们刚刚提到的进程组，也就是 Linux 里的线程组。</p>

<h3>进程组概念</h3>

<p>说到进程组，首先建议大家至少有个概念上的理解，然后我们再详细的解释一下。</p>

<p>还是前面那个例子：Helloworld 程序由四个进程组成，这些进程之间会共享一些资源和文件。那么现在有一个问题：假如说现在把 Helloworld 程序用容器跑起来，你会怎么去做？</p>

<p>当然，最自然的一个解法就是，我现在就启动一个 Docker 容器，里面运行四个进程。可是这样会有一个问题，这种情况下容器里面 PID=1 的进程该是谁? 比如说，它应该是我的 main 进程，那么问题来了，“谁”又负责去管理剩余的 3 个进程呢？</p>

<p>这个核心问题在于，容器的设计本身是一种“单进程”模型，不是说容器里只能起一个进程，由于容器的应用等于进程，所以只能去管理 PID=1 的这个进程，其他再起来的进程其实是一个托管状态。 所以说服务应用进程本身就具有“进程管理”的能力。</p>

<p>比如说 Helloworld 的程序有 system 的能力，或者直接把容器里 PID=1 的进程直接改成 systemd，否则这个应用，或者是容器是没有办法去管理很多个进程的。因为 PID=1 进程是应用本身，如果现在把这个 PID=1 的进程给 kill 了，或者它自己运行过程中死掉了，那么剩下三个进程的资源就没有人回收了，这个是非常非常严重的一个问题。</p>

<p>而反过来真的把这个应用本身改成了 systemd，或者在容器里面运行了一个 systemd，将会导致另外一个问题：使得管理容器，不再是管理应用本身了，而等于是管理 systemd，这里的问题就非常明显了。比如说我这个容器里面 run 的程序或者进程是 systemd，那么接下来，这个应用是不是退出了？是不是 fail 了？是不是出现异常失败了？实际上是没办法直接知道的，因为容器管理的是 systemd。这就是为什么在容器里面运行一个复杂程序往往比较困难的一个原因。</p>

<p>这里再帮大家梳理一下：<strong>由于容器实际上是一个“单进程”模型</strong>，所以如果你在容器里启动多个进程，只有一个可以作为 PID=1 的进程，而这时候，如果这个 PID=1 的进程挂了，或者说失败退出了，那么其他三个进程就会自然而然的成为孤儿，没有人能够管理它们，没有人能够回收它们的资源，这是一个非常不好的情况。</p>

<blockquote>

<p>注意：Linux 容器的“单进程”模型，指的是容器的生命周期等同于 PID=1 的进程（容器应用进程）的生命周期，而不是说容器里不能创建多进程。当然，一般情况下，容器应用进程并不具备进程管理能力，所以你通过 exec 或者 ssh 在容器里创建的其他进程，一旦异常退出（比如 ssh 终止）是很容易变成孤儿进程的。</p>

</blockquote>

<p>反过来，其实可以在容器里面 run 一个 systemd，用它来管理其他所有的进程。这样会产生第二个问题：实际上没办法直接管理我的应用了，因为我的应用被 systemd 给接管了，那么这个时候应用状态的生命周期就不等于容器生命周期。这个管理模型实际上是非常非常复杂的。</p>

<p><img src="assets/FhGvM2yWfWMEVfIXWnDGgGn45sh4" alt="avatar"></p>

<h3>Pod = “进程组”</h3>

<p>在 kubernetes 里面，Pod 实际上正是 kubernetes 项目为你抽象出来的一个可以类比为进程组的概念。</p>

<p>前面提到的，由四个进程共同组成的一个应用 Helloworld，在 Kubernetes 里面，实际上会被定义为一个拥有四个容器的 Pod，这个概念大家一定要非常仔细的理解。</p>

<p>就是说现在有四个职责不同、相互协作的进程，需要放在容器里去运行，在 Kubernetes 里面并不会把它们放到一个容器里，因为这里会遇到两个问题。那么在 Kubernetes 里会怎么去做呢？它会把四个独立的进程分别用四个独立的容器启动起来，然后把它们定义在一个 Pod 里面。</p>

<p>所以当 Kubernetes 把 Helloworld 给拉起来的时候，你实际上会看到四个容器，它们共享了某些资源，这些资源都属于 Pod，所以我们说 Pod 在 Kubernetes 里面只有一个逻辑单位，没有一个真实的东西对应说这个就是 Pod，不会有的。真正起来在物理上存在的东西，就是四个容器。这四个容器，或者说是多个容器的组合就叫做 Pod。并且还有一个概念一定要非常明确，Pod 是 Kubernetes 分配资源的一个单位，因为里面的容器要共享某些资源，所以 Pod 也是 Kubernetes 的原子调度单位。</p>

<p><img src="assets/FqzzX5i2imAnkOMeUQtiLkBal5wO" alt="avatar"></p>

<p>上面提到的 Pod 设计，也不是 Kubernetes 项目自己想出来的， 而是早在 Google 研发 Borg 的时候，就已经发现了这样一个问题。这个在 Borg paper 里面有非常非常明确的描述。简单来说 Google 工程师发现在 Borg 下面部署应用时，很多场景下都存在着类似于“进程与进程组”的关系。更具体的是，这些应用之前往往有着密切的协作关系，使得它们必须部署在同一台机器上并且共享某些信息。</p>

<p>以上就是进程组的概念，也是 Pod 的用法。</p>

<h3>为什么 Pod 必须是原子调度单位？</h3>

<p>可能到这里大家会有一些问题：虽然了解这个东西是一个进程组，但是为什么要把 Pod 本身作为一个概念抽象出来呢？或者说能不能通过调度把 Pod 这个事情给解决掉呢？为什么 Pod 必须是 Kubernetes 里面的原子调度单位？</p>

<p>下面我们通过一个例子来解释。</p>

<p>假如现在有两个容器，它们是紧密协作的，所以它们应该被部署在一个 Pod 里面。具体来说，第一个容器叫做 App，就是业务容器，它会写日志文件；第二个容器叫做 LogCollector，它会把刚刚 App 容器写的日志文件转发到后端的 ElasticSearch 中。</p>

<p>两个容器的资源需求是这样的：App 容器需要 1G 内存，LogCollector 需要 0.5G 内存，而当前集群环境的可用内存是这样一个情况：Node<em>A：1.25G 内存，Node</em>B：2G 内存。</p>

<p>假如说现在没有 Pod 概念，就只有两个容器，这两个容器要紧密协作、运行在一台机器上。可是，如果调度器先把 App 调度到了 Node<em>A 上面，接下来会怎么样呢？这时你会发现：LogCollector 实际上是没办法调度到 Node</em>A 上的，因为资源不够。其实此时整个应用本身就已经出问题了，调度已经失败了，必须去重新调度。</p>

<p><img src="assets/FkuVM-7l2GEyyNjeBLBOTxpQ6TEQ" alt="avatar"></p>

<p>以上就是一个非常典型的成组调度失败的例子。英文叫做：Task co-scheduling 问题，这个问题不是说不能解，在很多项目里面，这样的问题都有解法。</p>

<p>比如说在 Mesos 里面，它会做一个事情，叫做资源囤积（resource hoarding）：即当所有设置了 Affinity 约束的任务都达到时，才开始统一调度，这是一个非常典型的成组调度的解法。</p>

<p>所以上面提到的“App”和“LogCollector”这两个容器，在 Mesos 里面，他们不会说立刻调度，而是等两个容器都提交完成，才开始统一调度。这样也会带来新的问题，首先调度效率会损失，因为需要等待。由于需要等还会有外一个情况会出现，就是产生死锁，就是互相等待的一个情况。这些机制在 Mesos 里都是需要解决的，也带来了额外的复杂度。</p>

<p>另一种解法是 Google 的解法。它在 Omega 系统（就是 Borg 下一代）里面，做了一个非常复杂且非常厉害的解法，叫做乐观调度。比如说：不管这些冲突的异常情况，先调度，同时设置一个非常精妙的回滚机制，这样经过冲突后，通过回滚来解决问题。这个方式相对来说要更加优雅，也更加高效，但是它的实现机制是非常复杂的。这个有很多人也能理解，就是悲观锁的设置一定比乐观锁要简单。</p>

<p>而像这样的一个 Task co-scheduling 问题，在 Kubernetes 里，就直接通过 Pod 这样一个概念去解决了。因为在 Kubernetes 里，这样的一个 App 容器和 LogCollector 容器一定是属于一个 Pod 的，它们在调度时必然是以一个 Pod 为单位进行调度，所以这个问题是根本不存在的。</p>

<h3>再次理解 Pod</h3>

<p>在讲了前面这些知识点之后，我们来再次理解一下 Pod，首先 Pod 里面的容器是“超亲密关系”。</p>

<p>这里有个“超”字需要大家理解，正常来说，有一种关系叫做亲密关系，这个亲密关系是一定可以通过调度来解决的。</p>

<p><img src="assets/FukG5VrC0Q7k4_5uB3vstrF687k8" alt="avatar"></p>

<p>比如说现在有两个 Pod，它们需要运行在同一台宿主机上，那这样就属于亲密关系，调度器一定是可以帮助去做的。但是对于超亲密关系来说，有一个问题，即它必须通过 Pod 来解决。因为如果超亲密关系赋予不了，那么整个 Pod 或者说是整个应用都无法启动。</p>

<p>什么叫做超亲密关系呢？大概分为以下几类：</p>

<ul>

<li>比如说两个进程之间会发生文件交换，前面提到的例子就是这样，一个写日志，一个读日志；</li>

<li>两个进程之间需要通过 localhost 或者说是本地的 Socket 去进行通信，这种本地通信也是超亲密关系；</li>

<li>这两个容器或者是微服务之间，需要发生非常频繁的 RPC 调用，出于性能的考虑，也希望它们是超亲密关系；</li>

<li>两个容器或者是应用，它们需要共享某些 Linux Namespace。最简单常见的一个例子，就是我有一个容器需要加入另一个容器的 Network Namespace。这样我就能看到另一个容器的网络设备，和它的网络信息。</li>

</ul>

<p>像以上几种关系都属于超亲密关系，它们都是在 Kubernetes 中会通过 Pod 的概念去解决的。</p>

<p>现在我们理解了 Pod 这样的概念设计，理解了为什么需要 Pod。它解决了两个问题：</p>

<ol>

<li>我们怎么去描述超亲密关系；</li>

<li>我们怎么去对超亲密关系的容器或者说是业务去做统一调度，这是 Pod 最主要的一个诉求。</li>

</ol>

<h2>Pod 的实现机制</h2>

<h3>Pod 要解决的问题</h3>

<p>像 Pod 这样一个东西，本身是一个逻辑概念。那在机器上，它究竟是怎么实现的呢？这就是我们要解释的第二个问题。</p>

<p>既然说 Pod 要解决这个问题，核心就在于如何让一个 Pod 里的多个容器之间最高效的共享某些资源和数据。</p>

<p>因为容器之间原本是被 Linux Namespace 和 cgroups 隔开的，所以现在实际要解决的是怎么去打破这个隔离，然后共享某些事情和某些信息。这就是 Pod 的设计要解决的核心问题所在。</p>

<p><strong>所以说具体的解法分为两个部分：网络和存储。</strong></p>

<h4>共享网络</h4>

<p>第一个问题是 Pod 里的多个容器怎么去共享网络？下面是个例子：</p>

<p>比如说现在有一个 Pod，其中包含了一个容器 A 和一个容器 B，它们两个就要共享 Network Namespace。在 Kubernetes 里的解法是这样的：它会在每个 Pod 里，额外起一个 Infra container 小容器来共享整个 Pod 的  Network Namespace。</p>

<p>Infra container 是一个非常小的镜像，大概 100~200KB 左右，是一个汇编语言写的、永远处于“暂停”状态的容器。由于有了这样一个 Infra container 之后，其他所有容器都会通过 Join Namespace 的方式加入到 Infra container 的 Network Namespace 中。</p>

<p>所以说一个 Pod 里面的所有容器，它们看到的网络视图是完全一样的。即：它们看到的网络设备、IP地址、Mac地址等等，跟网络相关的信息，其实全是一份，这一份都来自于 Pod 第一次创建的这个 Infra container。这就是 Pod 解决网络共享的一个解法。</p>

<p>在 Pod 里面，一定有一个 IP 地址，是这个 Pod 的 Network Namespace 对应的地址，也是这个 Infra container 的 IP 地址。所以大家看到的都是一份，而其他所有网络资源，都是一个 Pod 一份，并且被 Pod 中的所有容器共享。这就是 Pod 的网络实现方式。</p>

<p>由于需要有一个相当于说中间的容器存在，所以整个 Pod 里面，必然是 Infra container 第一个启动。并且整个 Pod 的生命周期是等同于 Infra container 的生命周期的，与容器 A 和 B 是无关的。这也是为什么在 Kubernetes 里面，它是允许去单独更新 Pod 里的某一个镜像的，即：做这个操作，整个 Pod 不会重建，也不会重启，这是非常重要的一个设计。</p>

<p><img src="assets/Ft6RFUZb1d6RwSUmzi2xWHPGUnNI" alt="avatar"></p>

<h4>共享存储</h4>

<p>第二问题：Pod 怎么去共享存储？Pod 共享存储就相对比较简单。</p>

<p>比如说现在有两个容器，一个是 Nginx，另外一个是非常普通的容器，在 Nginx 里放一些文件，让我能通过 Nginx 访问到。所以它需要去 share 这个目录。我 share 文件或者是 share 目录在 Pod 里面是非常简单的，实际上就是把 volume 变成了 Pod level。然后所有容器，就是所有同属于一个 Pod 的容器，他们共享所有的 volume。</p>

<p><img src="assets/Flqk0RrFG3EEh1pq4LTNOkhxlgM8" alt="avatar"></p>

<p>比如说上图的例子，这个 volume 叫做 shared-data，它是属于 Pod level 的，所以在每一个容器里可以直接声明：要挂载 shared-data 这个 volume，只要你声明了你挂载这个 volume，你在容器里去看这个目录，实际上大家看到的就是同一份。这个就是 Kubernetes 通过 Pod 来给容器共享存储的一个做法。</p>

<p>所以在之前的例子中，应用容器 App 写了日志，只要这个日志是写在一个 volume 中，只要声明挂载了同样的 volume，这个 volume 就可以立刻被另外一个 LogCollector 容器给看到。以上就是 Pod 实现存储的方式。</p>

<h2>详解容器设计模式</h2>

<p>现在我们知道了为什么需要 Pod，也了解了 Pod 这个东西到底是怎么实现的。最后，以此为基础，详细介绍一下 Kubernetes 非常提倡的一个概念，叫做容器设计模式。</p>

<h3>举例</h3>

<p>接下来将会用一个例子来给大家进行讲解。</p>

<p>比如我现在有一个非常常见的一个诉求：我现在要发布一个应用，这个应用是 JAVA 写的，有一个 WAR 包需要把它放到 Tomcat 的 web APP 目录下面，这样就可以把它启动起来了。可是像这样一个 WAR 包或 Tomcat 这样一个容器的话，怎么去做，怎么去发布？这里面有几种做法。</p>

<p><img src="assets/FrYExOsbHoig5-2WLJByhSR25Z9n" alt="avatar"></p>

<ul>

<li>第一种方式：可以把 WAR 包和 Tomcat 打包放进一个镜像里面。但是这样带来一个问题，就是现在这个镜像实际上揉进了两个东西。那么接下来，无论是我要更新 WAR 包还是说我要更新 Tomcat，都要重新做一个新的镜像，这是比较麻烦的；</li>

<li>第二种方式：就是镜像里面只打包 Tomcat。它就是一个 Tomcat，但是需要使用数据卷的方式，比如说 hostPath，从宿主机上把 WAR 包挂载进我们 Tomcat 容器中，挂到我的 web APP 目录下面，这样把这个容器启用起来之后，里面就能用了。</li>

</ul>

<p>但是这时会发现一个问题：这种做法一定需要维护一套分布式存储系统。因为这个容器可能第一次启动是在宿主机 A 上面，第二次重新启动就可能跑到 B 上去了，容器它是一个可迁移的东西，它的状态是不保持的。所以必须维护一套分布式存储系统，使容器不管是在 A 还是在 B 上，都可以找到这个 WAR 包，找到这个数据。</p>

<blockquote>

<p>注意，即使有了分布式存储系统做 Volume，你还需要负责维护 Volume 里的 WAR 包。比如：你需要单独写一套 Kubernetes Volume 插件，用来在每次 Pod 启动之前，把应用启动所需的 WAR 包下载到这个 Volume 里，然后才能被应用挂载使用到。</p>

</blockquote>

<p>这样操作带来的复杂程度还是比较高的，且这个容器本身必须依赖于一套持久化的存储插件（用来管理 Volume 里的 WAR 包内容）。</p>

<h3>InitContainer</h3>

<p>所以大家有没有考虑过，像这样的组合方式，有没有更加通用的方法？哪怕在本地 Kubernetes 上，没有分布式存储的情况下也能用、能玩、能发布。</p>

<p>实际上方法是有的，在 Kubernetes 里面，像这样的组合方式，叫做 Init Container。</p>

<p><img src="assets/Fm6Z2IByCbIWWwpZCb8LVL9jLeOk" alt="avatar"></p>

<p>还是同样一个例子：在上图的 yaml 里，首先定义一个 Init Container，它只做一件事情，就是把 WAR 包从镜像里拷贝到一个 Volume 里面，它做完这个操作就退出了，所以 Init Container 会比用户容器先启动，并且严格按照定义顺序来依次执行。</p>

<p>然后，这个关键在于刚刚拷贝到的这样一个目的目录：APP 目录，实际上是一个 Volume。而我们前面提到，一个 Pod 里面的多个容器，它们是可以共享 Volume 的，所以现在这个 Tomcat 容器，只是打包了一个 Tomcat 镜像。但在启动的时候，要声明使用 APP 目录作为我的 Volume，并且要把它们挂载在 Web APP 目录下面。</p>

<p>而这个时候，由于前面已经运行过了一个 Init Container，已经执行完拷贝操作了，所以这个 Volume 里面已经存在了应用的 WAR 包：就是 sample.war，绝对已经存在这个 Volume 里面了。等到第二步执行启动这个 Tomcat 容器的时候，去挂这个 Volume，一定能在里面找到前面拷贝来的 sample.war。</p>

<p>所以可以这样去描述：这个 Pod 就是一个自包含的，可以把这一个 Pod 在全世界任何一个 Kubernetes 上面都顺利启用起来。不用担心没有分布式存储、Volume 不是持久化的，它一定是可以公布的。</p>

<p>所以这是一个通过组合两个不同角色的容器，并且按照这样一些像 Init Container 这样一种编排方式，统一的去打包这样一个应用，把它用 Pod 来去做的非常典型的一个例子。像这样的一个概念，在 Kubernetes 里面就是一个非常经典的容器设计模式，叫做：“Sidecar”。</p>

<h3>容器设计模式：Sidecar</h3>

<p>什么是 Sidecar？就是说其实在 Pod 里面，可以定义一些专门的容器，来执行主业务容器所需要的一些辅助工作，比如我们前面举的例子，其实就干了一个事儿，这个 Init Container，它就是一个 Sidecar，它只负责把镜像里的 WAR 包拷贝到共享目录里面，以便被 Tomcat 能够用起来。</p>

<p>其它有哪些操作呢？比如说：</p>

<ul>

<li>原本需要在容器里面执行 SSH 需要干的一些事情，可以写脚本、一些前置的条件，其实都可以通过像 Init Container 或者另外像 Sidecar 的方式去解决；</li>

<li>当然还有一个典型例子就是我的日志收集，日志收集本身是一个进程，是一个小容器，那么就可以把它打包进 Pod 里面去做这个收集工作；</li>

<li>还有一个非常重要的东西就是 Debug 应用，实际上现在 Debug 整个应用都可以在应用 Pod 里面再次定义一个额外的小的 Container，它可以去 exec 应用 pod 的 namespace；</li>

<li>查看其他容器的工作状态，这也是它可以做的事情。不再需要去 SSH 登陆到容器里去看，只要把监控组件装到额外的小容器里面就可以了，然后把它作为一个 Sidecar 启动起来，跟主业务容器进行协作，所以同样业务监控也都可以通过 Sidecar 方式来去做。</li>

</ul>

<p>这种做法一个非常明显的优势就是在于其实将辅助功能从我的业务容器解耦了，所以我就能够独立发布 Sidecar 容器，并且更重要的是这个能力是可以重用的，即同样的一个监控 Sidecar 或者日志 Sidecar，可以被全公司的人共用的。这就是设计模式的一个威力。</p>

<p><img src="assets/FhxZpYP2nTou9k0ggnxZ_j6EYNvR" alt="avatar"></p>

<h4>Sidecar：应用与日志收集</h4>

<p>接下来，我们再详细细化一下 Sidecar 这样一个模式，它还有一些其他的场景。</p>

<p>比如说前面提到的应用日志收集，业务容器将日志写在一个 Volume 里面，而由于 Volume 在 Pod 里面是被共享的，所以日志容器 —— 即 Sidecar 容器一定可以通过共享该 Volume，直接把日志文件读出来，然后存到远程存储里面，或者转发到另外一个例子。现在业界常用的 Fluentd 日志进程或日志组件，基本上都是这样的工作方式。</p>

<p><img src="assets/Fs8v2oOG-5g0o9JoVkUrxgk1boRm" alt="avatar"></p>

<h4>Sidecar：代理容器</h4>

<p>Sidecar 的第二个用法，可以称作为代理容器 Proxy。什么叫做代理容器呢？</p>

<p>假如现在有个 Pod 需要访问一个外部系统，或者一些外部服务，但是这些外部系统是一个集群，那么这个时候如何通过一个统一的、简单的方式，用一个 IP 地址，就把这些集群都访问到？有一种方法就是：修改代码。因为代码里记录了这些集群的地址；另外还有一种解耦的方法，即通过 Sidecar 代理容器。</p>

<p>简单说，单独写一个这么小的 Proxy，用来处理对接外部的服务集群，它对外暴露出来只有一个 IP 地址就可以了。所以接下来，业务容器主要访问 Proxy，然后由 Proxy 去连接这些服务集群，这里的关键在于 Pod 里面多个容器是通过 localhost 直接通信的，因为它们同属于一个 network Namespace，网络视图都一样，所以它们俩通信 localhost，并没有性能损耗。</p>

<p>所以说代理容器除了做了解耦之外，并不会降低性能，更重要的是，像这样一个代理容器的代码就又可以被全公司重用了。</p>

<p><img src="assets/FsSwJgFCqPpWQoebX5UY3a7zHHED" alt="avatar"></p>

<h4>Sidecar：适配器容器</h4>

<p>Sidecar 的第三个设计模式 —— 适配器容器 Adapter，什么叫 Adapter 呢？</p>

<p>现在业务暴露出来的 API，比如说有个 API 的一个格式是 A，但是现在有一个外部系统要去访问我的业务容器，它只知道的一种格式是 API B ,所以要做一个工作，就是把业务容器怎么想办法改掉，要去改业务代码。但实际上，你可以通过一个 Adapter 帮你来做这层转换。</p>

<p><img src="assets/FuVvWIzvVyBA_ugdQCMJUqqtrP2T" alt="avatar"></p>

<p>现在有个例子：现在业务容器暴露出来的监控接口是 /metrics，访问这个这个容器的 metrics 的这个 URL 就可以拿到了。可是现在，这个监控系统升级了，它访问的 URL 是 /health，我只认得暴露出 health 健康检查的 URL，才能去做监控，metrics 不认识。那这个怎么办？那就需要改代码了，但可以不去改代码，而是额外写一个 Adapter，用来把所有对 health 的这个请求转发给 metrics 就可以了，所以这个 Adapter 对外暴露的是 health 这样一个监控的 URL，这就可以了，你的业务就又可以工作了。</p>

<p>这样的关键还在于 Pod 之中的容器是通过 localhost 直接通信的，所以没有性能损耗，并且这样一个 Adapter 容器可以被全公司重用起来，这些都是设计模式给我们带来的好处。</p>

<h2>本节总结</h2>

<ul>

<li>Pod 是 Kubernetes 项目里实现“容器设计模式”的核心机制；</li>

<li>“容器设计模式”是 Google Borg 的大规模容器集群管理最佳实践之一，也是 Kubernetes 进行复杂应用编排的基础依赖之一；</li>

<li>所有“设计模式”的本质都是：解耦和重用。</li>

</ul>

</div>

                    </div><div class="book-post">

                        <p id="tip" align="center"></p>

                        <div><h1>05 应用编排与管理：核心原理</h1>

<h2>资源元信息</h2>

<h3>1. Kubernetes 资源对象</h3>

<p>首先，我们来回顾一下 Kubernetes 的资源对象组成：主要包括了 Spec、Status 两部分。其中 Spec 部分用来描述期望的状态，Status 部分用来描述观测到的状态。</p>

<p>今天我们将为大家介绍 K8s 的另外一个部分，即元数据部分。该部分主要包括了用来识别资源的标签：Labels， 用来描述资源的注解；Annotations， 用来描述多个资源之间相互关系的 OwnerReference。这些元数据在 K8s 运行中有非常重要的作用。后续课程中将会反复讲到。</p>

<h3>2. labels</h3>

<p>第一个元数据，也是最重要的一个元数据是：资源标签。资源标签是一种具有标识型的 Key：Value 元数据，这里展示了几个常见的标签。</p>

<p>前三个标签都打在了 Pod 对象上，分别标识了对应的应用环境、发布的成熟度和应用的版本。从应用标签的例子可以看到，标签的名字包括了一个域名的前缀，用来描述打标签的系统和工具， 最后一个标签打在 Node 对象上，还在域名前增加了版本的标识 beta 字符串。</p>

<p>标签主要用来筛选资源和组合资源，可以使用类似于 SQL 查询 select，来根据 Label 查询相关的资源。</p>

<p><img src="assets/FuRl-mt3gUv2ImgBRt-J2tecWbdw" alt="avatar"></p>

<h3>3. Selector</h3>

<p>最常见的 Selector 就是相等型 Selector。现在举一个简单的例子：</p>

<p>假设系统中有四个 Pod，每个 Pod 都有标识系统层级和环境的标签，我们通过 Tie：front 这个标签，可以匹配左边栏的 Pod，相等型 Selector 还可以包括多个相等条件，多个相等条件之间是逻辑”与“的关系。</p>

<p>在刚才的例子中，通过 Tie=front,Env=dev 的Selector，我们可以筛选出所有 Tie=front，而且 Env=dev 的 Pod，也就是下图中左上角的 Pod。另外一种 Selector 是集合型 Selector，在例子中，Selector 筛选所有环境是 test 或者 gray 的 Pod。</p>

<p>除了 in 的集合操作外，还有 notin 集合操作，比如 tie notin（front,back），将会筛选所有 tie 不是 front 且不是 back 的 Pod。另外，也可以根据是否存在某 lable 的筛选，如：Selector release，筛选所有带 release 标签的 Pod。集合型和相等型的 Selector，也可以用“，”来连接，同样的标识逻辑”与“的关系。</p>

<p><img src="assets/Fuf8da-tm5zukqsZ8VQuo_AnGGlA" alt="avatar"></p>

<h3>4. Annotations</h3>

<p>另外一种重要的元数据是：annotations。一般是系统或者工具用来存储资源的非标示性信息，可以用来扩展资源的 spec/status 的描述，这里给了几个 annotations 的例子：</p>

<p>第一个例子，存储了阿里云负载器的证书 ID，我们可以看到 annotations 一样可以拥有域名的前缀，标注中也可以包含版本信息。第二个 annotation存储了 nginx 接入层的配置信息，我们可以看到 annotations 中包括“，”这样无法出现在 label 中的特殊字符。第三个 annotations 一般可以在 kubectl apply 命令行操作后的资源中看到， annotation 值是一个结构化的数据，实际上是一个 json 串，标记了上一次 kubectl 操作的资源的 json 的描述。</p>

<p><img src="assets/FrM0KI5aZdRn65i2DCQtowldEmYs" alt="avatar"></p>

<h3>5. Ownereference</h3>

<p>我们当时讲到最后一个元数据叫做 Ownereference，所谓所有者，一般就是指集合类的资源，比如说 Pod 集合，就有 replicaset、statefulset，这个将在后序的课程中讲到。</p>

<p>集合类资源的控制器会创建对应的归属资源。比如：replicaset 控制器在操作中会创建 Pod，被创建 Pod 的 Ownereference 就指向了创建 Pod 的 replicaset，Ownereference 使得用户可以方便地查找一个创建资源的对象，另外，还可以用来实现级联删除的效果。** **</p>

<h2>操作演示</h2>

<p>这里通过 kubectl 命令去连接我们 ACK 中已经创建好的一个 K8s 集群，然后来展示一下怎么查看和修改 K8s 对象中的元数据，主要就是 Pod 的一个标签、注解，还有对应的 Ownerference。</p>

<p>首先我们看一下集群里现在的配置情况：</p>

<p>查看 Pod，现在没有任何的一个 Pod；</p>

<ul>

<li><strong>kubectl get pods</strong></li>

</ul>

<p>然后用事先准备好的一个 Pod 的 yaml，创建一个 Pod 出来；</p>

<ul>

<li><strong>kubectl apply -f pod1.yaml</strong></li>

<li><strong>kubectl apply -f pod2.yaml</strong></li>

</ul>

<p>现在查看一下 Pod 打的标签，我们用 --show-labels 这个选项，可以看到这两个 Pod 都打上了一个部署环境和层级的标签；</p>

<ul>

<li><strong>kubectl get pods —show-labels</strong></li>

</ul>

<p>我们也可以通过另外一种方式来查看具体的资源信息。首先查看 nginx1 第一个 Pod 的一个信息，用 -o yaml 的方式输出，可以看到这个 Pod 元数据里面包括了一个 lables 的字段，里面有两个 lable；</p>

<ul>

<li><strong>kubectl get pods nginx1 -o yaml | less</strong></li>

</ul>

<p>现在再想一下，怎么样对 Pod 已有的 lable 进行修改？我们先把它的部署环境，从开发环境改成测试环境，然后指定 Pod 名字，在环境再加上它的一个值 test ，看一下能不能成功。 这里报了一个错误，可以看到，它其实是说现在这个 label 已经有值了；</p>

<ul>

<li><strong>kubectl label pods nginx1 env=test</strong></li>

</ul>

<p>如果想覆盖掉它的话，得额外再加上一个覆盖的选项。加上之后呢，我们应该可以看到这个打标已经成功了；</p>

<ul>

<li><strong>kubectl label pods nginx1 env=test —overwrite</strong></li>

</ul>

<p>我们再看一下现在集群的 lable 设置情况，首先可以看到 nginx1 的确已经加上了一个部署环境 test 标签；</p>

<ul>

<li><strong>kubectl get pods —show-labels</strong></li>

</ul>

<p>如果想要对 Pod 去掉一个标签，也是跟打标签一样的操作，但是 env 后就不是等号了。只加上 label 名字，后面不加等号，改成用减号表示去除 label 的 k:v；</p>

<ul>

<li><strong>kubectl label pods nginx tie-</strong></li>

</ul>

<p>可以看到这个 label，去标已经完全成功；</p>

<ul>

<li><strong>kubectl get pods —show-labels</strong></li>

</ul>

<p><img src="assets/FnCd1Jh9Hmtve4kTKUHalizeN2Ib" alt="avatar"></p>

<p>下面来看一下配置的 label 值，的确能看到 nginx1 的这个 Pod 少了一个 tie=front 的标签。有了这个 Pod 标签之后，可以看一下怎样用 label Selector 进行匹配？首先 label Selector 是通过 -l 这个选项来进行指定的 ，指定的时候，先试一下用相等型的一个 label 来筛选，所以我们指定的是部署环境等于测试的一个 Pod，我们可以看到能够筛选出一台；</p>

<ul>

<li><strong>kubectl get pods —show-labels -l env=test</strong></li>

</ul>

<p>假如说有多个相等的条件需要指定的，实际上这是一个与的关系，假如说 env 再等于 dev，我们实际上是一个 Pod 都拿不到的；</p>

<ul>

<li><strong>kubectl get pods —show-labels -l env=test,env=dev</strong></li>

</ul>

<p>然后假如说 env=dev，但是 tie=front，我们能够匹配到第二个 Pod，也就是 nginx2；</p>

<ul>

<li><strong>kubectl get pods —show-labels -l env=dev,tie=front</strong></li>

</ul>

<p>我们还可以再试一下怎么样用集合型的 label Selector 来进行筛选。这一次我们还是想要匹配出所有部署环境是 test 或者是 dev 的一个 Pod，所以在这里加上一个引号，然后在括号里面指定所有部署环境的一个集合。这次能把两个创建的 Pod 都筛选出来；</p>

<ul>

<li><strong>kubectl get pods —show-labels -l ’env in (dev,test)’</strong></li>

</ul>

<p>我们再试一下怎样对 Pod 增加一个注解，注解的话，跟打标是一样的操作，但是把 label 命令改成 annotate 命令；然后，一样指定类型和对应的名字。后面就不是加上 label 的 k:v 了，而是加上 annotation 的 k:v。这里我们可以指定一个任意的字符串，比如说加上空格、加上逗号都可以；</p>

<ul>

<li><strong>kubectl annotate pods nginx1 my-annotate=‘my annotate,ok’</strong></li>

</ul>

<p>然后，我们再看一下这个 Pod 的一些元数据，我们这边能够看到这个 Pod 的元数据里面 annotations，这是有一个 my-annotate 这个 Annotations；</p>

<ul>

<li><strong>kubectl get pods nging1 -o yaml | less</strong></li>

</ul>

<p>然后我们这里其实也能够看到有一个 kubectl apply 的时候，kubectl 工具增加了一个 annotation，这也是一个 json 串。</p>

<p><img src="assets/FrDqSFZdIHtorvWlAY-nc8-3DqWT" alt="avatar"></p>

<p>然后我们再演示一下看 Pod 的 Ownereference 是怎么出来的。原来的 Pod 都是直接通过创建 Pod 这个资源方式来创建的，这次换一种方式来创建：通过创建一个 ReplicaSet 对象来创建 Pod 。首先创建一个 ReplicaSet 对象，这个 ReplicaSet 对象可以具体查看一下；</p>

<ul>

<li><strong>kubectl apply -f rs.yaml</strong></li>

<li><strong>kubectl get replicasets nginx-replicasets -o yaml |less</strong></li>

</ul>

<p><img src="assets/FuTPDtE9S68BmM8Mp_TR8ymKAWLp" alt="avatar"></p>

<ol>

<li>我们可以关注一下这个 ReplicaSet 里面 spec 里面，提到会创建两个 Pod，然后 selector 通过匹配部署环境是 product 生产环境的这个标签来进行匹配。所以我们可以看一下，现在集群中的 Pod 情况；</li>

</ol>

<ul>

<li>**kubectl get pods **</li>

</ul>

<p><img src="assets/FoFZ1wBEq_3Io_0ZQA_FoiPU4exE" alt="avatar"></p>

<ol>

<li>将会发现多了两个 Pod，仔细查看这两个 Pod，可以看到 ReplicaSet 创建出来的 Pod 有一个特点，即它会带有 Ownereference，然后 Ownereference 里面指向了是一个 replicasets 类型，名字就叫做 nginx-replicasets；</li>

</ol>

<ul>

<li><strong>kubectl get pods nginx-replicasets-rhd68 -o yaml | less</strong></li>

</ul>

<p><img src="assets/Fig2L5-OUuHHvVz0sc3CNozOC9fu" alt="avatar"></p>

<h2>控制器模式</h2>

<h3>1、控制循环</h3>

<p>控制型模式最核心的就是控制循环的概念。在控制循环中包括了控制器，被控制的系统，以及能够观测系统的传感器，三个逻辑组件。</p>

<p>当然这些组件都是逻辑的，外界通过修改资源 spec 来控制资源，控制器比较资源 spec 和 status，从而计算一个 diff，diff 最后会用来决定执行对系统进行什么样的控制操作，控制操作会使得系统产生新的输出，并被传感器以资源 status 形式上报，控制器的各个组件将都会是独立自主地运行，不断使系统向 spec 表示终态趋近。</p>

<p><img src="assets/FnamS9km_fxJTAt6gkBC2MnNBveC" alt="avatar"></p>

<h3>2、Sensor</h3>

<p>控制循环中逻辑的传感器主要由 Reflector、Informer、Indexer 三个组件构成。</p>

<p>Reflector 通过 List 和 Watch K8s server 来获取资源的数据。List 用来在 Controller 重启以及 Watch 中断的情况下，进行系统资源的全量更新；而 Watch 则在多次 List 之间进行增量的资源更新；Reflector 在获取新的资源数据后，会在 Delta 队列中塞入一个包括资源对象信息本身以及资源对象事件类型的 Delta 记录，Delta 队列中可以保证同一个对象在队列中仅有一条记录，从而避免 Reflector 重新 List 和 Watch 的时候产生重复的记录。</p>

<p>Informer 组件不断地从 Delta 队列中弹出 delta 记录，然后把资源对象交给 indexer，让 indexer 把资源记录在一个缓存中，缓存在默认设置下是用资源的命名空间来做索引的，并且可以被 Controller Manager 或多个 Controller 所共享。之后，再把这个事件交给事件的回调函数</p>

<p><img src="assets/FlOL664XktEsrHzldj8dSQxMroHr" alt="avatar"></p>

<p>控制循环中的控制器组件主要由事件处理函数以及 worker 组成，事件处理函数之间会相互关注资源的新增、更新、删除的事件，并根据控制器的逻辑去决定是否需要处理。对需要处理的事件，会把事件关联资源的命名空间以及名字塞入一个工作队列中，并且由后续的 worker 池中的一个 Worker 来处理，工作队列会对存储的对象进行去重，从而避免多个 Woker 处理同一个资源的情况。</p>

<p>Worker 在处理资源对象时，一般需要用资源的名字来重新获得最新的资源数据，用来创建或者更新资源对象，或者调用其他的外部服务，Worker 如果处理失败的时候，一般情况下会把资源的名字重新加入到工作队列中，从而方便之后进行重试。</p>

<h3>3、控制循环例子-扩容</h3>

<p>这里举一个简单的例子来说明一下控制循环的工作原理。</p>

<p>ReplicaSet 是一个用来描述无状态应用的扩缩容行为的资源， ReplicaSet controler 通过监听 ReplicaSet 资源来维持应用希望的状态数量，ReplicaSet 中通过 selector 来匹配所关联的 Pod，在这里考虑 ReplicaSet rsA 的，replicas 从 2 被改到 3 的场景。</p>

<p><img src="assets/FgWb59Oe4WGvj7FWvGtnlRBgl_0J" alt="avatar"></p>

<p>首先，Reflector 会 watch 到 ReplicaSet 和 Pod 两种资源的变化，为什么我们还会 watch pod 资源的变化稍后会讲到。发现 ReplicaSet 发生变化后，在 delta 队列中塞入了对象是 rsA，而且类型是更新的记录。</p>

<p>Informer 一方面把新的 ReplicaSet 更新到缓存中，并与 Namespace nsA 作为索引。另外一方面，调用 Update 的回调函数，ReplicaSet 控制器发现 ReplicaSet 发生变化后会把字符串的 nsA/rsA 字符串塞入到工作队列中，工作队列后的一个 Worker 从工作队列中取到了 nsA/rsA 这个字符串的 key，并且从缓存中取到了最新的 ReplicaSet 数据。</p>

<p>Worker 通过比较 ReplicaSet 中 spec 和 status 里的数值，发现需要对这个 ReplicaSet 进行扩容，因此 ReplicaSet 的 Worker 创建了一个 Pod，这个 pod 中的 Ownereference 取向了 ReplicaSet rsA。</p>

<p><img src="assets/FrRFQkqMFFH3ZHiYow3LwbaHasLG" alt="avatar"></p>

<p>然后 Reflector Watch 到的 Pod 新增事件，在 delta 队列中额外加入了 Add 类型的 deta 记录，一方面把新的 Pod 记录通过 Indexer 存储到了缓存中，另一方面调用了 ReplicaSet 控制器的 Add 回调函数，Add 回调函数通过检查 pod ownerReferences 找到了对应的 ReplicaSet，并把包括 ReplicaSet 命名空间和字符串塞入到了工作队列中。</p>

<p>ReplicaSet 的 Woker 在得到新的工作项之后，从缓存中取到了新的 ReplicaSet 记录，并得到了其所有创建的 Pod，因为 ReplicaSet 的状态不是最新的，也就是所有创建 Pod 的数量不是最新的。因此在此时 ReplicaSet 更新 status 使得 spec 和 status 达成一致。</p>

<p><img src="assets/FpFDvQ_KQ1r7wvH6ftcjgeQw3pqM" alt="avatar"></p>

<h2>控制器模式总结</h2>

<h3>1、两种 API 设计方法</h3>

<p>Kubernetes 控制器模式依赖声明式的 API。另外一种常见的 API 类型是命令式 API。为什么 Kubernetes 采用声明式 API，而不是命令式 API 来设计整个控制器呢？</p>

<p>首先，比较两种 API 在交互行为上的差别。在生活中，常见的命令式的交互方式是家长和孩子交流方式，因为孩子欠缺目标意识，无法理解家长期望，家长往往通过一些命令，教孩子一些明确的动作，比如说：吃饭、睡觉类似的命令。我们在容器编排体系中，命令式 API 就是通过向系统发出明确的操作来执行的。</p>

<p>而常见的声明式交互方式，就是老板对自己员工的交流方式。老板一般不会给自己的员工下很明确的决定，实际上可能老板对于要操作的事情本身，还不如员工清楚。因此，老板通过给员工设置可量化的业务目标的方式，来发挥员工自身的主观能动性。比如说，老板会要求某个产品的市场占有率达到 80%，而不会指出要达到这个市场占有率，要做的具体操作细节。</p>

<p>类似的，在容器编排体系中，我们可以执行一个应用实例副本数保持在 3 个，而不用明确的去扩容 Pod 或是删除已有的 Pod，来保证副本数在三个。</p>

<p><img src="assets/FqWTFsi6obnFKf2IBO4N1KkPQuSE" alt="avatar"></p>

<h3>2、命令式 API 的问题</h3>

<p>在理解两个交互 API 的差别后，可以分析一下命令式 API 的问题。</p>

<ul>

<li>命令 API 最大的一个问题在于错误处理；</li>

</ul>

<p>在大规模的分布式系统中，错误是无处不在的。一旦发出的命令没有响应，调用方只能通过反复重试的方式来试图恢复错误，然而盲目的重试可能会带来更大的问题。</p>

<p>假设原来的命令，后台实际上已经执行完成了，重试后又多执行了一个重试的命令操作。为了避免重试的问题，系统往往还需要在执行命令前，先记录一下需要执行的命令，并且在重启等场景下，重做待执行的命令，而且在执行的过程中，还需要考虑多个命令的先后顺序、覆盖关系等等一些复杂的逻辑情况。</p>

<ul>

<li>实际上许多命令式的交互系统后台往往还会做一个巡检的系统，用来修正命令处理超时、重试等一些场景造成数据不一致的问题；</li>

</ul>

<p>然而，因为巡检逻辑和日常操作逻辑是不一样的，往往在测试上覆盖不够，在错误处理上不够严谨，具有很大的操作风险，因此往往很多巡检系统都是人工来触发的。</p>

<ul>

<li>最后，命令式 API 在处理多并发访问时，也很容易出现问题；</li>

</ul>

<p>假如有多方并发的对一个资源请求进行操作，并且一旦其中有操作出现了错误，就需要重试。那么最后哪一个操作生效了，就很难确认，也无法保证。很多命令式系统往往在操作前会对系统进行加锁，从而保证整个系统最后生效行为的可预见性，但是加锁行为会降低整个系统的操作执行效率。</p>

<ul>

<li>相对的，声明式 API 系统里天然地记录了系统现在和最终的状态。</li>

</ul>

<p>不需要额外的操作数据。另外因为状态的幂等性，可以在任意时刻反复操作。在声明式系统运行的方式里，正常的操作实际上就是对资源状态的巡检，不需要额外开发巡检系统，系统的运行逻辑也能够在日常的运行中得到测试和锤炼，因此整个操作的稳定性能够得到保证。</p>

<p>最后，因为资源的最终状态是明确的，我们可以合并多次对状态的修改。可以不需要加锁，就支持多方的并发访问。</p>

<p><img src="assets/FrwSJNoB6e0Vdlud3VUgj7iWFGSc" alt="avatar"></p>

<h3>3、控制器模式总结</h3>

<p>最后我们总结一下：</p>

<ol>

<li>Kubernetes 所采用的控制器模式，是由声明式 API 驱动的。确切来说，是基于对 Kubernetes 资源对象的修改来驱动的；</li>

<li>Kubernetes 资源之后，是关注该资源的控制器。这些控制器将异步的控制系统向设置的终态驱近；</li>

<li>这些控制器是自主运行的，使得系统的自动化和无人值守成为可能；</li>

<li>因为 Kubernetes 的控制器和资源都是可以自定义的，因此可以方便的扩展控制器模式。特别是对于有状态应用，我们往往通过自定义资源和控制器的方式，来自动化运维操作。这个也就是后续会介绍的 operator 的场景。</li>

</ol>

<p><img src="assets/FptCF23R_kZirzI8nExtx0H-XCyi" alt="avatar"></p>

<h2>本节总结</h2>

<p>本节课的主要内容就到此为止了，这里为大家简单总结一下：</p>

<ul>

<li>Kubernetes 资源对象中的元数据部分，主要包括了用来识别资源的标签：Labels， 用来描述资源的注解；Annotations， 用来描述多个资源之间相互关系的 OwnerReference。这些元数据在 K8s 运行中有非常重要的作用；</li>

<li>控制型模式中最核心的就是控制循环的概念；</li>

<li>两种 API 设计方法：声明式 API 和命令式 API ；Kubernetes 所采用的控制器模式，是由声明式 API 驱动的。</li>

</ul>

</div>

                    </div><div class="book-post">

                        <p id="tip" align="center"></p>

                        <div><h1>06 应用编排与管理</h1>

<h2>需求来源</h2>

<h3>背景问题</h3>

<p>首先，我们来看一下背景问题。如下图所示：如果我们直接管理集群中所有的 Pod，应用 A、B、C 的 Pod，其实是散乱地分布在集群中。</p>

<p><img src="assets/b7c97640-d2c3-11e9-a365-2956af74956a" alt="在这里插入图片描述"></p>

<p>现在有以下的问题：</p>

<ul>

<li>首先，如何保证集群内可用 Pod 的数量？也就是说我们应用 A 四个 Pod 如果出现了一些宿主机故障，或者一些网络问题，如何能保证它可用的数量？</li>

<li>如何为所有 Pod 更新镜像版本？我们是否要某一个 Pod 去重建新版本的 Pod？</li>

<li>然后在更新过程中，如何保证服务的可用性？</li>

<li>以及更新过程中，如果发现了问题，如何快速回滚到上一个版本？</li>

</ul>

<h3>Deployment：管理部署发布的控制器</h3>

<p>这里就引入了我们今天课程的主题：Deployment 管理部署发布的控制器。</p>

<p><img src="assets/FqRZXqnvbkypbT-ARZ9EuciekCqD" alt="avatar"></p>

<p>可以看到我们通过 Deployment 将应用 A、B、C 分别规划到不同的 Deployment 中，每个 Deployment 其实是管理的一组相同的应用 Pod，这组 Pod 我们认为它是相同的一个副本，那么 Deployment 能帮我们做什么事情呢？</p>

<p>首先，Deployment 定义了一种 Pod 期望数量，比如说应用 A，我们期望 Pod 数量是四个，那么这样的话，controller 就会持续维持 Pod 数量为期望的数量。当我们与 Pod 出现了网络问题或者宿主机问题的话，controller 能帮我们恢复，也就是新扩出来对应的 Pod，来保证可用的 Pod 数量与期望数量一致；</p>

<p>配置 Pod 发布方式，也就是说 controller 会按照用户给定的策略来更新 Pod，而且更新过程中，也可以设定不可用 Pod 数量在多少范围内；</p>

<p>如果更新过程中发生问题的话，即所谓“一键”回滚，也就是说你通过一条命令或者一行修改能够将 Deployment 下面所有 Pod 更新为某一个旧版本 。</p>

<h2>用例解读</h2>

<h3>Deployment 语法</h3>

<p>下面我们用一个简单的用例来解读一下如何操作 Deployment。</p>

<p><img src="assets/Fu2ivUsTaLUzVrDBu2futpWLaF1t" alt="avatar"></p>

<p>上图可以看到一个最简单的 Deployment 的 yaml 文件。</p>

<p>“apiVersion：apps/v1”，也就是说 Deployment 当前所属的组是 apps，版本是 v1。“metadata”是我们看到的 Deployment 元信息，也就是往期回顾中的 Labels、Selector、Pod.image，这些都是在往期中提到的知识点。</p>

<p>Deployment 作为一个 K8s 资源，它有自己的 metadata 元信息，这里我们定义的 Deployment.name 是 nginx.Deployment。Deployment.spec 中首先要有一个核心的字段，即 replicas，这里定义期望的 Pod 数量为三个；selector 其实是 Pod 选择器，那么所有扩容出来的 Pod，它的 Labels 必须匹配 selector 层上的 image.labels，也就是 app.nginx。</p>

<p>就如上面的 Pod 模板 template 中所述，这个 template 它其实包含了两部分内容：</p>

<ul>

<li>一部分是我们期望 Pod 的 metadata，其中包含了 labels，即跟 selector.matchLabels 相匹配的一个 Labels；</li>

<li>第二部分是 template 包含的一个 Pod.spec。这里 Pod.spec 其实是 Deployment 最终创建出来 Pod 的时候，它所用的 Pod.spec，这里定义了一个 container.nginx，它的镜像版本是 nginx:1.7.9。</li>

</ul>

<p>下面是遇到的新知识点：</p>

<ul>

<li>第一个是 replicas，就是 Deployment 中期望的或者终态数量；</li>

<li>第二个是 template，也就是 Pod 相关的一个模板。</li>

</ul>

<h3>查看 Deployment 状态</h3>

<p>当我们创建出一个 Deployment 的时候，可以通过 kubectl get deployment，看到 Deployment 总体的一个状态。如下图所示：</p>

<p><img src="assets/Fs4OqN5_81-gMNZc4uNCtlVfAvbI" alt="avatar"></p>

<p>上图中可以看到：</p>

<ul>

<li><strong>DESIRED</strong>：期望的 Pod 数量是 3 个；</li>

<li><strong>CURRENT</strong>：当前实际 Pod 数量是 3 个；</li>

<li><strong>UP-TO-DATE</strong>：其实是到达最新的期望版本的 Pod 数量；</li>

<li><strong>AVAILABLE</strong>：这个其实是运行过程中可用的 Pod 数量。后面会提到，这里 AVAILABLE 并不简单是可用的，也就是 Ready 状态的，它其实包含了一些可用超过一定时间长度的 Pod；</li>

<li><strong>AGE</strong>：deployment 创建的时长，如上图 Deployment 就是已经创建了 80 分钟。</li>

</ul>

<h3>查看 Pod</h3>

<p>最后我们可以查看一下 Pod。如下图所示：</p>

<p><img src="assets/FqPlCtPscJEM23b_Q-AjWN9FtiPT" alt="avatar"></p>

<p>上图中有三个 Pod，Pod 名字格式我们不难看到。</p>

<p>最前面一段：nginx-deployment，其实是 Pod 所属 Deployment.name；中间一段：template-hash，这里三个 Pod 是一样的，因为这三个 Pod 其实都是同一个 template 中创建出来的。</p>

<p>最后一段，是一个 random 的字符串，我们通过 get.pod 可以看到，Pod 的 ownerReferences 即 Pod 所属的 controller 资源，并不是 Deployment，而是一个 ReplicaSet。这个 ReplicaSet 的 name，其实是 nginx-deployment 加上 pod.template-hash，后面会提到。所有的 Pod 都是 ReplicaSet 创建出来的，而 ReplicaSet 它对应的某一个具体的 Deployment.template 版本。</p>

<h3>更新镜像</h3>

<p>接下来我们可以看一下，如何对一个给定的 Deployment 更新它所有Pod的镜像版本呢？这里我们可以执行一个 kubectl 命令：</p>

<p><strong>kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1</strong></p>

<p>首先 kubectl 后面有一个 set image 固定写法，这里指的是设定镜像；其次是一个 deployment.v1.apps，这里也是一个固定写法，写的是我们要操作的资源类型，deployment 是资源名、v1 是资源版本、apps 是资源组，这里也可以简写为 deployment 或者 deployment.apps，比如说写为 deployment 的时候，默认将使用 apps 组 v1 版本。</p>

<p>第三部分是要更新的 deployment 的 name，也就是我们的 nginx-deployment；再往后的 nginx 其实指的是 template，也就是 Pod 中的 container.name；这里我们可以注意到：一个 Pod 中，其实可能存在多个 container，而我们指定想要更新的镜像的 container.name，就是 nginx。</p>

<p>最后，指定我们这个容器期望更新的镜像版本，这里指的是 nginx: 1.9.1。如下图所示：当执行完这条命令之后，可以看到 deployment 中的 template.spec 已经更新为 nginx: 1.9.1。</p>

<p><img src="assets/FkERYt8vsgt2u6-pHkJ9mnCHR6MM" alt="avatar"></p>

<h3>快速回滚</h3>

<p>如果我们在发布过程中遇到了问题，也支持快速回滚。通过 kubectl 执行的话，其实是“kubectl rollout undo”这个命令，可以回滚到 Deployment 上一版本；通过“rollout undo”加上“to-revision”来指定可以回滚到某一个具体的版本。</p>

<p><img src="assets/FkjjEs6sM54eWZN_7alEPntwKJdt" alt="avatar"></p>

<h3>DeploymeStatus</h3>

<p>最后我们来看一下 DeploymeStatus。前面的课程我们学习到，每一个资源都有它的 spec.Status。这里可以看一下，deploymentStatus 中描述的三个其实是它的 conversion 状态，也就是 Processing、Complete 以及 Failed。</p>

<p><img src="assets/Fl9CQjw6K_EzMe27iIdxhFpnck3O" alt="avatar"></p>

<p>以 Processing 为例：Processing 指的是 Deployment 正在处于扩容和发布中。比如说 Processing 状态的 deployment，它所有的 replicas 及 Pod 副本全部达到最新版本，而且是 available，这样的话，就可以进入 complete 状态。而 complete 状态如果发生了一些扩缩容的话，也会进入 processing 这个处理工作状态。</p>

<p>如果在处理过程中遇到一些问题：比如说拉镜像失败了，或者说 readiness probe 检查失败了，就会进入 failed 状态；如果在运行过程中即 complete 状态，中间运行时发生了一些 pod readiness probe 检查失败，这个时候 deployment 也会进入 failed 状态。进入 failed 状态之后，除非所有点 replicas 均变成 available，而且是 updated 最新版本，deployment 才会重新进入 complete 状态。</p>

<h2>操作演示</h2>

<h3>Deployment 创建及状态</h3>

<p>下面我们来进行操作演示：这里连接一个阿里云服务集群。我们可以看到当前集群已经有几个可用的 node。</p>

<p><img src="assets/Fun-rY_7MbND5PC1obeqwxp1EZaR" alt="avatar"></p>

<p>首先创建对应的 deployment。可以看到 deployment 中的 desired、current、up-to-date 以及 available 已经都达到了可用的期望状态。</p>

<p><img src="assets/FuSh7BFpVmPc66eS61Jf7tRJVFW7" alt="avatar"></p>

<h3>Deployment 的结构</h3>

<p>这里看到 spec 中的 replicas 是三个，selector 以及 template labels中定义的标签都是 app：nginx，spec 中的 image 是我们期望的 nginx: 1.7.9；status 中的 available.replicas，readReplicas 以及 updatedReplicas 都是 3 个。</p>

<p><img src="assets/Fr7_fNTKJdbEARnEChYc7VyQ0HYu" alt="avatar"></p>

<h3>Pod 状态</h3>

<p>我们可以再选择一个 Pod 看一下状态：</p>

<p>可以看到：Pod 中 ownerReferences 的功能是 ReplicaSet；pod.spec.container 里的镜像是 1.7.9。这个 Pod 已经是 Running 状态，而且它的 conditions.status 是“true”，表示它的服务已经可用了。</p>

<p><img src="assets/FiM92jx0nQVOikobcBbEnSNglgAO" alt="avatar"></p>

<h3>更新升级</h3>

<p>当前只有最新版本的 replicaset，那么现在尝试对 deployment 做一次升级。</p>

<p><img src="assets/Ft2Q8l7S2xNRd8PfBxuwckqnGFXz" alt="avatar"></p>

<p>“kubectl set image”这个操作命令，后面接 “deployment”，加 deployment.name，最后指定容器名，以及我们期望升级的镜像版本。</p>

<p><img src="assets/FgOs2Oj0b5VSYLQnd_BK2QSn7bLv" alt="avatar"></p>

<p>接下来我们看下 deployment 中的 template 中的 image 已经更新为 1.9.1。</p>

<p><img src="assets/FgFsNXZdE74hsTGSjYawyQnJl4D2" alt="avatar"></p>

<p>这个时候我们再 get pod 看一下状态。</p>

<p><img src="assets/FsIn9W6sH4_OVMXUR6VfTX_AwOoD" alt="avatar"></p>

<p>三个 pod 已经升级为新版本，pod 名字中的 pod-template-hash 也已更新。</p>

<p><img src="assets/FhAamAvv0WT8fxmVODrO5gHeMRaa" alt="avatar"></p>

<p>可以看到：旧版本 replicaset 的 spec 数量以及 pod 数量是都是 0，新版本的 pod 数量是 3 个。</p>

<hr>

<p>假设又做了一次更新，这个时候 get.pod 其实可以看到：当前的 pod 其实是有两个旧版本的处于 running，另一个旧版本是在删除中；而两个新版本的 pod，一个已经进入 running，一个还在 creating 中。</p>

<p>这时我们可用的 pod 数量即非删除状态的 pod 数量，其实是 4 个，已经超过了 replica 原先在 deployment 设置的数量 3 个。这个原因是我们在 deployment 中有 maxavailable 和 maxsugar 两个操作，这两个配置可以限制我们在发布过程中的一些策略。在后面架构设计中会讲到这个问题。</p>

<p><img src="assets/FpiPmioOOxkeXk7Ve-7ro7RmkFGS" alt="avatar">** **</p>

<h3>历史版本保留 revisionHistoryLimit</h3>

<p>上图看到，我们当前最新版本的 replicaset 是 3 个 pod，另外还有两个历史版本的 replicaset，那么会不会存在一种情况：就是随着 deployment 持续的更新，这个旧版本的 replicaset 会越积越多呢？其实 deployment 提供了一个机制来避免这个问题：在 deployment spec 中，有一个 revisionHistoryLimit，它的默认值为 10，它其实保证了保留历史版本的 replicaset 的数量，我们尝试把它改为 1。</p>

<p><img src="assets/Fn-kTF85gQEi2Vkk8Jvqhyz2PgdI" alt="avatar"> <img src="assets/FtV6gw2MsQflGn8jprEnRwn8cTmg" alt="avatar"></p>

<p>由上面第二张图，可以看到两个 replicaset，也就是说，除了当前版本的 replicaset 之外，旧版本的 replicaset 其实只保留了一个。</p>

<h3>回滚</h3>

<p>最后再尝试做一下回滚。首先再来看一下 replicaset，这时发现旧版本的 replicaset 数量从 0 个增到 2 个，而新版本的 replicaset 数量从 3 个削减为 1 个，表示它已经开始在做回滚的操作。然后再观察一下， 旧版本的数量已经是 3 个，即已经回滚成功，而新版本的 pod 数量变为 0 个。</p>

<p><img src="assets/FiS6JI8v_MMVSX0e_PeKKLAjuszH" alt="avatar"></p>

<p>我们最后再 get pod 看一下：</p>

<p><img src="assets/FtkkvKGGWYAMDqK6d7Icx2HtL2xI" alt="avatar"></p>

<p>这时，3 个 pod.template-hash 已经更新为旧版本的 hash，但其实这 3 个 pod 都是重新创建出来的，而并非我们在前一版本中创建的 3 个 pod。换句话说，也就是我们回滚的时候，其实是创建了 3 个旧版本的 pod，而并非把先前的 3 个 pod 找回来。</p>

<h2>架构设计</h2>

<h3>管理模式</h3>

<p><img src="assets/FuCB_FCHGkMRcZR1RE6EoxcT8IlH" alt="avatar"></p>

<p>我们来看一下架构设计。首先简单看一下管理模式：Deployment 只负责管理不同版本的 ReplicaSet，由 ReplicaSet 来管理具体的 Pod 副本数，每个 ReplicaSet 对应 Deployment template 的一个版本。在上文的例子中可以看到，每一次修改 template，都会生成一个新的 ReplicaSet，这个 ReplicaSet 底下的 Pod 其实都是相同的版本。</p>

<p>如上图所示：Deployment 创建 ReplicaSet，而 ReplicaSet 创建 Pod。他们的 OwnerRef 其实都对应了其控制器的资源。</p>

<h3>Deployment 控制器</h3>

<p>我们先简单看一下控制器实现原理。</p>

<p>首先，我们所有的控制器都是通过 Informer 中的 Event 做一些 Handler 和 Watch。这个地方 Deployment 控制器，其实是关注 Deployment 和 ReplicaSet 中的 event，收到事件后会加入到队列中。而 Deployment controller 从队列中取出来之后，它的逻辑会判断 Check Paused，这个 Paused 其实是 Deployment 是否需要新的发布，如果 Paused 设置为 true 的话，就表示这个 Deployment 只会做一个数量上的维持，不会做新的发布。</p>

<p><img src="assets/FpxE-4XAdA5wn4J97zkPKBJWkPgL" alt="avatar"></p>

<p>如上图，可以看到如果 Check paused 为 Yes 也就是 true 的话，那么只会做 Sync replicas。也就是说把 replicas sync 同步到对应的 ReplicaSet 中，最后再 Update Deployment status，那么 controller 这一次的 ReplicaSet 就结束了。</p>

<p>那么如果 paused 为 false 的话，它就会做 Rollout，也就是通过 Create 或者是 Rolling 的方式来做更新，更新的方式其实也是通过 Create/Update/Delete 这种 ReplicaSet 来做实现的。</p>

<h3>ReplicaSet 控制器</h3>

<p><img src="assets/Fr-7FZjePgouiaG6ism-GSMLJl_q" alt="avatar"></p>

<p>当 Deployment 分配 ReplicaSet 之后，ReplicaSet 控制器本身也是从 Informer 中 watch 一些事件，这些事件包含了 ReplicaSet 和 Pod 的事件。从队列中取出之后，ReplicaSet controller 的逻辑很简单，就只管理副本数。也就是说如果 controller 发现 replicas 比 Pod 数量大的话，就会扩容，而如果发现实际数量超过期望数量的话，就会删除 Pod。</p>

<p>上面 Deployment 控制器的图中可以看到，Deployment 控制器其实做了更复杂的事情，包含了版本管理，而它把每一个版本下的数量维持工作交给 ReplicaSet 来做。</p>

<h3>扩/缩容模拟</h3>

<p>下面来看一些操作模拟，比如说扩容模拟。这里有一个 Deployment，它的副本数是 2，对应的 ReplicaSet 有 Pod1 和 Pod2。这时如果我们修改 Deployment replicas， controller 就会把 replicas 同步到当前版本的 ReplicaSet 中，这个 ReplicaSet 发现当前有 2 个 Pod，不满足当前期望 3 个，就会创建一个新的 Pod3。</p>

<p><img src="assets/FimypfAsafXJ_EL3iBSPjVbUelVa" alt="avatar"></p>

<h3>发布模拟</h3>

<p>我们再模拟一下发布，发布的情况会稍微复杂一点。这里可以看到 Deployment 当前初始的 template，比如说 template1 这个版本。template1 这个 ReplicaSet 对应的版本下有三个 Pod：Pod1，Pod2，Pod3。</p>

<p>这时修改 template 中一个容器的 image， Deployment controller 就会新建一个对应 template2 的 ReplicaSet。创建出来之后 ReplicaSet 会逐渐修改两个 ReplicaSet 的数量，比如它会逐渐增加 ReplicaSet2 中 replicas 的期望数量，而逐渐减少 ReplicaSet1 中的 Pod 数量。</p>

<p>那么最终达到的效果是：新版本的 Pod 为 Pod4、Pod5和Pod6，旧版本的 Pod 已经被删除了，这里就完成了一次发布。</p>

<p><img src="assets/FmZKar-wXQtxLH9-rT4BZmZULMDw" alt="avatar"></p>

<h3>回滚模拟</h3>

<p>来看一下回滚模拟，根据上面的发布模拟可以知道 Pod4、Pod5、Pod6 已经发布完成。这时发现当前的业务版本是有问题的，如果做回滚的话，不管是通过 rollout 命令还是通过回滚修改 template，它其实都是把 template 回滚为旧版本的 template1。</p>

<p>这个时候 Deployment 会重新修改 ReplicaSet1 中 Pod 的期望数量，把期望数量修改为 3 个，且会逐渐减少新版本也就是 ReplicaSet2 中的 replica 数量，最终的效果就是把 Pod 从旧版本重新创建出来。</p>

<p><img src="assets/Fv9lYQXHc3Kh6d7VgdlAgDcXdGyh" alt="avatar"></p>

<p>发布模拟的图中可以看到，其实初始版本中 Pod1、Pod2、Pod3 是旧版本，而回滚之后其实是 Pod7、Pod8、Pod9。就是说它的回滚并不是把之前的 Pod 重新找出来，而是说重新创建出符合旧版本 template 的 Pod。</p>

<h3>spec 字段解析</h3>

<p>最后再来简单看一些 Deployment 中的字段解析。首先看一下 Deployment 中其他的 spec 字段：</p>

<ul>

<li>MinReadySeconds：Deployment 会根据 Pod ready 来看 Pod 是否可用，但是如果我们设置了 MinReadySeconds 之后，比如设置为 30 秒，那 Deployment 就一定会等到 Pod ready 超过 30 秒之后才认为 Pod 是 available 的。Pod available 的前提条件是 Pod ready，但是 ready 的 Pod 不一定是 available 的，它一定要超过 MinReadySeconds 之后，才会判断为 available；</li>

<li>revisionHistoryLimit：保留历史 revision，即保留历史 ReplicaSet 的数量，默认值为 10 个。这里可以设置为一个或两个，如果回滚可能性比较大的话，可以设置数量超过 10；</li>

<li>paused：paused 是标识，Deployment 只做数量维持，不做新的发布，这里在 Debug 场景可能会用到；</li>

<li>progressDeadlineSeconds：前面提到当 Deployment 处于扩容或者发布状态时，它的 condition 会处于一个 processing 的状态，processing 可以设置一个超时时间。如果超过超时时间还处于 processing，那么 controller 将认为这个 Pod 会进入 failed 的状态。</li>

</ul>

<p><img src="assets/FiM7kHLUWa2eO47ePovKQtAkeLdA" alt="avatar"></p>

<h3>升级策略字段解析</h3>

<p>最后来看一下升级策略字段解析。</p>

<p>Deployment 在 RollingUpdate 中主要提供了两个策略，一个是 MaxUnavailable，另一个是 MaxSurge。这两个字段解析的意思，可以看下图中详细的 comment，或者简单解释一下：</p>

<ul>

<li>MaxUnavailable：滚动过程中最多有多少个 Pod 不可用；</li>

<li>MaxSurge：滚动过程中最多存在多少个 Pod 超过预期 replicas 数量。</li>

</ul>

<p>上文提到，ReplicaSet 为 3 的 Deployment 在发布的时候可能存在一种情况：新版本的 ReplicaSet 和旧版本的 ReplicaSet 都可能有两个 replicas，加在一起就是 4 个，超过了我们期望的数量三个。这是因为我们默认的 MaxUnavailable 和 MaxSurge 都是 25%，默认 Deployment 在发布的过程中，可能有 25% 的 replica 是不可用的，也可能超过 replica 数量 25% 是可用的，最高可以达到 125% 的 replica 数量。</p>

<p>这里其实可以根据用户实际场景来做设置。比如当用户的资源足够，且更注重发布过程中的可用性，可设置 MaxUnavailable 较小、MaxSurge 较大。但如果用户的资源比较紧张，可以设置 MaxSurge 较小，甚至设置为 0，这里要注意的是 MaxSurge 和 MaxUnavailable 不能同时为 0。</p>

<p>理由不难理解，当 MaxSurge 为 0 的时候，必须要删除 Pod，才能扩容 Pod；如果不删除 Pod 是不能新扩 Pod 的，因为新扩出来的话，总共的 Pod 数量就会超过期望数量。而两者同时为 0 的话，MaxSurge 保证不能新扩 Pod，而 MaxUnavailable 不能保证 ReplicaSet 中有 Pod 是 available 的，这样就会产生问题。所以说这两个值不能同时为 0。用户可以根据自己的实际场景来设置对应的、合适的值。</p>

<p><img src="assets/FsnmWASNbiP8rP7uWBJ9o9J7uGFI" alt="avatar"></p>

<h2>本节总结</h2>

<p>本节课的主要内容就到此为止了，这里为大家简单总结一下。</p>

<ul>

<li>Deployment 是 Kubernetes 中常见的一种 Workload，支持部署管理多版本的 Pod；</li>

<li>Deployment 管理多版本的方式，是针对每个版本的 template 创建一个 ReplicaSet，由 ReplicaSet 维护一定数量的 Pod 副本，而 Deployment 只需要关心不同版本的 ReplicaSet 里要指定多少数量的 Pod；</li>

<li>因此，Deployment 发布部署的根本原理，就是 Deployment 调整不同版本 ReplicaSet 里的终态副本数，以此来达到多版本 Pod 的升级和回滚。</li>

</ul>

</div>

                    </div><div class="book-post">

                        <p id="tip" align="center"></p>

                        <div><h1>07 应用编排与管理：Job &amp; DaemonSet</h1>

<h2>Job</h2>

<h3>需求来源</h3>

<h4>Job 背景问题</h4>

<p>首先我们来看一下 Job 的需求来源。我们知道 K8s 里面，最小的调度单元是 Pod，我们可以直接通过 Pod 来运行任务进程。这样做将会产生以下几种问题：</p>

<ul>

<li>我们如何保证 Pod 内进程正确的结束？</li>

<li>如何保证进程运行失败后重试？</li>

<li>如何管理多个任务，且任务之间有依赖关系？</li>

<li>如何并行地运行任务，并管理任务的队列大小？</li>

</ul>

<h4>Job：管理任务的控制器</h4>

<p>我们来看一下 Kubernetes 的 Job 为我们提供了什么功能：</p>

<ul>

<li>首先 kubernetes 的 Job 是一个管理任务的控制器，它可以创建一个或多个 Pod 来指定 Pod 的数量，并可以监控它是否成功地运行或终止；</li>

<li>我们可以根据 Pod 的状态来给 Job 设置重置的方式及重试的次数；</li>

<li>我们还可以根据依赖关系，保证上一个任务运行完成之后再运行下一个任务；</li>

<li>同时还可以控制任务的并行度，根据并行度来确保 Pod 运行过程中的并行次数和总体完成大小。</li>

</ul>

<h3>用例解读</h3>

<p>我们根据一个实例来看一下Job是如何来完成下面的应用的。</p>

<h4>Job 语法</h4>

<p><img src="assets/FlJHcV-dW9UFVax6IaehPYtKCld-" alt="avatar"></p>

<p>上图是 Job 最简单的一个 yaml 格式，这里主要新引入了一个 kind 叫 Job，这个 Job 其实就是 job-controller 里面的一种类型。 然后 metadata 里面的 name 来指定这个 Job 的名称，下面 spec.template 里面其实就是 pod 的 spec。</p>

<p>这里面的内容都是一样的，唯一多了两个点：</p>

<ul>

<li>第一个是 restartPolicy，在 Job 里面我们可以设置 Never、OnFailure、Always 这三种重试策略。在希望 Job 需要重新运行的时候，我们可以用 Never；希望在失败的时候再运行，再重试可以用 OnFailure；或者不论什么情况下都重新运行时 Alway；</li>

<li>另外，Job 在运行的时候不可能去无限的重试，所以我们需要一个参数来控制重试的次数。这个 backoffLimit 就是来保证一个 Job 到底能重试多少次。</li>

</ul>

<p>所以在 Job 里面，我们主要重点关注的一个是 <strong>restartPolicy 重启策略</strong>和 <strong>backoffLimit 重试次数限制</strong>。</p>

<h4>Job 状态</h4>

<p><img src="assets/Fo9k3bh5T3hAJxqA-3hFXVRkYF70" alt="avatar"></p>

<p>Job 创建完成之后，我们就可以通过 kubectl get jobs 这个命令，来查看当前 job 的运行状态。得到的值里面，基本就有 Job 的名称、当前完成了多少个 Pod，进行多长时间。</p>

<p><strong>AGE</strong>的含义是指这个 Pod 从当前时间算起，减去它当时创建的时间。这个时长主要用来告诉你 Pod 的历史、Pod 距今创建了多长时间。</p>

<p><strong>DURATION</strong>主要来看我们 Job 里面的实际业务到底运行了多长时间，当我们的性能调优的时候，这个参数会非常的有用。<strong>COMPLETIONS</strong>主要来看我们任务里面这个 Pod 一共有几个，然后它其中完成了多少个状态，会在这个字段里面做显示。</p>

<h4>查看 Pod</h4>

<p>下面我们来看一下 Pod，其实 Job 最后的执行单元还是 Pod。我们刚才创建的 Job 会创建出来一个叫“pi”的一个 Pod，这个任务就是来计算这个圆周率，Pod 的名称会以“job−name−job−name−{random-suffix}”，我们可以看一下下面 Pod 的 yaml 格式。</p>

<p><img src="assets/FhXtzQPMBAE-mI_CZZ1pNtldCOHY" alt="avatar"></p>

<p>它比普通的 Pod 多了一个叫 <strong>ownerReferences</strong>，这个东西来声明此 pod 是归哪个上一层 controller 来管理。可以看到这里的 ownerReferences 是归 batch/v1，也就是上一个 Job 来管理的。这里就声明了它的 controller 是谁，然后可以通过 pod 返查到它的控制器是谁，同时也能根据 Job 来查一下它下属有哪些 Pod。</p>

<h4>并行运行 Job</h4>

<p>我们有时候有些需求：希望 Job 运行的时候可以最大化的并行，并行出 n 个 Pod 去快速地执行。同时，由于我们的节点数有限制，可能也不希望同时并行的 Pod 数过多，有那么一个管道的概念，我们可以希望最大的并行度是多少，Job 控制器都可以帮我们来做到。</p>

<p>这里主要看两个参数：<strong>一个是 completions，一个是 parallelism。</strong></p>

<ul>

<li>首先第一个参数是用来指定本 Pod 队列执行次数。可能这个不是很好理解，其实可以把它认为是这个 Job 指定的可以运行的总次数。比如这里设置成 8，即这个任务一共会被执行 8 次；</li>

<li>第二个参数代表这个并行执行的个数。所谓并行执行的次数，其实就是一个管道或者缓冲器中缓冲队列的大小，把它设置成 2，也就是说这个 Job 一定要执行 8 次，每次并行 2 个 Pod，这样的话，一共会执行 4 个批次。</li>

</ul>

<h4>查看并行 Job 运行</h4>

<p><img src="assets/FsfyKX4pZ6ltGRES3Ma1GoCcb-64" alt="avatar"></p>

<p>下面来看一下它的实际运行效果，上图就是当这个 Job 整体运行完毕之后可以看到的效果，首先看到 job 的名字，然后看到它一共创建出来了 8 个 pod，执行了 2 分 23 秒，这是创建的时间。</p>

<p>接着来看真正的 pods，pods 总共出来了 8 个 pod，每个 pod 的状态都是完成的，然后来看一下它的 AGE，就是时间。从下往上看，可以看到分别有 73s、40s、110s 和 2m26s。每一组都有两个 pod 时间是相同的，即：时间段是 40s 的时候是最后一个创建、 2m26s 是第一个创建的。也就是说，总是两个 pod 同时创建出来，并行完毕、消失，然后再创建、再运行、再完毕。</p>

<p>比如说，刚刚我们其实通过第二个参数来控制了当前 Job 并行执行的次数，这里就可以了解到这个缓冲器或者说管道队列大小的作用。</p>

<h4>Cronjob 语法</h4>

<p><img src="assets/Fv2wyMUcvFwpxCSrNUDdBEP4LeE9" alt="avatar"></p>

<p>下面来介绍另外一个 Job，叫做 CronJob，其实也可以叫定时运行 Job。CronJob 其实和 Job 大体是相似的，唯一的不同点就是它可以设计一个时间。比如说可以定时在几点几分执行，特别适合晚上做一些清理任务，还有可以几分钟执行一次，几小时执行一次等等，这就叫定时任务。</p>

<p>定时任务和 Job 相比会多几个不同的字段：</p>

<ul>

<li><strong>schedule</strong>：schedule 这个字段主要是设置时间格式，它的时间格式和 Linux 的 crontime 是一样的，所以直接根据 Linux 的 crontime 书写格式来书写就可以了。举个例子： */1 指每分钟去执行一下 Job，这个 Job 需要做的事情就是打印出大约时间，然后打印出“Hello from the kubernetes cluster” 这一句话；</li>

<li>**startingDeadlineSeconds：**即：每次运行 Job 的时候，它最长可以等多长时间，有时这个 Job 可能运行很长时间也不会启动。所以这时，如果超过较长时间的话，CronJob 就会停止这个 Job；</li>

<li><strong>concurrencyPolicy</strong>：就是说是否允许并行运行。所谓的并行运行就是，比如说我每分钟执行一次，但是这个 Job 可能运行的时间特别长，假如两分钟才能运行成功，也就是第二个 Job 要到时间需要去运行的时候，上一个 Job 还没完成。如果这个 policy 设置为 true 的话，那么不管你前面的 Job 是否运行完成，每分钟都会去执行；如果是 false，它就会等上一个 Job 运行完成之后才会运行下一个；</li>

<li>**JobsHistoryLimit：**这个就是每一次 CronJob 运行完之后，它都会遗留上一个 Job 的运行历史、查看时间。当然这个额不能是无限的，所以需要设置一下历史存留数，一般可以设置默认 10 个或 100 个都可以，这主要取决于每个人集群不同，然后根据每个人的集群数来确定这个时间。</li>

</ul>

<h3>操作演示</h3>

<h4>Job 的编排文件</h4>

<p>下面看一下具体如何使用 Job。</p>

<p><img src="assets/FmV-DIGuMjGLQS12FhNgSIpi8MsJ" alt="avatar"></p>

<h4>Job 的创建及运行验证</h4>

<p>首先看一下 job.yaml。这是一个非常简单的计算 pi 的一个任务。使用 kubectl creat-f job.yaml，这样 job 就能提交成功了。来看一下 kubectl.get.jobs，可以看到这个 job 正在运行；get pods 可以看到这个 pod 应该是运行完成了，那么接下来 logs 一下这个 job 以及 pod。可以看到下图里面打印出来了圆周率。</p>

<p><img src="assets/Fq8EvBaQV8x_2y4-MDw2kob3hGsQ" alt="avatar"></p>

<h4>并行 Job 的编排文件</h4>

<p>下面再来看第二个例子：</p>

<p><img src="assets/Fv2O65ZDO--pHLz8xicotfl9_zZw" alt="avatar"></p>

<h4>并行 Job 的创建及运行验证</h4>

<p>这个例子就是指刚才的并行运行 Job 创建之后，可以看到有第二个并行的 Job。</p>

<p><img src="assets/Fk5BhN8oUJVYijYcFib7rfe0vVRx" alt="avatar"></p>

<p>现在已经有两个 Pod 正在 running，可以看到它大概执行了快到 30s</p>

<p><img src="assets/FtXVmf5BjO8BRcc5LuhwXksXOFRD" alt="avatar"></p>

<p>30s 之后它应该会起第二个。</p>

<p><img src="assets/FuMQjONTy68OVSKCjTd6iyPUXhdt" alt="avatar"></p>

<p>第一批的 pod 已经执行完毕，第二批的 pod 正在 running，每批次分别是两个Pod。也就是说后面每隔 40s 左右，就会有两个 pod 在并行执行，它一共会执行 4 批，共 8 个 pod，等到所有的 pod 执行完毕，就是刚才所说的并行执行的缓冲队列功能。</p>

<p>过一段时间再看这个 pods，可以发现第二批已经执行结束，接下来开始创建第三批……</p>

<p><img src="assets/Fh_xD3hXSn3EwNuY6pLWrmxxB2VB" alt="avatar"></p>

<h4>Cronjob 的编排文件</h4>

<p>下面来看第三个例子 —— CronJob。 CronJob 是每分钟执行一次，每次一个 job。</p>

<p><img src="assets/FhwqFsIjlfL4aUAAEiE0uHaL2Sb1" alt="avatar"></p>

<h4>Cronjob 的创建及运行验证</h4>

<p>如下图 CronJob 已经创建了，可以通过 get cronjob 来看到当前有一个 CronJob，这个时候再来看 jobs，由于它是每分钟执行一次，所以得稍微等一下。</p>

<p><img src="assets/FjIIXBz04a7wvbYbD5plUsy7tRCf" alt="avatar"></p>

<p>同时可以看到，上一个 job 还在运行，它的时间是 2m12s 左右，它的完成度是 7/8、6/8，刚刚看到 7/8 到 8/8，也就是说我们上一个任务执行了最后一步，而且每次都是两个两个地去运行。每次两个运行的 job 都会让我们在运行一些大型工作流或者工作任务的时候感到特别的方便。</p>

<p><img src="assets/Ftwts5NgymiBDZRydR_CIcde835x" alt="avatar"></p>

<p>上图中可以看到突然出现了一个 job，“hello-xxxx”这个 job 就是刚才所说的 CronJob。它距离刚才 CronJob 提交已经过去 1 分钟了，这样就会自动创建出来一个 job，如果不去干扰它的话，它以后大概每一分钟都会创建出来这么一个 job，除非等我们什么时候指定它不可以再运行的时候它才会停止创建。</p>

<p>在这里 CronJob 其实主要是用来运作一些清理任务或者说执行一些定时任务。比如说 Jenkins 构建等方面的一些任务，会特别有效。</p>

<h3>架构设计</h3>

<h4>Job 管理模式</h4>

<p><img src="assets/FlUJ1-4Ofde5w74sDvQ5OrEL66Rb" alt="avatar"></p>

<p>我们来看一下 job 的架构设计。Job Controller 其实还是主要去创建相对应的 pod，然后 Job Controller 会去跟踪 Job 的状态，及时地根据我们提交的一些配置重试或者继续创建。同时我们刚刚也提到，每个 pod 会有它对应的 label，来跟踪它所属的 Job Controller，并且还去配置并行的创建， 并行或者串行地去创建 pod。</p>

<h4>Job 控制器</h4>

<p><img src="assets/FtfcJwcyKLYwjGBjY9qHBGSr7SSA" alt="avatar"></p>

<p>上图是一个 Job 控制器的主要流程。所有的 job 都是一个 controller，它会 watch 这个 API Server，我们每次提交一个 Job 的 yaml 都会经过 api-server 传到 ETCD 里面去，然后 Job Controller 会注册几个 Handler，每当有添加、更新、删除等操作的时候，它会通过一个内存级的消息队列，发到 controller 里面。</p>

<p>通过 Job Controller 检查当前是否有运行的 pod，如果没有的话，通过 Scale up 把这个 pod 创建出来；如果有的话，或者如果大于这个数，对它进行 Scale down，如果这时 pod 发生了变化，需要及时 Update 它的状态。</p>

<p>同时要去检查它是否是并行的 job，或者是串行的 job，根据设置的配置并行度、串行度，及时地把 pod 的数量给创建出来。最后，它会把 job 的整个的状态更新到 API Server 里面去，这样我们就能看到呈现出来的最终效果了。</p>

<h2>DaemonSet</h2>

<h3>需求来源</h3>

<h4>DaemonSet 背景问题</h4>

<p>下面介绍第二个控制器：**DaemonSet。**同样的问题：如果我们没有 DaemonSet 会怎么样？下面有几个需求：</p>

<ul>

<li>首先如果希望每个节点都运行同样一个 pod 怎么办？</li>

<li>如果新节点加入集群的时候，想要立刻感知到它，然后去部署一个 pod，帮助我们初始化一些东西，这个需求如何做？</li>

<li>如果有节点退出的时候，希望对应的 pod 会被删除掉，应该怎么操作？</li>

<li>如果 pod 状态异常的时候，我们需要及时地监控这个节点异常，然后做一些监控或者汇报的一些动作，那么这些东西运用什么控制器来做？</li>

</ul>

<h4>DaemonSet：守护进程控制器</h4>

<p>DaemonSet 也是 Kubernetes 提供的一个 default controller，它实际是做一个守护进程的控制器，它能帮我们做到以下几件事情：</p>

<ul>

<li>首先能保证集群内的每一个节点都运行一组相同的 pod；</li>

<li>同时还能根据节点的状态保证新加入的节点自动创建对应的 pod；</li>

<li>在移除节点的时候，能删除对应的 pod；</li>

<li>而且它会跟踪每个 pod 的状态，当这个 pod 出现异常、Crash 掉了，会及时地去 recovery 这个状态。</li>

</ul>

<h3>用例解读</h3>

<h4>DaemonSet 语法</h4>

<p>下面举个例子来看一下，DaemonSet.yaml 会稍微长一些。</p>

<p><img src="assets/Fn0gBsJY1Ugr9_M7kNFjcKV3O-0D" alt="avatar"></p>

<p>首先是 kind:DaemonSet。如果前面学过 deployment 后，其实我们再看这个 yaml 会比较简单。例如它会有 matchLabel，通过 matchLabel 去管理对应所属的 pod，这个 pod.label 也要和这个 DaemonSet.controller.label 想匹配，它才能去根据 label.selector 去找到对应的管理 Pod。下面 spec.container 里面的东西都是一致的。</p>

<p>这里用 fluentd 来做例子。DaemonSet 最常用的点在于以下几点内容：</p>

<ul>

<li>首先是存储，GlusterFS 或者 Ceph 之类的东西，需要每台节点上都运行一个类似于 Agent 的东西，DaemonSet 就能很好地满足这个诉求；</li>

<li>另外，对于日志收集，比如说 logstash 或者 fluentd，这些都是同样的需求，需要每台节点都运行一个 Agent，这样的话，我们可以很容易搜集到它的状态，把各个节点里面的信息及时地汇报到上面；</li>

<li>还有一个就是，需要每个节点去运行一些监控的事情，也需要每个节点去运行同样的事情，比如说 Promethues 这些东西，也需要 DaemonSet 的支持。</li>

</ul>

<h4>查看 DaemonSet 状态</h4>

<p><img src="assets/FukQvePjfaEKTrUJeuGIOznO4Qf5" alt="avatar"></p>

<p>创建完 DaemonSet 之后，我们可以使用 kubectl get DaemonSet（DaemonSet 缩写为 ds）。可以看到 DaemonSet 返回值和 deployment 特别像，即它当前一共有正在运行的几个，然后我们需要几个，READY 了几个。当然这里面，READY 都是只有 Pod，所以它最后创建出来所有的都是 pod。</p>

<p>这里有几个参数，分别是：需要的 pod 个数、当前已经创建的 pod 个数、就绪的个数，以及所有可用的、通过健康检查的 pod；还有 NODE SELECTOR，因为 NODE SELECTOR 在 DaemonSet 里面非常有用。有时候我们可能希望只有部分节点去运行这个 pod 而不是所有的节点，所以有些节点上被打了标的话，DaemonSet 就只运行在这些节点上。比如，我只希望 master 节点运行某些 pod，或者只希望 Worker 节点运行某些 pod，就可以使用这个 NODE SELECTOR。</p>

<h4>更新 DaemonSet</h4>

<p><img src="assets/FgVuatnFylUD_LRK6upX1PxBbwIQ" alt="avatar"></p>

<p>其实 DaemonSet 和 deployment 特别像，它也有两种更新策略：一个是 <strong>RollingUpdate</strong>，另一个是 <strong>OnDelete</strong>。</p>

<ul>

<li>RollingUpdate 其实比较好理解，就是会一个一个的更新。先更新第一个 pod，然后老的 pod 被移除，通过健康检查之后再去见第二个 pod，这样对于业务上来说会比较平滑地升级，不会中断；</li>

<li>OnDelete 其实也是一个很好的更新策略，就是模板更新之后，pod 不会有任何变化，需要我们手动控制。我们去删除某一个节点对应的 pod，它就会重建，不删除的话它就不会重建，这样的话对于一些我们需要手动控制的特殊需求也会有特别好的作用。</li>

</ul>

<h3>操作演示</h3>

<h4>DaemonSet 的编排</h4>

<p>下面举一个例子。比如说我们去改了些 DaemonSet 的镜像，然后看到了它的状态，它就会去一个一个地更新。</p>

<p><img src="assets/FvUc47P919bgEmefYt1kzYntI2ax" alt="avatar"></p>

<p>上图这个就是刚才 DaemonSet 的 yaml，会比刚才会多一些， 我们做一些资源的限制，这个都不影响。</p>

<h4>DaemonSet 的创建与运行验证</h4>

<p>下面我们创建一下 DaemonSet ，然后再看一下它的状态。下图就是我们刚才看到的 DaemonSet 在 ready 里打出来的状态。</p>

<p><img src="assets/FvwwDrdF39A-IWcrKl5pQZBtEdpg" alt="avatar"></p>

<p>从下图中可以看到，一共有 4 个 pod 被创建出来。为什么是 4 个 pod呢？因为只有 4 个节点，所以每个节点上都会运行一个对应的 pod。</p>

<p><img src="assets/FtUY1TTz--z4l7YMXZEbXZ8yDSNn" alt="avatar"></p>

<h4>DaemonSet 的更新</h4>

<p>这时，我们来更新 DaemonSet， 执行完了kubectl apply -f 后，它的 DaemonSet 就已经更新了。接下来我们去查看 DaemonSet 的更新状态。</p>

<p><img src="assets/FtvHBGbpXhkp9Pq9PpJWmgGWrP5l" alt="avatar"></p>

<p>上图中可以看到：DaemonSet 默认这个是 RollingUpdate 的，我们看到是 0-4，现在是 1-4，也就是说它在更新第一个，第一个更新完成会去更新第二个，第二个更新完，就更新第三个……这个就是 RollingUpdate。RollingUpdate 可以做到全自动化的更新，不用有人值守，而是一个一个地去自动更新，更新的过程也比较平滑，这样可以有利于我们在现场发布或者做一些其他操作。</p>

<p>上图结尾处可以看到，整个的 DaemonSet 已经 RollingUpdate 完毕。</p>

<h3>架构设计</h3>

<h4>DaemonSet 管理模式</h4>

<p><img src="assets/FqqpxDD5zywwJs1YPoFiBFtWN0D_" alt="avatar"></p>

<p>接下来看一下 DaemonSet 架构设计。DaemonSet 还是一个 controller，它最后真正的业务单元也是 Pod，DaemonSet 其实和 Job controller 特别相似，它也是通过 controller 去 watch API Server 的状态，然后及时地添加 pod。唯一不同的是，它会监控节点的状态，节点新加入或者消失的时候会在节点上创建对应的 pod，然后同时根据你配置的一些 affinity 或者 label 去选择对应的节点。</p>

<h4>DaemonSet 控制器</h4>

<p><img src="assets/FlgZBztBwWxmJ2lFOPMD5NBnQZN5" alt="avatar"></p>

<p>最后我们来看一下 DaemonSet 的控制器，DaemonSet 其实和 Job controller 做的差不多：两者都需要根据 watch 这个 API Server 的状态。现在 DaemonSet 和 Job controller 唯一的不同点在于，DaemonsetSet Controller需要去 watch node 的状态，但其实这个 node 的状态还是通过 API Server 传递到 ETCD 上。</p>

<p>当有 node 状态节点发生变化时，它会通过一个内存消息队列发进来，然后DaemonSet controller 会去 watch 这个状态，看一下各个节点上是都有对应的 Pod，如果没有的话就去创建。当然它会去做一个对比，如果有的话，它会比较一下版本，然后加上刚才提到的是否去做 RollingUpdate？如果没有的话就会重新创建，Ondelete 删除 pod 的时候也会去做 check 它做一遍检查，是否去更新，或者去创建对应的 pod。</p>

<p>当然最后的时候，如果全部更新完了之后，它会把整个 DaemonSet 的状态去更新到 API Server 上，完成最后全部的更新。</p>

<h2>本节总结</h2>

<ul>

<li>Job &amp; CronJobs 基础操作与概念解析：本节详细介绍了 Job 和 CronJob 的概念，并通过两个实际的例子介绍了 Job 和 CronJob 的使用，对于 Job 和 CronJob 内的各种功能便签都进行了详细的演示；</li>

<li>DaemonSet 基础操作与概念解析：通过类比 Deployment 控制器，我们理解了一下 DaemonSet 控制器的工作流程与方式，并且通过对 DaemonSet 的更新了解了滚动更新的概念和相对应的操作方式。</li>

</ul>

</div>

                    </div><div class="book-post">

                        <p id="tip" align="center"></p>

                        <div><h1>08 应用配置管理</h1>

<p><strong>本节课程要点</strong></p>

<ol>

<li>ConfigMaps 和 Secret 资源的创建和使用；</li>

<li>Pod 身份认证的实现和原理；</li>

<li>容器资源、安全、前置校验等配置和使用。</li>

</ol>

<p>细分为以下八个方面：</p>

<p><img src="assets/Fjsb4Hne8BIfKR3lDG8UBDnYGIhm" alt="avatar"></p>

<h2>需求来源</h2>

<h3>背景问题</h3>

<p>首先一起来看一下需求来源。大家应该都有过这样的经验，就是用一个容器镜像来启动一个 container。要启动这个容器，其实有很多需要配套的问题待解决：</p>

<ul>

<li>第一，比如说一些可变的配置。因为我们不可能把一些可变的配置写到镜像里面，当这个配置需要变化的时候，可能需要我们重新编译一次镜像，这个肯定是不能接受的；</li>

<li>第二就是一些敏感信息的存储和使用。比如说应用需要使用一些密码，或者用一些 token；</li>

<li>第三就是我们容器要访问集群自身。比如我要访问 kube-apiserver，那么本身就有一个身份认证的问题；</li>

<li>第四就是容器在节点上运行之后，它的资源需求；</li>

<li>第五个就是容器在节点上，它们是共享内核的，那么它的一个安全管控怎么办？</li>

<li>最后一点我们说一下容器启动之前的一个前置条件检验。比如说，一个容器启动之前，我可能要确认一下 DNS 服务是不是好用？又或者确认一下网络是不是联通的？那么这些其实就是一些前置的校验。</li>

</ul>

<h3>Pod 的配置管理</h3>

<p>在 Kubernetes 里面，它是怎么做这些配置管理的呢？如下图所示：</p>

<p><img src="assets/Fqq-GY63MRwTa0dMQJUYjH_V09c5" alt="avatar"></p>

<ul>

<li>可变配置就用 ConfigMap；</li>

<li>敏感信息是用 Secret；</li>

<li>身份认证是用 ServiceAccount 这几个独立的资源来实现的；</li>

<li>资源配置是用 Resources；</li>

<li>安全管控是用 SecurityContext；</li>

<li>前置校验是用 InitContainers 这几个在 spec 里面加的字段，来实现的这些配置管理。</li>

</ul>

<h2>ConfigMap</h2>

<h3>ConfigMap 介绍</h3>

<p>下面我们来介绍第一个部分，就是 ConfigMap。我们先来介绍 ConfigMap 它是用来做什么的、以及它带来的一个好处。它其实主要是管理一些可变配置信息，比如说我们应用的一些配置文件，或者说它里面的一些环境变量，或者一些命令行参数。</p>

<p>它的好处在于它可以让一些可变配置和容器镜像进行解耦，这样也保证了容器的可移植性。看一下下图中右边的编排文件截图。</p>

<p><img src="assets/Ft1ZSVs2DzkFQF5sPF5WW4nGezPx" alt="avatar"></p>

<p>这是 ConfigMap 本身的一个定义，它包括两个部分：一个是 ConfigMap 元信息，我们关注 name 和 namespace 这两个信息。接下来这个 data 里面，可以看到它管理了两个配置文件。它的结构其实是这样的：从名字看ConfigMap中包含Map单词，Map 其实就是 key:value，key 是一个文件名，value 是这个文件的内容。</p>

<h3>ConfigMap 创建</h3>

<p>看过介绍之后，再具体看一下它是怎么创建的。我们推荐用 kubectl 这个命令来创建，它带的参数主要有两个：一个是指定 name，第二个是 DATA。其中 DATA 可以通过指定文件或者指定目录，以及直接指定键值对，下面可以看一下这个例子。</p>

<p><img src="assets/FvrNuDtWTmrInGsc7UqXxhzSB-sq" alt="avatar"></p>

<p>指定文件的话，文件名就是 Map 中的 key，文件内容就是 Map 中的 value。然后指定键值对就是指定数据键值对，即：key:value 形式，直接映射到 Map 的key:value。</p>

<h3>ConfigMap 使用</h3>

<p>创建完了之后，应该怎么使用呢？</p>

<p><img src="assets/FqyXJnlGMqWcgjH3qzTp4Ell1fIV" alt="avatar"></p>

<p>如上图所示，主要是在 pod 里来使用 ConfigMap：</p>

<ul>

<li>第一种是环境变量。环境变量的话通过 valueFrom，然后 ConfigMapKeyRef 这个字段，下面的 name 是指定 ConfigMap 名，key 是 ConfigMap.data 里面的 key。这样的话，在 busybox 容器启动后容器中执行 env 将看到一个 SPECIAL<em>LEVEL</em>KEY 环境变量；</li>

<li>第二个是命令行参数。命令行参数其实是第一行的环境变量直接拿到 cmd 这个字段里面来用；</li>

<li>最后一个是通过 volume 挂载的方式直接挂到容器的某一个目录下面去。上面的例子是把 special-config 这个 ConfigMap 里面的内容挂到容器里面的 /etc/config 目录下，这个也是使用的一种方式。</li>

</ul>

<h3>ConfigMap 注意要点</h3>

<p>现在对 ConfigMap 的使用做一个总结，以及它的一些注意点，注意点一共列了以下五条：</p>

<ol>

<li>ConfigMap 文件的大小。虽然说 ConfigMap 文件没有大小限制，但是在 ETCD 里面，数据的写入是有大小限制的，现在是限制在 1MB 以内；</li>

<li>第二个注意点是 pod 引入 ConfigMap 的时候，必须是相同的 Namespace 中的 ConfigMap，前面其实可以看到，ConfigMap.metadata 里面是有 namespace 字段的；</li>

<li>第三个是 pod 引用的 ConfigMap。假如这个 ConfigMap 不存在，那么这个 pod 是无法创建成功的，其实这也表示在创建 pod 前，必须先把要引用的 ConfigMap 创建好；</li>

<li>第四点就是使用 envFrom 的方式。把 ConfigMap 里面所有的信息导入成环境变量时，如果 ConfigMap 里有些 key 是无效的，比如 key 的名字里面带有数字，那么这个环境变量其实是不会注入容器的，它会被忽略。但是这个 pod 本身是可以创建的。这个和第三点是不一样的方式，是 ConfigMap 文件存在基础上，整体导入成环境变量的一种形式；</li>

<li>最后一点是：什么样的 pod 才能使用 ConfigMap？这里只有通过 K8s api 创建的 pod 才能使用 ConfigMap，比如说通过用命令行 kubectl 来创建的 pod，肯定是可以使用 ConfigMap 的，但其他方式创建的 pod，比如说 kubelet 通过 manifest 创建的 static pod，它是不能使用 ConfigMap 的。</li>

</ol>

<h2>Secret</h2>

<h3>Secret 介绍</h3>

<p>现在我们讲一下 Secret，Secret 是一个主要用来存储密码 token 等一些敏感信息的资源对象。其中，敏感信息是采用 base-64 编码保存起来的，我们来看下图中 Secret 数据的定义。</p>

<p><img src="assets/FgAZG4jdQ3l_Ef-PpNIWvhJHNAOd" alt="avatar"></p>

<p>元数据的话，里面主要是 name、namespace 两个字段；接下来是 type，它是非常重要的一个字段，是指 Secret 的一个类型。Secret 类型种类比较多，下面列了常用的四种类型：</p>

<ul>

<li>第一种是 Opaque，它是普通的 Secret 文件；</li>

<li>第二种是 service-account-token，是用于 service-account 身份认证用的 Secret；</li>

<li>第三种是 dockerconfigjson，这是拉取私有仓库镜像的用的一种 Secret；</li>

<li>第四种是 bootstrap.token，是用于节点接入集群校验用的 Secret。</li>

</ul>

<p>再接下来是 data，是存储的 Secret 的数据，它也是 key-value 的形式存储的。</p>

<h3>Secret 创建</h3>

<p>接下来我们看一下 Secret 的创建。</p>

<p><img src="assets/Fscn9lbK1vZiMXu46Gaz0DWwTldC" alt="avatar"></p>

<p>如上图所示，有两种创建方式：</p>

<ul>

<li>系统创建：比如 K8s 为每一个 namespace 的默认用户（default ServiceAccount）创建 Secret；</li>

<li>用户手动创建：手动创建命令，推荐 kubectl 这个命令行工具，它相对 ConfigMap 会多一个 type 参数。其中 data 也是一样，它也是可以指定文件和键值对的。type 的话，要是你不指定的话，默认是 Opaque 类型。</li>

</ul>

<p>上图中两个例子。第一个是通过指定文件，创建了一个拉取私有仓库镜像的 Secret，指定的文件是 /root/.docker/config.json。type 的话指定的是 dockerconfigjson，另外一个我们指定键值对，我们 type 没有指定，默认是 Opaque。键值对是 key:value 的形式，其中对 value 内容进行 base64 加密。创建 Secret 就是这么一个情况。</p>

<h3>Secret 使用</h3>

<p>创建完 Secret 之后，再来看一下如何使用它。它主要是被 pod 来使用，一般是通过 volume 形式挂载到容器里指定的目录，然后容器里的业务进程再到目录下读取 Secret 来进行使用。另外在需要访问私有镜像仓库时，也是通过引用 Secret 来实现。</p>

<p><img src="assets/FsdNm_pTBcnlLZZfu46xrUPogZ65" alt="avatar"></p>

<p>我们先来看一下挂载到用户指定目录的方式：</p>

<ul>

<li>第一种方式：如上图左侧所示，用户直接指定，把 mysecret 挂载到容器 /etc/foo 目录下面；</li>

<li>第二种方式：如上图右侧所示，系统自动生成，把 serviceaccount-secret 自动挂载到容器 /var/run/secrets/kubernetes.io/serviceaccount 目录下，它会生成两个文件，一个是 ca.crt，一个是 token。这是两个保存了认证信息的证书文件。</li>

</ul>

<h3>使用私有镜像库</h3>

<p>下面看一下用 Secret 来使用私有镜像仓库。首先，私有镜像仓库的信息是存储在 Secret 里面的(具体参照上述的Secret创建章节)，然后拉取私有仓库镜像，那么通过下图中两种方法的配置就可以：</p>

<ul>

<li>第一种方式：如下图左侧所示，直接在 pod 里面，通过 imagePullSecrets 字段来配置；</li>

<li>第二种方式是自动注入。用户提前在 pod 会使用的 serviceaccount 里配置 imagePullSecrets，Pod创建时系统自动注入这个 imagePullSecrets。</li>

</ul>

<p><img src="assets/FvaOlw27b0oI3XT5U4oiAZqSC8Hj" alt="avatar"></p>

<h3>Secret 使用注意要点</h3>

<p>最后来看一下 Secret 使用的一些注意点，下面列了三点：</p>

<ol>

<li>第一个是 Secret 的文件大小限制。这个跟 ConfigMap 一样，也是 1MB；</li>

<li>第二个是 Secret 采用了 base-64 编码，但是它跟明文也没有太大区别。所以说，如果有一些机密信息要用 Secret 来存储的话，还是要很慎重考虑。也就是说谁会来访问你这个集群，谁会来用你这个 Secret，还是要慎重考虑，因为它如果能够访问这个集群，就能拿到这个 Secret。</li>

</ol>

<p>如果是对 Secret 敏感信息要求很高，对加密这块有很强的需求，推荐可以使用 Kubernetes 和开源的 vault做一个解决方案，来解决敏感信息的加密和权限管理。</p>

<ol>

<li>第三个就是 Secret 读取的最佳实践，建议不要用 list/watch，如果用 list/watch 操作的话，会把 namespace 下的所有 Secret 全部拉取下来，这样其实暴露了更多的信息。推荐使用 GET 的方法，这样只获取你自己需要的那个 Secret。</li>

</ol>

<h2>ServiceAccount</h2>

<h3>ServiceAccount 介绍</h3>

<p>接下来，我们讲一下 ServiceAccount。ServiceAccount 首先是用于解决 pod 在集群里面的身份认证问题，身份认证信息是存在于 Secret 里面。</p>

<p><img src="assets/FlztCbpkZqPleUn4jWjlpTK1yR9K" alt="avatar"></p>

<p>先看一下上面的左侧截图，可以看到最下面的红框里，有一个 Secret 字段，它指定 ServiceAccount 用哪一个 Secret，这个是 K8s 自动为 ServiceAccount 加上的。然后再来看一下上图中的右侧截图，它对应的 Secret 的 data 里有两块数据，一个是 ca.crt，一个是 token。ca.crt 用于对服务端的校验，token 用于 Pod 的身份认证，它们都是用 base64 编码过的。然后可以看到 metadata 即元信息里，其实是有关联 ServiceAccount 信息的（这个 secret 被哪个 ServiceAccount 使用）。最后我们注意一下 type，这个就是 service-account-token 这种类型。</p>

<h3>举例：Pod 里的应用访问它所属的 K8s 集群</h3>

<p>介绍完 ServiceAccount 以及它对应的 secret 后，我们来看一下，pod 是怎么利用 ServiceAccount 或者说它是怎么利用 secret 来访问所属 K8s 集群的。</p>

<p>其实 pod 创建的时候，首先它会把这个 secret 挂载到容器固定的目录下，这是 K8s 功能上实现的。它要把这个 ca.crt 和 token 这两个文件挂载到固定目录下面。</p>

<p>pod 要访问集群的时候，它是怎么来利用这个文件的呢？我们看一下下面的代码截图：</p>

<p><img src="assets/FjIXm36m_N0xMO22XI3Msds0gfjv" alt="avatar"></p>

<p>我们在 Go 里面实现 Pod 访问 K8s 集群时，一般直接会调一个 InClusterConfig 方法，来生成这个访问服务 Client 的一些信息。然后可以看一下，最后这个 Config 里面有两部分信息：</p>

<ul>

<li>一个是 tlsClientConfig，这个主要是用于 ca.crt 校验服务端；</li>

<li>第二个是 Bearer Token，这个就是 pod 的身份认证。在服务端，会利用 token 对 pod 进行一个身份认证。</li>

</ul>

<p>再次回到上图左侧。认证完之后 pod 的身份信息会有两部分：一个是 Group，一个是 User。身份认证是就是认证这两部分信息。接着可以使用 RBAC 功能，对 pod 进行一个授权管理。</p>

<p>假如 RBAC 没有配置的话，默认的 pod 具有资源 GET 权限，就是可以从所属的 K8s 集群里 get 数据。如果是需要更多的权限，那么就需要 自行配置 RBAC 。RBAC 的相关知识，我们在后面的课程里面会详细介绍，大家可以关注一下。</p>

<h2>Resource</h2>

<h3>容器资源配合管理</h3>

<p>下面介绍一下 Resource，即：容器的一个资源配置管理。</p>

<p>目前内部支持类型有三种：CPU、内存，以及临时存储。当用户觉得这三种不够，有自己的一些资源，比如说 GPU，或者其他资源，也可以自己来定义，但配置时，指定的数量必须为整数。目前资源配置主要分成 request 和 limit 两种类型，一个是需要的数量，一个是资源的界限。CPU、内存以及临时存储都是在 container 下的 Resource 字段里进行一个声明。</p>

<p><img src="assets/FoChGRBMolmK6AuzgDMF3Ug4Jm30" alt="avatar"></p>

<p>举个例子，wordpress 容器的资源需求，一个是 request ，一个是 limits，它分别对需要的资源和资源临界进行一个声明。</p>

<h3>Pod 服务质量 (QoS) 配置</h3>

<p>根据 CPU 对容器内存资源的需求，我们对 pod 的服务质量进行一个分类，分别是 Guaranteed、Burstable 和 BestEffort。</p>

<ul>

<li>Guaranteed ：pod 里面每个容器都必须有内存和 CPU 的 request 以及 limit 的一个声明，且 request 和 limit 必须是一样的，这就是 Guaranteed；</li>

<li>Burstable：Burstable 至少有一个容器存在内存和 CPU 的一个 request；</li>

<li>BestEffort：只要不是 Guaranteed 和 Burstable，那就是 BestEffort。</li>

</ul>

<p>那么这个服务质量是什么样的呢？资源配置好后，当这个节点上 pod 容器运行，比如说节点上 memory 配额资源不足，kubelet会把一些低优先级的，或者说服务质量要求不高的（如：BestEffort、Burstable）pod 驱逐掉。它们是按照先去除 BestEffort，再去除 Burstable 的一个顺序来驱逐 pod 的。</p>

<h2>SecurityContext</h2>

<h3>SecurityContext 介绍</h3>

<p>SecurityContext 主要是用于限制容器的一个行为，它能保证系统和其他容器的安全。这一块的能力不是 Kubernetes 或者容器 runtime 本身的能力，而是 Kubernetes 和 runtime 通过用户的配置，最后下传到内核里，再通过内核的机制让 SecurityContext 来生效。所以这里讲的内容，会比较简单或者说比较抽象一点。</p>

<p>SecurityContext 主要分为三个级别：</p>

<ul>

<li>第一个是容器级别，仅对容器生效；</li>

<li>第二个是 pod 级别，对 pod 里所有容器生效；</li>

<li>第三个是集群级别，就是 PSP，对集群内所有 pod 生效。</li>

</ul>

<p>权限和访问控制设置项，现在一共列有七项（这个数量后续可能会变化）：</p>

<ol>

<li>第一个就是通过用户 ID 和组 ID 来控制文件访问权限；</li>

<li>第二个是 SELinux，它是通过策略配置来控制用户或者进程对文件的访问控制；</li>

<li>第三个是特权容器；</li>

<li>第四个是 Capabilities，它也是给特定进程来配置一个 privileged 能力；</li>

<li>第五个是 AppArmor，它也是通过一些配置文件来控制可执行文件的一个访问控制权限，比如说一些端口的读写；</li>

<li>第六个是一个对系统调用的控制；</li>

<li>第七个是对子进程能否获取比父亲更多的权限的一个限制。</li>

</ol>

<p>最后其实都是落到内核来控制它的一些权限。</p>

<p><img src="assets/FtKrtLFM1bSpO8fc51PWMvnEyB-v" alt="avatar"></p>

<p>上图是对 pod 级别和容器级别配置 SecurityContext 的一个例子，如果大家对这些内容有更多的需求，可以根据这些信息去搜索更深入的资料来学习。</p>

<h2>InitContainer</h2>

<h3>InitContainer 介绍</h3>

<p>接下来看一下 InitContainer，首先介绍 InitContainer 和普通 container 的区别，有以下三点内容：</p>

<ol>

<li>InitContainer 首先会比普通 container 先启动，并且直到所有的 InitContainer 执行成功后，普通 container 才会被启动；</li>

<li>InitContainer 之间是按定义的次序去启动执行的，执行成功一个之后再执行第二个，而普通的 container 是并发启动的；</li>

<li>InitContainer 执行成功后就结束退出，而普通容器可能会一直在执行。它可能是一个 longtime 的，或者说失败了会重启，这个也是 InitContainer 和普通 container 不同的地方。</li>

</ol>

<p>根据上面三点内容，我们看一下 InitContainer 的一个用途。它其实主要为普通 container 服务，比如说它可以为普通 container 启动之前做一个初始化，或者为它准备一些配置文件， 配置文件可能是一些变化的东西。再比如做一些前置条件的校验，如网络是否联通。</p>

<p><img src="assets/FvTri6pAVaBUBv3tFPeabdtIlG1j" alt="avatar"></p>

<p>上面的截图是 flannel 组件的 InitContainer 的一个配置，它的 InitContainer 主要是为 kube-flannel 这个普通容器启动之前准备一些网络配置文件。</p>

<h2>结束语</h2>

<ul>

<li>ConfigMap 和 Secret: 首先介绍了 ConfigMap 和 Secret 的创建方法和使用场景，然后对 ConfigMap 和 Secret 的常见使用注意点进行了分类和整理。最后介绍了私有仓库镜像的使用和配置；</li>

<li>Pod 身份认证: 首先介绍了 ServiceAccount 和 Secret 的关联关系，然后从源码角度对 Pod 身份认证流程和实现细节进行剖析，同时引出了 Pod 的权限管理(即 RBAC 的配置管理)；</li>

<li>容器资源和安全： 首先介绍了容器常见资源类型 (CPU/Memory) 的配置，然后对 Pod 服务质量分类进行详细的介绍。同时对 SecurityContext 有效层级和权限配置项进行简要说明；</li>

<li>InitContainer: 首先介绍了 InitContainer 和普通 container 的区别以及 InitContainer 的用途。然后基于实际用例对 InitContainer 的用途进行了说明。</li>

</ul>

<p>好的，我们今天的内容讲到这里，谢谢大家。</p>

</div>

                    </div><div class="book-post">

                        <p id="tip" align="center"></p>

                        <div><h1>09 应用存储和持久化数据卷：核心知识</h1>

<h2>Volumes 介绍</h2>

<h3>Pod Volumes</h3>

<p>首先来看一下 Pod Volumes 的使用场景：</p>

<ul>

<li>场景一：如果 pod 中的某一个容器在运行时异常退出，被 kubelet 重新拉起之后，如何保证之前容器产生的重要数据没有丢失？</li>

<li>场景二：如果同一个 pod 中的多个容器想要共享数据，应该如何去做？</li>

</ul>

<p>以上两个场景，其实都可以借助 Volumes 来很好地解决，接下来首先看一下 Pod Volumes 的常见类型：</p>

<ol>

<li>本地存储，常用的有 emptydir/hostpath；</li>

<li>网络存储：网络存储当前的实现方式有两种，一种是 in-tree，它的实现的代码是放在 K8s 代码仓库中的，随着k8s对存储类型支持的增多，这种方式会给k8s本身的维护和发展带来很大的负担；而第二种实现方式是 out-of-tree，它的实现其实是给 K8s 本身解耦的，通过抽象接口将不同存储的driver实现从k8s代码仓库中剥离，因此out-of-tree 是后面社区主推的一种实现网络存储插件的方式；</li>

<li>Projected Volumes：它其实是将一些配置信息，如 secret/configmap 用卷的形式挂载在容器中，让容器中的程序可以通过POSIX接口来访问配置数据；</li>

<li>PV 与 PVC 就是今天要重点介绍的内容。</li>

</ol>

<h3>Persistent Volumes</h3>

<p><img src="assets/FmJuAEtK_8fagBqyLSp53h2zgIg3" alt="avatar"></p>

<p>接下来看一下 PV（Persistent Volumes）。既然已经有了 Pod Volumes，为什么又要引入 PV 呢？我们知道 pod 中声明的 volume 生命周期与 pod 是相同的，以下有几种常见的场景：</p>

<ul>

<li>场景一：pod 重建销毁，如用 Deployment 管理的 pod，在做镜像升级的过程中，会产生新的 pod并且删除旧的 pod ，那新旧 pod 之间如何复用数据？</li>

<li>场景二：宿主机宕机的时候，要把上面的 pod 迁移，这个时候 StatefulSet 管理的 pod，其实已经实现了带卷迁移的语义。这时通过 Pod Volumes 显然是做不到的；</li>

<li>场景三：多个 pod 之间，如果想要共享数据，应该如何去声明呢？我们知道，同一个 pod 中多个容器想共享数据，可以借助 Pod Volumes 来解决；当多个 pod 想共享数据时，Pod Volumes 就很难去表达这种语义；</li>

<li>场景四：如果要想对数据卷做一些功能扩展性，如：snapshot、resize 这些功能，又应该如何去做呢？</li>

</ul>

<p>以上场景中，通过 Pod Volumes 很难准确地表达它的复用/共享语义，对它的扩展也比较困难。因此 K8s 中又引入了 **Persistent Volumes **概念，它可以将存储和计算分离，通过不同的组件来管理存储资源和计算资源，然后解耦 pod 和 Volume 之间生命周期的关联。这样，当把 pod 删除之后，它使用的PV仍然存在，还可以被新建的 pod 复用。</p>

<h3>PVC 设计意图</h3>

<p><img src="assets/FvnzsKTaEYFgKeolFgUOmDcJqHed" alt="avatar"></p>

<p>了解 PV 后，应该如何使用它呢？</p>

<p>用户在使用 PV 时其实是通过 PVC，为什么有了 PV 又设计了 PVC 呢？主要原因是为了简化K8s用户对存储的使用方式，做到职责分离。通常用户在使用存储的时候，只用声明所需的存储大小以及访问模式。</p>

<p>访问模式是什么？其实就是：我要使用的存储是可以被多个node共享还是只能单node独占访问(注意是node level而不是pod level)？只读还是读写访问？用户只用关心这些东西，与存储相关的实现细节是不需要关心的。</p>

<p>通过 PVC 和 PV 的概念，将用户需求和实现细节解耦开，用户只用通过 PVC 声明自己的存储需求。PV是有集群管理员和存储相关团队来统一运维和管控，这样的话，就简化了用户使用存储的方式。可以看到，PV 和 PVC 的设计其实有点像面向对象的接口与实现的关系。用户在使用功能时，只需关心用户接口，不需关心它内部复杂的实现细节。</p>

<p>既然 PV 是由集群管理员统一管控的，接下来就看一下 PV 这个对象是怎么产生的。</p>

<h3>Static Volume Provisioning</h3>

<p>第一种产生方式：静态产生方式 - 静态 Provisioning。</p>

<p><img src="assets/FoFecEsyKiE9My9MHytiChC-hU4Y" alt="avatar"></p>

<p>静态 Provisioning：由集群管理员事先去规划这个集群中的用户会怎样使用存储，它会先预分配一些存储，也就是预先创建一些 PV；然后用户在提交自己的存储需求（也就是 PVC）的时候，K8s 内部相关组件会帮助它把 PVC 和 PV 做绑定；之后用户再通过 pod 去使用存储的时候，就可以通过 PVC 找到相应的 PV，它就可以使用了。</p>

<p>静态产生方式有什么不足呢？可以看到，首先需要集群管理员预分配，预分配其实是很难预测用户真实需求的。举一个最简单的例子：如果用户需要的是 20G，然而集群管理员在分配的时候可能有 80G 、100G 的，但没有 20G 的，这样就很难满足用户的真实需求，也会造成资源浪费。有没有更好的方式呢？</p>

<h3>Dynamic Volume Provisioning</h3>

<p>第二种访问方式：动态 Dynamic Provisioning。</p>

<p><img src="assets/FtjZGrp4T4up6CRjAeHJAtWjybUa" alt="avatar"></p>

<p>动态供给是什么意思呢？就是说现在集群管理员不预分配 PV，他写了一个模板文件，这个模板文件是用来表示创建某一类型存储（块存储，文件存储等）所需的一些参数，这些参数是用户不关心的，给存储本身实现有关的参数。用户只需要提交自身的存储需求，也就是PVC文件，并在 PVC 中指定使用的存储模板（StorageClass）。</p>

<p>K8s 集群中的管控组件，会结合 PVC 和 StorageClass 的信息动态，生成用户所需要的存储（PV），将 PVC 和 PV 进行绑定后，pod 就可以使用 PV 了。通过 StorageClass 配置生成存储所需要的存储模板，再结合用户的需求动态创建 PV 对象，做到按需分配，在没有增加用户使用难度的同时也解放了集群管理员的运维工作。</p>

<h2>用例解读</h2>

<p>接下来看一下 Pod Volumes、PV、PVC 及 StorageClass 具体是如何使用的。</p>

<h3>Pod Volumes 的使用</h3>

<p><img src="assets/FhstZbg24CMthwpyzKZLk28XMbDH" alt="avatar"></p>

<p>首先来看一下 Pod Volumes 的使用。如上图左侧所示，我们可以在 pod yaml 文件中的 Volumes 字段中，声明我们卷的名字以及卷的类型。声明的两个卷，一个是用的是 emptyDir，另外一个用的是 hostPath，这两种都是本地卷。在容器中应该怎么去使用这个卷呢？它其实可以通过 volumeMounts 这个字段，volumeMounts 字段里面指定的 name 其实就是它使用的哪个卷，mountPath 就是容器中的挂载路径。</p>

<p>这里还有个 subPath，subPath 是什么？</p>

<p>先看一下，这两个容器都指定使用了同一个卷，就是这个 cache-volume。那么，在多个容器共享同一个卷的时候，为了隔离数据，我们可以通过 subPath 来完成这个操作。它会在卷里面建立两个子目录，然后容器 1 往 cache 下面写的数据其实都写在子目录 cache1 了，容器 2 往 cache 写的目录，其数据最终会落在这个卷里子目录下面的 cache2 下。</p>

<p>还有一个 readOnly 字段，readOnly 的意思其实就是只读挂载，这个挂载你往挂载点下面实际上是没有办法去写数据的。</p>

<p>另外emptyDir、hostPath 都是本地存储，它们之间有什么细微的差别呢？emptyDir 其实是在 pod 创建的过程中会临时创建的一个目录，这个目录随着 pod 删除也会被删除，里面的数据会被清空掉；hostPath 顾名思义，其实就是宿主机上的一个路径，在 pod 删除之后，这个目录还是存在的，它的数据也不会被丢失。这就是它们两者之间一个细微的差别。</p>

<h3>静态 PV 使用</h3>

<p><img src="assets/FrZFcplmkDDCGzqKSiuZtS74yHPW" alt="avatar"></p>

<p>接下来再看一下，PV 和 PVC 是怎么使用的。</p>

<p>先看一个静态 PV 创建方式。静态 PV 的话，首先是由管理员来创建的，管理员我们这里以 NAS，就是阿里云文件存储为例。我需要先在阿里云的文件存储控制台上去创建 NAS 存储，然后把 NAS 存储的相关信息要填到 PV 对象中，这个 PV 对象预创建出来后，用户可以通过 PVC 来声明自己的存储需求，然后再去创建 pod。创建 pod 还是通过我们刚才讲解的字段把存储挂载到某一个容器中的某一个挂载点下面。</p>

<p>那么接下来看一下 yaml 怎么写。集群管理员首先是在云存储厂商那边先去把存储创建出来，然后把相应的信息填写到 PV 对象中。</p>

<p><img src="assets/Fl5h3c_gIfVNlpfn09bxn5UL7R23" alt="avatar"></p>

<p>刚刚创建的阿里云 NAS 文件存储对应的PV，有个比较重要的字段：capacity，即创建的这个存储的大小，accessModes，创建出来的这个存储它的访问方式，我们后面会讲解总共有几种访问方式。</p>

<p>然后有个 ReclaimPolicy，ReclaimPolicy 的意思就是：这块存储在被使用后，等它的使用方 pod 以及 PVC 被删除之后，这个 PV 是应该被删掉还是被保留呢？其实就是PV的回收策略。</p>

<p>接下来看看用户怎么去使用该PV对象。用户在使用存储的时候，需要先创建一个 PVC 对象。PVC 对象里面，只需要指定存储需求，不用关心存储本身的具体实现细节。存储需求包括哪些呢？首先是需要的大小，也就是 resources.requests.storage；然后是它的访问方式，即需要这个存储的访问方式，这里声明为ReadWriteMany，也即支持多node读写访问，这也是文件存储的典型特性。</p>

<p><img src="assets/FthKM6QA0grpUbN_3Sjxchr8PPIG" alt="avatar"></p>

<p>上图中左侧，可以看到这个声明：它的 size 和它的access mode，跟我们刚才静态创建这块 PV 其实是匹配的。这样的话，当用户在提交 PVC 的时候，K8s 集群相关的组件就会把 PV 的 PVC bound 到一起。之后，用户在提交 pod yaml 的时候，可以在卷里面写上 PVC声明，在 PVC声明里面可以通过 claimName 来声明要用哪个 PVC。这时，挂载方式其实跟前面讲的一样，当提交完 yaml 的时候，它可以通过 PVC 找到 bound 着的那个 PV，然后就可以用那块存储了。这是静态 Provisioning到被pod使用的一个过程。</p>

<h3>动态 PV 使用</h3>

<p>然后再看一下动态 Provisioning。动态 Provisioning 上面提到过，系统管理员不再预分配 PV，而只是创建一个模板文件。</p>

<p><img src="assets/FsMpZ01GlLkyvLGLME5tJEOrfOun" alt="avatar"></p>

<p>这个模板文件叫 StorageClass，在StorageClass里面，我们需要填的重要信息：第一个是 provisioner，provisioner 是什么？它其实就是说我当时创建 PV 和对应的存储的时候，应该用哪个存储插件来去创建。</p>

<p>这些参数是通过k8s创建存储的时候，需要指定的一些细节参数。对于这些参数，用户是不需要关心的，像这里 regionld、zoneld、fsType 和它的类型。ReclaimPolicy跟我们刚才讲解的 PV 里的意思是一样的，就是说动态创建出来的这块 PV,当使用方使用结束、Pod 及 PVC 被删除后，这块 PV 应该怎么处理，我们这个地方写的是 delete，意思就是说当使用方 pod 和 PVC 被删除之后，这个 PV 也会被删除掉。</p>

<p>接下来看一下，集群管理员提交完 StorageClass，也就是提交创建 PV 的模板之后，用户怎么用，首先还是需要写一个 PVC 的文件。</p>

<p><img src="assets/Fimve05og289st9eo03GaBix2048" alt="avatar"></p>

<p>PVC 的文件里存储的大小、访问模式是不变的。现在需要新加一个字段，叫 StorageClassName，它的意思是指定动态创建PV的模板文件的名字，这里StorageClassName填的就是上面声明的csi-disk。</p>

<p>在提交完 PVC之后，K8s 集群中的相关组件就会根据 PVC 以及对应的 StorageClass 动态生成这块 PV 给这个 PVC 做一个绑定，之后用户在提交自己的 yaml 时，用法和接下来的流程和前面的静态使用方式是一样的，通过 PVC 找到我们动态创建的 PV，然后把它挂载到相应的容器中就可以使用了。</p>

<h3>PV Spec 重要字段解析</h3>

<p>接下来，我们讲解一下 PV 的一些重要字段：</p>

<p><img src="assets/Fg7NdhdioxWhbqr8pi51pgMUWqXM" alt="avatar"></p>

<ul>

<li><strong>Capacity</strong>：这个很好理解，就是存储对象的大小；</li>

<li>**AccessModes：**也是用户需要关心的，就是说我使用这个 PV 的方式。它有三种使用方式。</li>

<li>一种是单 node 读写访问；</li>

<li>第二种是多个 node 只读访问，是常见的一种数据的共享方式；</li>

<li>第三种是多个 node 上读写访问。</li>

</ul>

<p>用户在提交 PVC 的时候，最重要的两个字段 —— Capacity 和 AccessModes。在提交 PVC后，k8s集群中的相关组件是如何去找到合适的PV呢？首先它是通过为PV建立的AccessModes索引找到所有能够满足用户的 PVC 里面的 AccessModes 要求的PV list，然后根据PVC的 Capacity，StorageClassName, Label Selector 进一步筛选PV，如果满足条件的PV有多个，选择PV的size最小的，accessmodes列表最短的PV，也即最小适合原则。</p>

<ul>

<li><strong>ReclaimPolicy</strong>：这个就是刚才提到的，我的用户方 PV 的 PVC 在删除之后，我的 PV 应该做如何处理？常见的有三种方式。</li>

<li>第一种方式我们就不说了，现在 K8s 中已经不推荐使用了；</li>

<li>第二种方式 delete，也就是说 PVC 被删除之后，PV 也会被删除；</li>

<li>第三种方式 Retain，就是保留，保留之后，后面这个 PV 需要管理员来手动处理。</li>

<li><strong>StorageClassName</strong>：StorageClassName 这个我们刚才说了，我们动态 Provisioning 时必须指定的一个字段，就是说我们要指定到底用哪一个模板文件来生成 PV ；</li>

<li><strong>NodeAffinity</strong>：就是说我创建出来的 PV，它能被哪些 node 去挂载使用，其实是有限制的。然后通过 NodeAffinity 来声明对node的限制，这样其实对 使用该PV的pod调度也有限制，就是说 pod 必须要调度到这些能访问 PV 的 node 上，才能使用这块 PV，这个字段在我们下一讲讲解存储拓扑调度时在细说。</li>

</ul>

<h3>PV 状态流转</h3>

<p><img src="assets/FgMdJEPXW7ocrXw8wJKtGvK0Fysb" alt="avatar"></p>

<p>接下来我们看一下 PV 的状态流转。首先在创建 PV 对象后，它会处在短暂的pending 状态；等真正的 PV 创建好之后，它就处在 available 状态。</p>

<p>available 状态意思就是可以使用的状态，用户在提交 PVC 之后，被 K8s 相关组件做完 bound（即：找到相应的 PV），这个时候 PV 和 PVC 就结合到一起了，此时两者都处在 bound 状态。当用户在使用完 PVC，将其删除后，这个 PV 就处在 released 状态，之后它应该被删除还是被保留呢？这个就会依赖我们刚才说的 ReclaimPolicy。</p>

<p>这里有一个点需要特别说明一下：当 PV 已经处在 released 状态下，它是没有办法直接回到 available 状态，也就是说接下来无法被一个新的 PVC 去做绑定。如果我们想把已经 released 的 PV 复用，我们这个时候通常应该怎么去做呢？

第一种方式：我们可以新建一个 PV 对象，然后把之前的 released 的 PV 的相关字段的信息填到新的 PV 对象里面，这样的话，这个 PV 就可以结合新的 PVC 了；第二种是我们在删除 pod 之后，不要去删除 PVC 对象，这样给 PV 绑定的 PVC 还是存在的，下次 pod 使用的时候，就可以直接通过 PVC 去复用。K8s中的StatefulSet管理的Pod带存储的迁移就是通过这种方式。</p>

<h2>操作演示</h2>

<p>接下来，我会在实际的环境中给大家演示一下，静态 Provisioning 以及动态 Provisioning 具体操作方式。</p>

<h3>静态 Provisioning 例子</h3>

<p>静态 Provisioning 主要用的是阿里云的 NAS 文件存储；动态 Provisioning 主要用了阿里云的云盘。它们需要相应存储插件，插件我已经提前部署在我的 K8s 集群中了(csi-nasplugin<em>是为了在 k8s 中使用阿里云 NAS 所需的插件，csi-disk</em>是为了在 k8s 中使用阿里云云盘所需要的插件)。</p>

<p><img src="assets/FpJhAEAs04or-prbVkNoqkZUZIEK" alt="avatar"></p>

<p>我们接下来先看一下静态 Provisioning 的 PV 的 yaml 文件。</p>

<p><img src="assets/Fi9o0KtIDLFFmmurEkXDGWHQuIFQ" alt="avatar"></p>

<p>volumeAttributes是我在阿里云nas控制台预先创建的 NAS 文件系统的相关信息，我们主要需要关心的有 capacity 为5Gi; accessModes 为多node读写访问; reclaimPolicy：Retain，也就是当我使用方的 PVC 被删除之后，我这个 PV 是要保留下来的；以及在使用这个卷的过程中使用的driver。</p>

<p>然后我们把对应的 PV 创建出来：</p>

<p><img src="assets/Fo6LxYpVZcpurwOrxWHR2HircjRF" alt="avatar"></p>

<p>我们看一下上图 PV 的状态，已经处在 Available，也就是说它已经可以被使用了。</p>

<p>再创建出来 nas-pvc：</p>

<p><img src="assets/FulXLoM1BylIB2KKiuuX4_tGpmzP" alt="avatar"></p>

<p>我们看这个时候 PVC 已经新创建出来了，而且也已经和我们上面创建的PV绑定到一起了。我们看一下 PVC 的 yaml 里面写的什么。</p>

<p><img src="assets/Fgaj1H2CcWxzLc3mwI4GDjv7O0Ro" alt="avatar"></p>

<p>其实很简单 ，就是我需要的大小以及我需要的 accessModes。提交完之后，它就与我们集群中已经存在的 PV 做匹配，匹配成功之后，它就会做 bound。</p>

<p>接下来我们去创建使用 nas-fs 的 pod：</p>

<p><img src="assets/Fm36cv--ETArYNuLUspEOSI86mO8" alt="avatar"></p>

<p>上图看到，这两个 Pod 都已经处在 running 状态了。</p>

<p>我们先看一下这个 pod yaml：</p>

<p><img src="assets/FvBoeg3pm8fva4Ix8EmSIzxQs2LR" alt="avatar"></p>

<p>pod yaml 里面声明了刚才我们创建出来的PVC对象，然后把它挂载到nas-container容器中的 /data 下面。我们这个 pod 是通过前面课程中讲解 deployment 创建两个副本，通过反亲和性，将两个副本调度在不同的 node 上面。</p>

<p><img src="assets/FgnxVbrCTKqn5dAPz0br2jyZoUqL" alt="avatar"></p>

<p>上图我们可以看一下，两个Pod所在的宿主机是不一样的。</p>

<p>如下图所示：我们登陆到第一个上面，findmnt 看一下它的挂载信息，这个其实就挂载在我声明的 nas-fs 上，那我们再在下面 touch 个 test.test.test 文件，我们也会登陆到另外一个容器看一下，它有没有被共享。</p>

<p><img src="assets/FlCrgKZ-MAmT3oZJg7hlgqgQCQIT" alt="avatar"></p>

<p>我们退出再登陆另外一个 pod（刚才登陆的是第一个，现在登陆第二个）。</p>

<p>如下图所示：我们也 findmnt 一下，可以看到，这两个 pod 的远程挂载路径一样，也就是说我们用的是同一个 NAS PV，我们再看一下刚才创建出来的那个是否存在。</p>

<p><img src="assets/FvFvcwfeRquZImc3d8cGoOACwbj8" alt="avatar"></p>

<p>可以看到，这个也是存在的，就说明这两个运行在不同node上的 pod 共享了同一个 nas 存储。</p>

<p>接下来我们看一下把两个 pod 删掉之后的情况。先删Pod，接着再删一下对应的 PVC(K8s内部对pvc对象由保护机制，在删除pvc对象时如果发现有pod在使用pvc，pvc是删除不掉的)，这个可能要稍等一下。</p>

<p><img src="assets/FvE83zl6fe452Ise-GffNKldJ1Cq" alt="avatar"></p>

<p>看一下下图对应的 PVC 是不是已经被删掉了。</p>

<p><img src="assets/FjAol2eKYuoVn-a4Yd0J42kBi_Zo" alt="avatar"></p>

<p>上图显示，它已经被删掉了。再看一下，刚才的 nas PV 还是在的，它的状态是处在 Released 状态，也就是说刚才使用它的 PVC 已经被删掉了，然后它被 released 了。又因为我们 RECLAIN POLICY 是 Retain，所以它这个 PV 是被保留下来的。</p>

<h3>动态 Provisioning 例子</h3>

<p>接下来我们来看第二个例子，动态 Provisioning 的例子。我们先把保留下来的 PV 手动删掉，可以看到集群中没有 PV了。接下来演示一下动态 Provisioning。</p>

<p>首先，先去创建一个生成 PV 的模板文件，也就是 storageclass。看一下 storageclass 里面的内容，其实很简单。</p>

<p><img src="assets/FqUmuY0twPO9rCISNk1gC2JcVk_V" alt="avatar"></p>

<p>如上图所示，我事先指定的是我要创建存储的卷插件(阿里云云盘插件，由阿里云团队开发)，这个我们已经提前部署好了；我们可以看到，parameters部分是创建存储所需要的一些参数，但是用户不需要关心这些信息；然后是 reclaimPolicy，也就是说通过这个 storageclass 创建出来的 PV 在给绑定到一起的 PVC 删除之后，它是要保留还是要删除。</p>

<p><img src="assets/Fg_mTM6GX9f_4M1xt42iwYlttuRL" alt="avatar"></p>

<p>如上图所示：现在这个集群中是没有 PV 的，我们动态提交一个 PVC 文件，先看一下它的 PVC 文件。它的 accessModes-ReadWriteOnce(因为阿里云云盘其实只能是单node读写的，所以我们声明这样的方式），它的存储大小需求是 30G，它的 storageClassName 是 csi-disk，就是我们刚才创建的 storageclass，也就是说它指定要通过这个模板去生成 PV。</p>

<p><img src="assets/FsqNYtTTGnUyLV2u4hG39vkUcJb9" alt="avatar"></p>

<p>这个 PVC 此时正处在 pending 状态，这就说明它对应的 PV 还在创建过程中。</p>

<p><img src="assets/FlUl2GEfG_o04F150cunvqKb1p4n" alt="avatar"></p>

<p>稍过一会，我们看到已经有一个新的 PV 生成，这个 PV 其实就是根据我们提交的 PVC 以及 PVC 里面指定的storageclass 动态生成的。之后k8s会将生成的 PV 以及我们提交的 PVC，就是这个 disk PVC 做绑定，之后我们就可以通过创建 pod 来使用了。</p>

<p>再看一下 pod yaml：</p>

<p><img src="assets/Fr1xeD1emQDSPXFL9AtuzwE1jxxH" alt="avatar"></p>

<p>pod yaml 很简单，也是通过 PVC 声明，表明使用这个 PVC。然后是挂载点，下面我们可以创建看一下。</p>

<p><img src="assets/FjNOzsDuK9EcSKXYc3zyMfFg4KDm" alt="avatar"></p>

<p>如下图所示：我们可以大概看一下 Events，首先被调度器调度，调度完之后，接下来会有个 attachdetach controller，它会去做 disk的attach操作，就是把我们对应的 PV 挂载到调度器调度的 node 上，然后Pod对应的容器才能启动，启动容器才能使用对应的盘。</p>

<p><img src="assets/FjuLUczGXW9kx-JlbPNRery3OVHj" alt="avatar"></p>

<p>接下来我会把 PVC 删掉，看一下PV 会不会根据我们的 reclaimPolicy 随之删掉呢？我们先看一下，这个时候 PVC 还是存在的，对应的 PV 也是存在的。</p>

<p><img src="assets/Ft2vyWMQNtk0invLWtxoRLM4kDgJ" alt="avatar"></p>

<p>然后删一下 PVC，删完之后再看一下：我们的 PV 也被删了，也就是说根据 reclaimPolicy，我们在删除 PVC 的同时，PV 也会被删除掉。</p>

<p><img src="assets/FrkV_53J6EMx7p7Ol-KOZydGskD_" alt="avatar"></p>

<p>我们的演示部分就到这里了。</p>

<h2>架构设计</h2>

<h3>PV 和 PVC 的处理流程</h3>

<p>我们接下来看一下 K8s 中的 PV 和 PVC 体系的完整处理流程。我首先看一下这张图的右下部分里面提到的 csi。</p>

<p><img src="assets/FpszYRyleOBPZv8gyFWEQwdV1wbu" alt="avatar"></p>

<p>csi 是什么？csi 的全称是 container storage interface，它是K8s社区后面对存储插件实现(out of tree)的官方推荐方式。csi 的实现大体可以分为两部分：</p>

<ul>

<li>第一部分是由k8s社区驱动实现的通用的部分，像我们这张图中的 csi-provisioner和 csi-attacher controller；</li>

<li>另外一种是由云存储厂商实践的，对接云存储厂商的 OpenApi，主要是实现真正的 create/delete/mount/unmount 存储的相关操作，对应到上图中的csi-controller-server和csi-node-server。</li>

</ul>

<p>接下来看一下，当用户提交 yaml 之后，k8s内部的处理流程。用户在提交 PVCyaml 的时候，首先会在集群中生成一个 PVC 对象，然后 PVC 对象会被 csi-provisioner controller watch到，csi-provisioner 会结合 PVC 对象以及 PVC 对象中声明的 storageClass，通过 GRPC 调用 csi-controller-server，然后，到云存储服务这边去创建真正的存储，并最终创建出来 PV 对象。最后，由集群中的 PV controller 将 PVC 和 PV 对象做 bound 之后，这个 PV 就可以被使用了。</p>

<p>用户在提交 pod 之后，首先会被调度器调度选中某一个合适的node，之后该 node 上面的 kubelet 在创建 pod 流程中会通过首先 csi-node-server 将我们之前创建的 PV 挂载到我们 pod 可以使用的路径，然后kubelet开始 create &amp;&amp; start pod中的所有container。</p>

<h3>PV、PVC 以及通过 csi 使用存储流程</h3>

<p>我们接下来通过另一张图来更加详细看一下我们 PV、PVC 以及通过 CSI 使用存储的完整流程。</p>

<p><img src="assets/Fhy2xgbxikVRZjTVWgVHXxcCJ3cC" alt="avatar"></p>

<p>主要分为三个阶段：</p>

<ul>

<li>第一个阶段(Create阶段)是用户提交完 PVC，由 csi-provisioner 创建存储，并生成 PV 对象，之后 PV controller 将 PVC 及生成的 PV 对象做 bound，bound 之后，create 阶段就完成了；</li>

<li>之后用户在提交 pod yaml 的时候，首先会被调度选中某一个 合适的node，等 pod 的运行 node 被选出来之后，会被 AD Controller watch 到 pod 选中的 node，它会去查找 pod 中使用了哪些 PV。然后它会生成一个内部的对象叫 VolumeAttachment 对象，从而去触发 csi-attacher去调用csi-controller-server 去做真正的 attache 操作，attach操作调到云存储厂商OpenAPI。这个 attach 操作就是将存储 attach到 pod 将会运行的 node 上面。第二个阶段 —— attach阶段完成；</li>

<li>然后我们接下来看第三个阶段。第三个阶段 发生在kubelet 创建 pod的过程中，它在创建 pod 的过程中，首先要去做一个 mount，这里的 mount 操作是为了将已经attach到这个 node 上面那块盘，进一步 mount 到 pod 可以使用的一个具体路径，之后 kubelet 才开始创建并启动容器。这就是 PV 加 PVC 创建存储以及使用存储的第三个阶段 —— mount 阶段。</li>

</ul>

<p>总的来说，有三个阶段：第一个 create 阶段，主要是创建存储；第二个 attach 阶段，就是将那块存储挂载到 node 上面(通常为将存储load到node的/dev下面)；第三个 mount 阶段，将对应的存储进一步挂载到 pod 可以使用的路径。这就是我们的 PVC、PV、已经通过CSI实现的卷从创建到使用的完整流程。</p>

<h2>结束语</h2>

<p>我们今天的内容大概就到这里，下一节我将为大家来分享 Volume Snapshot 以及 Volume Topology-aware Scheduling 相关的知识以及具体处理流程，谢谢大家~</p>

<h2>本节总结（需补充）</h2>

<p>本节课的主要内容就到此为止了，这里为大家简单总结一下。</p>

<ul>

<li>K8s Volume是用户Pod保存业务数据的重要接口和手段</li>

<li>PVC和PV体系增强了K8s Volumes在多Pod共享/迁移/存储扩展等场景下的能力</li>

<li>PV（存储）的不同供给模式(static and dynamic)可以通过多种方式为集群中的Pod供给所需的存储</li>

</ul>

</div>

                    </div><div class="book-post">

                        <p id="tip" align="center"></p>

                        <div><h1>10 应用存储和持久化数据卷：存储快照与拓扑调度(至天)</h1>

<p><strong>本文将主要分享以下两方面的内容：</strong></p>

<ol>

<li>存储快照概念、使用与工作原理；</li>

<li>存储拓扑调度背景、概念、使用与工作原理。</li>

</ol>

<h2>基本知识</h2>

<h3>存储快照产生背景</h3>

<p>在使用存储时，为了提高数据操作的容错性，我们通常有需要对线上数据进行snapshot，以及能快速restore的能力。另外，当需要对线上数据进行快速的复制以及迁移等动作，如进行环境的复制、数据开发等功能时，都可以通过存储快照来满足需求，而 K8s 中通过 CSI Snapshotter controller 来实现存储快照的功能。</p>

<h3>存储快照用户接口-Snapshot</h3>

<p>我们知道，K8s 中通过 pvc 以及 pv 的设计体系来简化用户对存储的使用，而存储快照的设计其实是仿照 pvc &amp; pv 体系的设计思想。当用户需要存储快照的功能时，可以通过 VolumeSnapshot 对象来声明，并指定相应的 VolumeSnapshotClass 对象，之后由集群中的相关组件动态生成存储快照以及存储快照对应的对象 VolumeSnapshotContent。如下对比图所示，动态生成 VolumeSnapshotContent 和动态生成 pv 的流程是非常相似的。</p>

<p><img src="assets/FqywR5tXyYX2nUX21QYkccxi4aus" alt="avatar"></p>

<h3>存储快照用户接口-Restore</h3>

<p>有了存储快照之后，如何将快照数据快速恢复过来呢？如下图所示：</p>

<p><img src="assets/FmVxlIXPLmxZCmMUXisC9q57olkr" alt="avatar"></p>

<p>如上所示的流程，可以借助 PVC 对象将其的 dataSource 字段指定为 VolumeSnapshot 对象。这样当 PVC 提交之后，会由集群中的相关组件找到 dataSource 所指向的存储快照数据，然后新创建对应的存储以及 pv 对象，将存储快照数据恢复到新的 pv 中，这样数据就恢复回来了，这就是存储快照的restore用法。</p>

<h3>Topolopy-含义</h3>

<p>首先了解一下拓扑是什么意思：这里所说的拓扑是 K8s 集群中为管理的 nodes 划分的一种“位置”关系，意思为：可以通过在 node 的 labels 信息里面填写某一个 node 属于某一个拓扑。</p>

<p>常见的有三种，这三种在使用时经常会遇到的：</p>

<ul>

<li>第一种，在使用云存储服务的时候，经常会遇到 <strong>region</strong>，也就是地区的概念，在 K8s 中常通过 label failure-domain.beta.kubernetes.io/region 来标识。这个是为了标识单个 K8s 集群管理的跨 region 的 nodes 到底属于哪个地区；</li>

<li>第二种，比较常用的是可用区，也就是 available <strong>zone</strong>，在 K8s 中常通过 label failure-domain.beta.kubernetes.io/zone 来标识。这个是为了标识单个 K8s 集群管理的跨 zone 的 nodes 到底属于哪个可用区；</li>

<li>第三种，是 **hostname，**就是单机维度，是拓扑域为 node 范围，在 K8s 中常通过 label kubernetes.io/hostname 来标识，这个在文章的最后讲 local pv 的时候，会再详细描述。</li>

</ul>

<p>上面讲到的三个拓扑是比较常用的，而拓扑其实是可以自己定义的。可以定义一个字符串来表示一个拓扑域，这个 key 所对应的值其实就是拓扑域下不同的拓扑位置。</p>

<p>举个例子：可以用 <strong>rack</strong> 也就是机房中的机架这个纬度来做一个拓扑域。这样就可以将不同机架 (rack) 上面的机器标记为不同的拓扑位置，也就是说可以将不同机架上机器的位置关系通过 rack 这个纬度来标识。属于 rack1 上的机器，node label 中都添加 rack 的标识，它的 value 就标识成 rack1，即 rack=rack1；另外一组机架上的机器可以标识为 rack=rack2，这样就可以通过机架的纬度就来区分来 K8s 中的 node 所处的位置。</p>

<p>接下来就一起来看看拓扑在 K8s 存储中的使用。</p>

<h3>存储拓扑调度产生背景</h3>

<p>上一节课我们说过，K8s 中通过 PV 的 PVC 体系将存储资源和计算资源分开管理了。如果创建出来的 PV有"访问位置"的限制，也就是说，它通过 nodeAffinity 来指定哪些 node 可以访问这个 PV。为什么会有这个访问位置的限制？</p>

<p>因为在 K8s 中创建 pod 的流程和创建 PV 的流程，其实可以认为是并行进行的，这样的话，就没有办法来保证 pod 最终运行的 node 是能访问到 有位置限制的 PV 对应的存储，最终导致 pod 没法正常运行。这里来举两个经典的例子：</p>

<p>首先来看一下 <strong>Local PV 的例子</strong>，Local PV 是将一个 node 上的本地存储封装为 PV，通过使用 PV 的方式来访问本地存储。为什么会有 Local PV 的需求呢？简单来说，刚开始使用 PV 或 PVC 体系的时候，主要是用来针对分布式存储的，分布式存储依赖于网络，如果某些业务对 I/O 的性能要求非常高，可能通过网络访问分布式存储没办法满足它的性能需求。这个时候需要使用本地存储，刨除了网络的 overhead，性能往往会比较高。但是用本地存储也是有坏处的！分布式存储可以通过多副本来保证高可用，但本地存储就需要业务自己用类似 Raft 协议来实现多副本高可用。</p>

<p>接下来看一下 Local PV 场景可能如果没有对PV做“访问位置”的限制会遇到什么问题？</p>

<p><img src="assets/Fn4TJg9YDp8eV9M9bJ4_lqHjg-Fd" alt="avatar"></p>

<p>当用户在提交完 PVC 的时候，K8s PV controller可能绑定的是 node2 上面的 PV。但是，真正使用这个 PV 的 pod，在被调度的时候，有可能调度在 node1 上，最终导致这个 pod 在起来的时候没办法去使用这块存储，因为 pod 真实情况是要使用 node2 上面的存储。</p>

<p>第二个(如果不对 PV 做“访问位置”的限制会出问题的)场景：</p>

<p><img src="assets/Fg3fhQOmCjbf1WcuPGryeLAlkvi6" alt="avatar"></p>

<p>如果搭建的 K8s 集群管理的 nodes 分布在单个区域多个可用区内。在创建动态存储的时候，创建出来的存储属于可用区 2，但之后在提交使用该存储的 pod，它可能会被调度到可用区 1 了，那就可能没办法使用这块存储。因此像阿里云的云盘，也就是块存储，当前不能跨可用区使用，如果创建的存储其实属于可用区 2，但是 pod 运行在可用区 1，就没办法使用这块存储，这是第二个常见的问题场景。</p>

<p>接下来我们来看看 K8s 中如何通过存储拓扑调度来解决上面的问题的。</p>

<h3>存储拓扑调度</h3>

<p>首先总结一下之前的两个问题，它们都是 PV 在给 PVC 绑定或者动态生成 PV 的时候，我并不知道后面将使用它的 pod 将调度在哪些 node 上。但 PV 本身的使用，是对 pod 所在的 node 有拓扑位置的限制的，如 Local PV 场景是我要调度在指定的 node 上我才能使用那块 PV，而对第二个问题场景就是说跨可用区的话，必须要在将使用该 PV 的 pod 调度到同一个可用区的 node 上才能使用阿里云云盘服务，那 K8s 中怎样去解决这个问题呢？</p>

<p>简单来说，在 K8s 中将 PV 和 PVC 的 binding 操作和动态创建 PV 的操作做了 delay，delay 到 pod 调度结果出来之后，再去做这两个操作。这样的话有什么好处？</p>

<ul>

<li>首先，如果要是所要使用的 PV 是预分配的，如 Local PV，其实使用这块 PV 的 pod 它对应的 PVC 其实还没有做绑定，就可以通过调度器在调度的过程中，结合 pod 的计算资源需求(如 cpu/mem) 以及 pod 的 PVC 需求，选择的 node 既要满足计算资源的需求又要 pod 使用的 pvc 要能 binding 的 pv 的 nodeaffinity 限制;</li>

<li>其次对动态生成 PV 的场景其实就相当于是如果知道 pod 运行的 node 之后，就可以根据 node 上记录的拓扑信息来动态的创建这个 PV，也就是保证新创建出来的 PV 的拓扑位置与运行的 node 所在的拓扑位置是一致的，如上面所述的阿里云云盘的例子，既然知道 pod 要运行到可用区 1，那之后创建存储时指定在可用区 1 创建即可。</li>

</ul>

<p>为了实现上面所说的延迟绑定和延迟创建 PV，需要在 K8s 中的改动涉及到的相关组件有三个：</p>

<ul>

<li>PV Controller 也就是 persistent volume controller，它需要支持延迟 Binding 这个操作。</li>

<li>另一个是动态生成 PV 的组件，如果 pod 调度结果出来之后，它要根据 pod 的拓扑信息来去动态的创建 PV。</li>

<li>第三组件，也是最重要的一个改动点就是 kube-scheduler。在为 pod 选择 node 节点的时候，它不仅要考虑 pod 对 CPU/MEM 的计算资源的需求，它还要考虑这个 pod 对存储的需求，也就是根据它的 PVC，它要先去看一下当前要选择的 node，能否满足能和这个 PVC 能匹配的 PV 的 nodeAffinity；或者是动态生成 PV 的过程，它要根据 StorageClass 中指定的拓扑限制来 check 当前的 node 是不是满足这个拓扑限制，这样就能保证调度器最终选择出来的 node 就能满足存储本身对拓扑的限制。</li>

</ul>

<p>这就是存储拓扑调度的相关知识。</p>

<h2>用例解读</h2>

<p>接下来通过 yaml 用例来解读一下第一部分的基本知识。</p>

<h3>Volume Snapshot/Restore示例</h3>

<p><img src="assets/FhqnT0amePxQJR3N88uvEyz3PmtI" alt="avatar"></p>

<p>下面来看一下存储快照如何使用：首先需要集群管理员，在集群中创建 VolumeSnapshotClass 对象，VolumeSnapshotClass 中一个重要字段就是 Snapshot，它是指定真正创建存储快照所使用的卷插件，这个卷插件是需要提前部署的，稍后再说这个卷插件。</p>

<p>接下来用户他如果要做真正的存储快照，需要声明一个 VolumeSnapshotClass，VolumeSnapshotClass 首先它要指定的是 VolumeSnapshotClassName，接着它要指定的一个非常重要的字段就是 source，这个 source 其实就是指定快照的数据源是啥。这个地方指定 name 为 disk-pvc，也就是说通过这个 pvc 对象来创建存储快照。提交这个 VolumeSnapshot 对象之后，集群中的相关组件它会找到这个 PVC 对应的 PV 存储，对这个 PV 存储做一次快照。</p>

<p>有了存储快照之后，那接下来怎么去用存储快照恢复数据呢？这个其实也很简单，通过声明一个新的 PVC 对象并在它的 spec 下面的 DataSource 中来声明我的数据源来自于哪个 VolumeSnapshot，这里指定的是 disk-snapshot 对象，当我这个 PVC 提交之后，集群中的相关组件，它会动态生成新的 PV 存储，这个新的 PV 存储中的数据就来源于这个 Snapshot 之前做的存储快照。</p>

<h3>Local PV 的示例</h3>

<p>如下图看一下 Local PV 的 yaml 示例：</p>

<p><img src="assets/FrH6aI_eKnOSHARnuGYXmO0ePLqK" alt="avatar"></p>

<p>Local PV 大部分使用的时候都是通过静态创建的方式，也就是要先去声明 PV 对象，既然 Local PV 只能是本地访问，就需要在声明 PV 对象的，在 PV 对象中通过 nodeAffinity 来限制我这个 PV 只能在单 node 上访问，也就是给这个 PV 加上拓扑限制。如上图拓扑的 key 用 kubernetes.io/hostname 来做标记，也就是只能在 node1 访问。如果想用这个 PV，你的 pod 必须要调度到 node1 上。</p>

<p>既然是静态创建 PV 的方式，这里为什么还需要 storageClassname 呢？前面也说了，在 Local PV 中，如果要想让它正常工作，需要用到延迟绑定特性才行，那既然是延迟绑定，当用户在写完 PVC 提交之后，即使集群中有相关的 PV 能跟它匹配，它也暂时不能做匹配，也就是说 PV controller 不能马上去做 binding，这个时候你就要通过一种手段来告诉 PV controller，什么情况下是不能立即做 binding。这里的 storageClass 就是为了起到这个副作用，我们可以看到 storageClass 里面的 provisioner 指定的是 <strong>no-provisioner</strong>，其实就是相当于告诉 K8s 它不会去动态创建 PV，它主要用到 storageclass 的 VolumeBindingMode 字段，叫 WaitForFirstConsumer，可以先简单地认为它是延迟绑定。</p>

<p>当用户开始提交 PVC 的时候，pv controller 在看到这个 pvc 的时候，它会找到相应的 storageClass，发现这个 BindingMode 是延迟绑定，它就不会做任何事情。</p>

<p>之后当真正使用这个 pvc 的 pod，在调度的时候，当它恰好调度在符合 pv nodeaffinity 的 node 的上面后，这个 pod 里面所使用的 PVC 才会真正地与 PV 做绑定，这样就保证我 pod 调度到这台 node 上之后，这个 PVC 才与这个 PV 绑定，最终保证的是创建出来的 pod 能访问这块 Local PV，也就是静态 Provisioning 场景下怎么去满足 PV 的拓扑限制。</p>

<h3>限制 Dynamic Provisioning PV 拓扑示例</h3>

<p>再看一下动态 Provisioning PV 的时候，怎么去做拓扑限制的？</p>

<p><img src="assets/FrCtZOvNrgmr-wgZgSzTm8-Mb8TW" alt="avatar"></p>

<p>动态就是指动态创建 PV 就有拓扑位置的限制，那怎么去指定？</p>

<p>首先在 storageclass 还是需要指定 BindingMode，就是 WaitForFirstConsumer，就是延迟绑定。</p>

<p>其次特别重要的一个字段就是 <strong>allowedTopologies</strong>，限制就在这个地方。上图中可以看到拓扑限制是可用区的级别，这里其实有两层意思：</p>

<ol>

<li>第一层意思就是说我在动态创建 PV 的时候，创建出来的 PV 必须是在这个可用区可以访问的;</li>

<li>第二层含义是因为声明的是延迟绑定，那调度器在发现使用它的 PVC 正好对应的是该 storageclass 的时候，调度 pod 就要选择位于该可用区的 nodes。</li>

</ol>

<p>总之，就是要从两方面保证，一是动态创建出来的存储时要能被这个可用区访问的，二是我调度器在选择 node 的时候，要落在这个可用区内的，这样的话就保证我的存储和我要使用存储的这个 pod 它所对应的 node，它们之间的拓扑域是在同一个拓扑域，用户在写 PVC 文件的时候，写法是跟以前的写法是一样的，主要是在 storageclass 中要做一些拓扑限制。</p>

<h2>操作演示</h2>

<p>本节将在线上环境来演示一下前面讲解的内容。</p>

<p>首先来看一下我的阿里云服务器上搭建的 K8s 服务。总共有 3 个 node 节点。一个 master 节点，两个 node。其中 master 节点是不能调度 pod 的。</p>

<p><img src="assets/FgGZbGzYGCGD8Qs2jLYpOSnz-KeH" alt="avatar"></p>

<p>再看一下，我已经提前把我需要的插件已经布好了，一个是 snapshot 插件 (csi-external-snapshot*)，一个是动态云盘的插件 (csi-disk*)。</p>

<p><img src="assets/FgFqNGllrvIB7DXfry39hsnZrnaa" alt="avatar"></p>

<p>现在开始 snapshot 的演示。首先去动态创建云盘，然后才能做 snapshot。动态创建云盘需要先创建 storageclass，然后去根据 PVC 动态创建 PV，然后再创建一个使用它的 pod 了。</p>

<p><img src="assets/Fj4qQVKzjI_V-XWR6yovtJzl__Op" alt="avatar"></p>

<p>有个以上对象，现在就可以做 snapshot 了，首先看一下做 snapshot 需要的第一个配置文件：snapshotclass.yaml。</p>

<p><img src="assets/Fr3GtiIsnp7AYfikLMCWlymI4OxL" alt="avatar"></p>

<p>其实里面就是指定了在做存储快照的时候需要使用的插件，这个插件刚才演示了已经部署好了，就是 csi-external-snapshot-0 这个插件。</p>

<p><img src="assets/FhDNRvOpTtAVRw2J1VzfgDDafSIu" alt="avatar"></p>

<p>接下来创建 volume-snapshotclass 文件，创建完之后就开始了 snapshot。</p>

<p><img src="assets/FvofbVkLeFCnIIhI3qY3RVy-QGrO" alt="avatar"></p>

<p>然后看 snapshot.yaml，Volumesnapshot 声明创建存储快照了，这个地方就指定刚才创建的那个 PVC 来做的数据源来做 snapshot，那我们开始创建。</p>

<p><img src="assets/FiQaeN1cXcqUCnKmGNgSxMcvzEpf" alt="avatar"></p>

<p>我们看一下 Snapshot 有没有创建好，如下图所示，content 已经在 11 秒之前创建好了。</p>

<p><img src="assets/Fi1PejVX_muKSAYAT2WZ0GzniVVR" alt="avatar"></p>

<p>可以看一下它里面的内容，主要看 volumesnapshotcontent 记录的一些信息，这个是我 snapshot 出来之后，它记录的就是云存储厂商那边返回给我的 snapshot 的 ID。然后是这个 snapshot 数据源，也就是刚才指定的 PVC，可以通过它会找到对应的 PV。</p>

<p><img src="assets/FuHVBHxGXEY0d_U0I4Jy8zxVBj6S" alt="avatar"></p>

<p>snapshot 的演示大概就是这样，把刚才创建的 snapshot 删掉，还是通过 volumesnapshot 来删掉。然后看一下，动态创建的这个 volumesnapshotcontent 也被删掉。</p>

<p><img src="assets/FnYUoOxSSRNC9_Oa-oIsZz6TwSZs" alt="avatar"></p>

<p>接下来看一下动态 PV 创建的过程加上一些拓扑限制，首先将的 storageclass 创建出来，然后再看一下 storageclass 里面做的限制，storageclass 首先还是指定它的 BindingMode 为 WaitForFirstConsumer，也就是做延迟绑定，然后是对它的拓扑限制，我这里面在 allowedTopologies 字段中配置了一个可用区级别的限制。</p>

<p><img src="assets/FuOKKz-eyISv75YCjan-BTlDcg-W" alt="avatar"></p>

<p>来尝试创建一下的 PVC，PVC 创建出来之后，理论上它应该处在 pending 状态。看一下，它现在因为它要做延迟绑定，由于现在没有使用它的 pod，暂时没办法去做绑定，也没办法去动态创建新的 PV。</p>

<p><img src="assets/FgechIlEToFBRi3an4BC-PnGnLo8" alt="avatar"></p>

<p>接下来创建使用该 pvc 的 pod 看会有什么效果，看一下 pod，pod 也处在 pending了。</p>

<p><img src="assets/Fkj5FBesg3wmoqoOFewiegBYGign" alt="avatar"></p>

<p>那来看一下 pod 为啥处在 pending 状态，可以看一下是调度失败了，调度失败原因：一个 node 由于 taint 不能调度，这个其实是 master，另外两个 node 也是没有说是可绑定的 PV。</p>

<p><img src="assets/FovynJqEbrKb7cAZVcG_g5K5U-lh" alt="avatar"></p>

<p>为什么会有两个 node 出现没有可绑定的 pv 的错误？不是动态创建么？</p>

<p>我们来仔细看看 storageclass 中的拓扑限制，通过上面的讲解我们知道，这里限制使用该 storageclass 创建的 PV 存储必须在可用区 cn-hangzhou-d 可访问的，而使用该存储的 pod 也必须调度到 cn-hangzhou-d 的 node 上。</p>

<p><img src="assets/Fpgq7QCpLYN7uEj8lhCExv5wnxnV" alt="avatar"></p>

<p>那就来看一下 node 节点上有没有这个拓扑信息，如果它没有当然是不行了。</p>

<p>看一下第一个 node 的全量信息吧，主要找它的 labels 里面的信息，看 lables 里面的确有一个这样的 key。也就是说有一个这样的拓扑，但是这指定是 cn-hangzhou-b，刚才 storageclass 里面指定的是 cn-hangzhou-d。</p>

<p><img src="assets/FkvFt8-cfCeQIj_jzzDbnY0wBixa" alt="avatar"></p>

<p>那看一下另外的一个 node 上的这个拓扑信息写的也是 hangzhou-b，但是我们那个 storageclass 里面限制是 d。</p>

<p><img src="assets/Fp6PmvEiQ3C0Vwa7dpSBXdxKq_T4" alt="avatar"></p>

<p>这就导致最终没办法将 pod 调度在这两个 node 上。现在我们修改一下 storageclass 中的拓扑限制，将 cn-hangzhou-d 改为 cn-hangzhou-b。</p>

<p><img src="assets/FkHjuc-e2OwkXAIbfUpmrCruhzw9" alt="avatar"></p>

<p>改完之后再看一下，其实就是说我动态创建出来的 PV 要能被 hangzhou-b 这个可用区访问的，使用该存储的 pod 要调度到该可用区的 node 上。把之前的 pod 删掉，让它重新被调度看一下有什么结果，好，现在这个已经调度成功了，就是已经在启动容器阶段。</p>

<p><img src="assets/Fj4GIwbE3-jxXeOO0QPCQIb2rw1q" alt="avatar"></p>

<p>说明刚才把 storageclass 它里面的对可用区的限制从 hangzhou-d 改为 hangzhou-b 之后，集群中就有两个 node，它的拓扑关系是和 storageclass 里要求的拓扑关系是相匹配的，这样的话它就能保证它的 pod 是有 node 节点可调度的。上图中最后一点 Pod 已经 Running 了，说明刚才的拓扑限制改动之后可以 work 了。</p>

<h2>处理流程</h2>

<h3>kubernetes 对 Volume Snapshot/Restore 处理流程</h3>

<p>接下来看一下 K8s 中对存储快照与拓扑调度的具体处理流程。如下图所示：</p>

<p><img src="assets/FrEdRe8Vlz77avZYBODpMQ_8TGtv" alt="avatar"></p>

<p>首先来看一下存储快照的处理流程，这里来首先解释一下 csi 部分。K8s 中对存储的扩展功能都是推荐通过 csi out-of-tree 的方式来实现的。</p>

<p>csi 实现存储扩展主要包含两部分：</p>

<ul>

<li>第一部分是由 K8s 社区推动实现的 csi controller 部分，也就是这里的 csi-snapshottor controller 以及 csi-provisioner controller，这些主要是通用的 controller 部分;</li>

<li>另外一部分是由特定的云存储厂商用自身 OpenAPI 实现的不同的 csi-plugin 部分，也叫存储的 driver 部分。</li>

</ul>

<p>两部分部件通过 unix domain socket 通信连接到一起。有这两部分，才能形成一个真正的存储扩展功能。</p>

<p>如上图所示，当用户提交 VolumeSnapshot 对象之后，会被 csi-snapshottor controller watch 到。之后它会通过 GPPC 调用到 csi-plugin，csi-plugin 通过 OpenAPI 来真正实现存储快照的动作，等存储快照已经生成之后，会返回到 csi-snapshottor controller 中，csi-snapshottor controller 会将存储快照生成的相关信息放到 VolumeSnapshotContent 对象中并将用户提交的 VolumeSnapshot 做 bound。这个 bound 其实就有点类似 PV 和 PVC 的 bound 一样。</p>

<p>有了存储快照，如何去使用存储快照恢复之前的数据呢？前面也说过，通过声明一个新的 PVC 对象，并且指定他的 dataSource 为 Snapshot 对象，当提交 PVC 的时候会被 csi-provisioner watch 到，之后会通过 GRPC 去创建存储。这里创建存储跟之前讲解的 csi-provisioner 有一个不太一样的地方，就是它里面还指定了 Snapshot 的 ID，当去云厂商创建存储时，需要多做一步操作，即将之前的快照数据恢复到新创建的存储中。之后流程返回到 csi-provisioner，它会将新创建的存储的相关信息写到一个新的 PV 对象中，新的 PV 对象被 PV controller watch 到它会将用户提交的 PVC 与 PV 做一个 bound，之后 pod 就可以通过 PVC 来使用 Restore 出来的数据了。这是 K8s 中对存储快照的处理流程。</p>

<h3>kubernetes 对 Volume Topology-aware Scheduling 处理流程</h3>

<p>接下来看一下存储拓扑调度的处理流程：</p>

<p><img src="assets/Fr7qyi0qP1ytDNoVbd3WYZ8-290A" alt="avatar"></p>

<p><strong>第一个步骤</strong>其实就是要去声明延迟绑定，这个通过 StorageClass 来做的，上面已经阐述过，这里就不做详细描述了。</p>

<p>接下来看一下调度器，上图中红色部分就是调度器新加的存储拓扑调度逻辑，我们先来看一下不加红色部分时调度器的为一个 pod 选择 node 时，它的大概流程：</p>

<ul>

<li>首先用户提交完 pod 之后，会被调度器 watch 到，它就会去首先做预选，预选就是说它会将集群中的所有 node 都来与这个 pod 它需要的资源做匹配；</li>

<li>如果匹配上，就相当于这个 node 可以使用，当然可能不止一个 node 可以使用，最终会选出来一批 node；</li>

<li>然后再经过第二个阶段优选，优选就相当于我要对这些 node 做一个打分的过程，通过打分找到最匹配的一个 node；</li>

<li>之后调度器将调度结果写到 pod 里面的 spec.nodeName 字段里面，然后会被相应的 node 上面的 kubelet watch 到，最后就开始创建 pod 的整个流程。</li>

</ul>

<p>那现在看一下加上卷相关的调度的时候，筛选 node(<strong>第二个步骤</strong>)又是怎么做的？</p>

<ul>

<li>先就要找到 pod 中使用的所有 PVC，找到已经 bound 的 PVC，以及需要延迟绑定的这些 PVC；</li>

<li>对于已经 bound 的 PVC，要 check 一下它对应的 PV 里面的 nodeAffinity 与当前 node 的拓扑是否匹配 。如果不匹配， 就说明这个 node 不能被调度。如果匹配，继续往下走，就要去看一下需要延迟绑定的 PVC；</li>

<li>对于需要延迟绑定的 PVC。先去获取集群中存量的 PV，满足 PVC 需求的，先把它全部捞出来，然后再将它们一一与当前的 node labels 上的拓扑做匹配，如果它们(存量的 PV)都不匹配，那就说明当前的存量的 PV 不能满足需求，就要进一步去看一下如果要动态创建 PV 当前 node 是否满足拓扑限制，也就是还要进一步去 check StorageClass 中的拓扑限制，如果 StorageClass 中声明的拓扑限制与当前的 node 上面已经有的 labels 里面的拓扑是相匹配的，那其实这个 node 就可以使用，如果不匹配，说明该 node 就不能被调度。</li>

</ul>

<p>经过这上面步骤之后，就找到了所有即满足 pod 计算资源需求又满足 pod 存储资源需求的所有 nodes。</p>

<p>当 node 选出来之后，<strong>第三个步骤</strong>就是调度器内部做的一个优化。这里简单过一下，就是更新经过预选和优选之后，pod 的 node 信息，以及 PV 和 PVC 在 scheduler 中做的一些 cache 信息。</p>

<p><strong>第四个步骤</strong>也是重要的一步，已经选择出来 node 的 Pod，不管其使用的 PVC 是要 binding 已经存在的 PV，还是要做动态创建 PV，这时就可以开始做。由调度器来触发，调度器它就会去更新 PVC 对象和 PV 对象里面的相关信息，然后去触发 PV controller 去做 binding 操作，或者是由 csi-provisioner 去做动态创建流程。</p>

<h2>总结</h2>

<ol>

<li>通过对比 PVC&amp;PV 体系讲解了存储快照的相关 K8s 资源对象以及使用方法；</li>

<li>通过两个实际场景遇到的问题引出存储拓扑调度功能必要性，以及 K8s 中如何通过拓扑调度来解决这些问题；</li>

<li>通过剖析 K8s 中存储快照和存储拓扑调度内部运行机制，深入理解该部分功能的工作原理。</li>

</ol>

</div>

                    </div><div class="book-post">

                        <p id="tip" align="center"></p>

                        <div><h1>11 可观测性：你的应用健康吗？（莫源）</h1>

<p>本次课程的分享主要围绕以下五个部分：</p>

<ul>

<li>介绍一些整体需求的来源；</li>

<li>介绍在 K8s 中 Liveness 和 Readiness 的使用方式；</li>

<li>介绍在 K8s 中常见问题的诊断；</li>

<li>应用的远程调试的方式；</li>

<li>课程的总结与实践；</li>

</ul>

<h2>需求来源</h2>

<p>首先来看一下，整个需求的来源：当把应用迁移到 Kubernetes 之后，要如何去保障应用的健康与稳定呢？其实很简单，可以从两个方面来进行增强：</p>

<ol>

<li>首先是提高应用的可观测性；</li>

<li>第二是提高应用的可恢复能力。</li>

</ol>

<p>从可观测性上来讲，可以在三个方面来去做增强：</p>

<ol>

<li>首先是应用的健康状态上面，可以实时地进行观测；</li>

<li>第二个是可以获取应用的资源使用情况；</li>

<li>第三个是可以拿到应用的实时日志，进行问题的诊断与分析。</li>

</ol>

<p>当出现了问题之后，首先要做的事情是要降低影响的范围，进行问题的调试与诊断。最后当出现问题的时候，理想的状况是：可以通过和 K8s 集成的自愈机制进行完整的恢复。</p>

<h2>Liveness 与 Readiness</h2>

<p>本小节为大家介绍 Liveness probe 和 eadiness probe。</p>

<h3>应用健康状态-初识 Liveness 与 Readiness</h3>

<p>Liveness probe 也叫就绪指针，用来判断一个 pod 是否处在就绪状态。当一个 pod 处在就绪状态的时候，它才能够对外提供相应的服务，也就是说接入层的流量才能打到相应的 pod。当这个 pod 不处在就绪状态的时候，接入层会把相应的流量从这个 pod 上面进行摘除。</p>

<p>来看一下简单的一个例子：</p>

<p>如下图其实就是一个 Readiness 就绪的一个例子：</p>

<p><img src="assets/Fnlyp0R6GWpW_Hjt5qT-BLAWHgRc" alt="avatar"></p>

<p>当这个 pod 指针判断一直处在失败状态的时候，其实接入层的流量不会打到现在这个 pod 上。</p>

<p><img src="assets/FmD6io0Bvk_zcZRtG9PweRYbDURX" alt="avatar"></p>

<p>当这个 pod 的状态从 FAIL 的状态转换成 success 的状态时，它才能够真实地承载这个流量。

Liveness 指针也是类似的，它是存活指针，用来判断一个 pod 是否处在存活状态。当一个 pod 处在不存活状态的时候，会出现什么事情呢？</p>

<p><img src="assets/Ft-XeNH2zT9hloHcrkQw9oCrMzD5" alt="avatar"></p>

<p>这个时候会由上层的判断机制来判断这个 pod 是否需要被重新拉起。那如果上层配置的重启策略是 restart always 的话，那么此时这个 pod 会直接被重新拉起。</p>

<h3>应用健康状态-使用方式</h3>

<p>接下来看一下 Liveness 指针和 Readiness 指针的具体的用法。</p>

<h4>探测方式</h4>

<p>Liveness 指针和 Readiness 指针支持三种不同的探测方式：</p>

<ol>

<li>第一种是 httpGet。它是通过发送 http Get 请求来进行判断的，当返回码是 200-399 之间的状态码时，标识这个应用是健康的；</li>

<li>第二种探测方式是 Exec。它是通过执行容器中的一个命令来判断当前的服务是否是正常的，当命令行的返回结果是 0，则标识容器是健康的；</li>

<li>第三种探测方式是 tcpSocket。它是通过探测容器的 IP 和 Port 进行 TCP 健康检查，如果这个 TCP 的链接能够正常被建立，那么标识当前这个容器是健康的。</li>

</ol>

<h4>探测结果</h4>

<p>从探测结果来讲主要分为三种：</p>

<ul>

<li>第一种是 success，当状态是 success 的时候，表示 container 通过了健康检查，也就是 Liveness probe 或 Readiness probe 是正常的一个状态；</li>

<li>第二种是 Failure，Failure 表示的是这个 container 没有通过健康检查，如果没有通过健康检查的话，那么此时就会进行相应的一个处理，那在 Readiness 处理的一个方式就是通过 service。service 层将没有通过 Readiness 的 pod 进行摘除，而 Liveness 就是将这个 pod 进行重新拉起，或者是删除。</li>

<li>第三种状态是 Unknown，Unknown 是表示说当前的执行的机制没有进行完整的一个执行，可能是因为类似像超时或者像一些脚本没有及时返回，那么此时 Readiness-probe 或 Liveness-probe 会不做任何的一个操作，会等待下一次的机制来进行检验。</li>

</ul>

<p>那在 kubelet 里面有一个叫 ProbeManager 的组件，这个组件里面会包含 Liveness-probe 或 Readiness-probe，这两个 probe 会将相应的 Liveness 诊断和 Readiness 诊断作用在 pod 之上，来实现一个具体的判断。</p>

<h3>应用健康状态-Pod Probe Spec</h3>

<p>下面介绍这三种方式不同的检测方式的一个 yaml 文件的使用。</p>

<p>首先先看一下 exec，exec 的使用其实非常简单。如下图所示，大家可以看到这是一个 Liveness probe，它里面配置了一个 exec 的一个诊断。接下来，它又配置了一个 command 的字段，这个 command 字段里面通过 cat 一个具体的文件来判断当前 Liveness probe 的状态，当这个文件里面返回的结果是 0 时，或者说这个命令返回是 0 时，它会认为此时这个 pod 是处在健康的一个状态。</p>

<p><img src="assets/Fhk2djtw2BsQ-eFML2EoGe0Zhlz2" alt="avatar"></p>

<p>那再来看一下这个 httpGet，httpGet 里面有一个字段是路径，第二个字段是 port，第三个是 headers。这个地方有时需要通过类似像 header 头的一个机制做 health 的一个判断时，需要配置这个 header，通常情况下，可能只需要通过 health 和 port 的方式就可以了。</p>

<p><img src="assets/FvEFNoa9tI3JgXGgY1G-TV8Tkhix" alt="avatar"></p>

<p>第三种是 tcpSocket，tcpSocket 的使用方式其实也比较简单，你只需要设置一个检测的端口，像这个例子里面使用的是 8080 端口，当这个 8080 端口 tcp connect 审核正常被建立的时候，那 tecSocket，Probe 会认为是健康的一个状态。</p>

<p><img src="assets/Fn8qTINgCXS_nmxwB8hI4-qtOyOd" alt="avatar"></p>

<p>此外还有如下的五个参数，是 Global 的参数。</p>

<ul>

<li>第一个参数叫 initialDelaySeconds，它表示的是说这个 pod 启动延迟多久进行一次检查，比如说现在有一个 Java 的应用，它启动的时间可能会比较长，因为涉及到 jvm 的启动，包括 Java 自身 jar 的加载。所以前期，可能有一段时间是没有办法被检测的，而这个时间又是可预期的，那这时可能要设置一下 initialDelaySeconds；</li>

<li>第二个是 periodSeconds，它表示的是检测的时间间隔，正常默认的这个值是 10 秒；</li>

<li>第三个字段是 timeoutSeconds，它表示的是检测的超时时间，当超时时间之内没有检测成功，那它会认为是失败的一个状态；</li>

<li>第四个是 successThreshold，它表示的是：当这个 pod 从探测失败到再一次判断探测成功，所需要的阈值次数，默认情况下是 1 次，表示原本是失败的，那接下来探测这一次成功了，就会认为这个 pod 是处在一个探针状态正常的一个状态；</li>

<li>最后一个参数是 failureThreshold，它表示的是探测失败的重试次数，默认值是 3，表示的是当从一个健康的状态连续探测 3 次失败，那此时会判断当前这个pod的状态处在一个失败的状态。</li>

</ul>

<h3>应用健康状态-Liveness 与 Readiness 总结</h3>

<p>接下来对 Liveness 指针和 Readiness 指针进行一个简单的总结。</p>

<p><img src="assets/FjWTPaRjEPG-yw_ixjNZaKiuzOkx" alt="avatar"></p>

<h4>介绍</h4>

<p>Liveness 指针是存活指针，它用来判断容器是否存活、判断 pod 是否 running。如果 Liveness 指针判断容器不健康，此时会通过 kubelet 杀掉相应的 pod，并根据重启策略来判断是否重启这个容器。如果默认不配置 Liveness 指针，则默认情况下认为它这个探测默认返回是成功的。</p>

<p>Readiness 指针用来判断这个容器是否启动完成，即 pod 的 condition 是否 ready。如果探测的一个结果是不成功，那么此时它会从 pod 上 Endpoint 上移除，也就是说从接入层上面会把前一个 pod 进行摘除，直到下一次判断成功，这个 pod 才会再次挂到相应的 endpoint 之上。</p>

<h4>检测失败</h4>

<p>对于检测失败上面来讲 Liveness 指针是直接杀掉这个 pod，而 Readiness 指针是切掉 endpoint 到这个 pod 之间的关联关系，也就是说它把这个流量从这个 pod 上面进行切掉。</p>

<h4>适用场景</h4>

<p>Liveness 指针适用场景是支持那些可以重新拉起的应用，而 Readiness 指针主要应对的是启动之后无法立即对外提供服务的这些应用。</p>

<h4>注意事项</h4>

<p>在使用 Liveness 指针和 Readiness 指针的时候有一些注意事项。因为不论是 Liveness 指针还是 Readiness 指针都需要配置合适的探测方式，以免被误操作。</p>

<ul>

<li>第一个是调大超时的阈值，因为在容器里面执行一个 shell 脚本，它的执行时长是非常长的，平时在一台 ecs 或者在一台 vm 上执行，可能 3 秒钟返回的一个脚本在容器里面需要 30 秒钟。所以这个时间是需要在容器里面事先进行一个判断的，那如果可以调大超时阈值的方式，来防止由于容器压力比较大的时候出现偶发的超时；</li>

<li>第二个是调整判断的一个次数，3 次的默认值其实在比较短周期的判断周期之下，不一定是最佳实践，适当调整一下判断的次数也是一个比较好的方式；</li>

<li>第三个是 exec，如果是使用 shell 脚本的这个判断，调用时间会比较长，比较建议大家可以使用类似像一些编译性的脚本 Golang 或者一些 C 语言、C++ 编译出来的这个二进制的 binary 进行判断，那这种通常会比 shell 脚本的执行效率高 30% 到 50%；</li>

<li>第四个是如果使用 tcpSocket 方式进行判断的时候，如果遇到了 TLS 的服务，那可能会造成后边 TLS 里面有很多这种未健全的 tcp connection，那这个时候需要自己对业务场景上来判断，这种的链接是否会对业务造成影响。</li>

</ul>

<h2>问题诊断</h2>

<p>接下来给大家讲解一下在 K8s 中常见的问题诊断。</p>

<h3>应用故障排查-了解状态机制</h3>

<p>首先要了解一下 K8s 中的一个设计理念，就是这个状态机制。因为 K8s 是整个的一个设计是面向状态机的，它里面通过 yaml 的方式来定义的是一个期望到达的一个状态，而真正这个 yaml 在执行过程中会由各种各样的 controller来负责整体的状态之间的一个转换。</p>

<p><img src="assets/Fv1SNFibPhnu1hzgc4HvJ-XejxEj" alt="avatar"></p>

<p><img src="assets/FmWSz1Y4Bfj7zyDJhc-Rplrz6Yuu" alt="avatar"></p>

<p>比如说上面的图，实际上是一个 Pod 的一个生命周期。刚开始它处在一个 pending 的状态，那接下来可能会转换到类似像 running，也可能转换到 Unknown，甚至可以转换到 failed。然后，当 running 执行了一段时间之后，它可以转换到类似像 successded 或者是 failed，然后当出现在 unknown 这个状态时，可能由于一些状态的恢复，它会重新恢复到 running 或者 successded 或者是 failed。</p>

<p>其实 K8s 整体的一个状态就是基于这种类似像状态机的一个机制进行转换的，而不同状态之间的转化都会在相应的 K8s对象上面留下来类似像 Status 或者像 Conditions 的一些字段来进行表示。</p>

<p>像下面这张图其实表示的就是说在一个 Pod 上面一些状态位的一些展现。</p>

<p><img src="assets/FnaNXmJuY0OIVStCW2qDnWVOiAO2" alt="avatar"></p>

<p>比如说在 Pod 上面有一个字段叫 Status，这个 Status 表示的是 Pod 的一个聚合状态，在这个里面，这个聚合状态处在一个 pending 状态。</p>

<p>然后再往下看，因为一个 pod 里面有多个 container，每个 container 上面又会有一个字段叫 State，然后 State 的状态表示当前这个 container 的一个聚合状态。那在这个例子里面，这个聚合状态处在的是 waiting 的状态，那具体的原因是因为什么呢？是因为它的镜像没有拉下来，所以处在 waiting 的状态，是在等待这个镜像拉取。然后这个 ready 的部分呢，目前是 false，因为它这个进行目前没有拉取下来，所以这个 pod 不能够正常对外服务，所以此时 ready 的状态是未知的，定义为 false。如果上层的 endpoint 发现底层这个 ready 不是 true 的话，那么此时这个服务是没有办法对外服务的。</p>

<p>再往下是 condition，condition 这个机制表示是说：在 K8s 里面有很多这种比较小的这个状态，而这个状态之间的聚合会变成上层的这个 Status。那在这个例子里面有几个状态，第一个是 Initialized，表示是不是已经初始化完成？那在这个例子里面已经是初始化完成的，那它走的是第二个阶段，是在这个 ready 的状态。因为上面几个 container 没有拉取下来相应的镜像，所以 ready 的状态是 false。</p>

<p>然后再往下可以看到这个 container 是否 ready，这里可以看到是 false，而这个状态是 PodScheduled，表示说当前这个 pod 是否是处在一个已经被调度的状态，它已经 bound 在现在这个 node 之上了，所以这个状态也是 true。</p>

<p>那可以通过相应的 condition 是 true 还是 false 来判断整体上方的这个状态是否是正常的一个状态。而在 K8s 里面不同的状态之间的这个转换都会发生相应的事件，而事件分为两种： 一种叫做 normal 的事件，一种是 warning 事件。大家可以看见在这第一条的事件是有个 normal 事件，然后它相应的 reason 是 scheduler，表示说这个 pod 已经被默认的调度器调度到相应的一个节点之上，然后这个节点是 cn-beijing192.168.3.167 这个节点之上。</p>

<p>再接下来，又是一个 normal 的事件，表示说当前的这个镜像在 pull 相应的这个 image。然后再往下是一个 warning 事件，这个 warning 事件表示说 pull 这个镜像失败了。</p>

<p>以此类推，这个地方表示的一个状态就是说在 K8s 里面这个状态机制之间这个状态转换会产生相应的事件，而这个事件又通过类似像 normal 或者是 warning 的方式进行暴露。开发者可以通过类似像通过这个事件的机制，可以通过上层 condition Status 相应的一系列的这个字段来判断当前这个应用的具体的状态以及进行一系列的诊断。</p>

<h3>应用故障排查-常见应用异常</h3>

<p>本小节介绍一下常见应用的一些异常。首先是 pod 上面，pod 上面可能会停留几个常见的状态。</p>

<h4>Pod 停留在 Pending</h4>

<p>第一个就是 pending 状态，pending 表示调度器没有进行介入。此时可以通过 kubectl describe pod 来查看相应的事件，如果由于资源或者说端口占用，或者是由于 node selector 造成 pod 无法调度的时候，可以在相应的事件里面看到相应的结果，这个结果里面会表示说有多少个不满足的 node，有多少是因为 CPU 不满足，有多少是由于 node 不满足，有多少是由于 tag 打标造成的不满足。</p>

<h4>Pod 停留在 waiting</h4>

<p>那第二个状态就是 pod 可能会停留在 waiting 的状态，pod 的 states 处在 waiting 的时候，通常表示说这个 pod 的镜像没有正常拉取，原因可能是由于这个镜像是私有镜像，但是没有配置 Pod secret；那第二种是说可能由于这个镜像地址是不存在的，造成这个镜像拉取不下来；还有一个是说这个镜像可能是一个公网的镜像，造成镜像的拉取失败。</p>

<h4>Pod 不断被拉取并且可以看到 crashing</h4>

<p>第三种是 pod 不断被拉起，而且可以看到类似像 backoff。这个通常表示说 pod 已经被调度完成了，但是启动失败，那这个时候通常要关注的应该是这个应用自身的一个状态，并不是说配置是否正确、权限是否正确，此时需要查看的应该是 pod 的具体日志。</p>

<h4>Pod 处在 Runing 但是没有正常工作</h4>

<p>第四种 pod 处在 running 状态，但是没有正常对外服务。那此时比较常见的一个点就可能是由于一些非常细碎的配置，类似像有一些字段可能拼写错误，造成了 yaml 下发下去了，但是有一段没有正常地生效，从而使得这个 pod 处在 running 的状态没有对外服务，那此时可以通过 apply-validate-f pod.yaml 的方式来进行判断当前 yaml 是否是正常的，如果 yaml 没有问题，那么接下来可能要诊断配置的端口是否是正常的，以及 Liveness 或 Readiness 是否已经配置正确。</p>

<h4>Service 无法正常的工作</h4>

<p>最后一种就是 service 无法正常工作的时候，该怎么去判断呢？那比较常见的 service 出现问题的时候，是自己的使用上面出现了问题。因为 service 和底层的 pod 之间的关联关系是通过 selector 的方式来匹配的，也就是说 pod 上面配置了一些 label，然后 service 通过 match label 的方式和这个 pod 进行相互关联。如果这个 label 配置的有问题，可能会造成这个 service 无法找到后面的 endpoint，从而造成相应的 service 没有办法对外提供服务，那如果 service 出现异常的时候，第一个要看的是这个 service 后面是不是有一个真正的 endpoint，其次来看这个 endpoint 是否可以对外提供正常的服务。</p>

<h2>应用远程调试</h2>

<p>本节讲解的是在 K8s 里面如何进行应用的远程调试，远程调试主要分为 pod 的远程调试以及 service 的远程调试。还有就是针对一些性能优化的远程调试。</p>

<h3>应用远程调试 - Pod 远程调试</h3>

<p>首先把一个应用部署到集群里面的时候，发现问题的时候，需要进行快速验证，或者说修改的时候，可能需要类似像登陆进这个容器来进行一些诊断。</p>

<p><img src="assets/FuR1AQmwzQmyNvmqB4AOqCasWXe0" alt="avatar"></p>

<p>比如说可以通过 exec 的方式进入一个 pod。像这条命令里面，通过 kubectl exec-it pod-name 后面再填写一个相应的命令，比如说 /bin/bash，表示希望到这个 pod 里面进入一个交互式的一个 bash。然后在 bash 里面可以做一些相应的命令，比如说修改一些配置，通过 supervisor 去重新拉起这个应用，都是可以的。</p>

<p>那如果指定这一个 pod 里面可能包含着多个 container，这个时候该怎么办呢？怎么通过 pod 来指定 container 呢？其实这个时候有一个参数叫做 -c，如上图下方的命令所示。-c 后面是一个 container-name，可以通过 pod 在指定 -c 到这个 container-name，具体指定要进入哪个 container，后面再跟上相应的具体的命令，通过这种方式来实现一个多容器的命令的一个进入，从而实现多容器的一个远程调试。</p>

<h3>应用远程调试 - Servic 远程调试</h3>

<p>那么 service 的远程调试该怎么做呢？service 的远程调试其实分为两个部分：</p>

<ul>

<li>第一个部分是说我想将一个服务暴露到远程的一个集群之内，让远程集群内的一些应用来去调用本地的一个服务，这是一条反向的一个链路；</li>

<li>还有一种方式是我想让这个本地服务能够去调远程的服务，那么这是一条正向的链路。</li>

</ul>

<p>在反向列入上面有这样一个开源组件，叫做 Telepresence，它可以将本地的应用代理到远程集群中的一个 service 上面，使用它的方式非常简单。</p>

<p><img src="assets/FnBOpWKlpdW6_HuQKG8EdmZAPXFG" alt="avatar"></p>

<p>首先先将 Telepresence 的一个 Proxy 应用部署到远程的 K8s 集群里面。然后将远程单一个 deployment swap 到本地的一个 application，使用的命令就是 Telepresence-swap-deployment 然后以及远程的 DEPLOYMENT_NAME。通过这种方式就可以将本地一个 application 代理到远程的 service 之上、可以将应用在远程集群里面进行本地调试，这个有兴趣的同学可以到 GitHub 上面来看一下这个插件的使用的方式。</p>

<p>第二个是如果本地应用需要调用远程集群的服务时候，可以通过 port-forward 的方式将远程的应用调用到本地的端口之上。比如说现在远程的里面有一个 API server，这个 API server 提供了一些端口，本地在调试 Code 时候，想要直接调用这个 API server，那么这时，比较简单的一个方式就是通过 port-forward 的方式。</p>

<p><img src="assets/FkAn97fsuVQaNKenhCg9DR2AXDNr" alt="avatar"></p>

<p>它的使用方式是 kubectl port-forward，然后 service 加上远程的 service name，再加上相应的 namespace，后面还可以加上一些额外的参数，比如说端口的一个映射，通过这种机制就可以把远程的一个应用代理到本地的端口之上，此时通过访问本地端口就可以访问远程的服务。</p>

<h3>开源的调试工具 - kubectl-debug</h3>

<p>最后再给大家介绍一个开源的调试工具，它也是 kubectl 的一个插件，叫 kubectl-debug。我们知道在 K8s 里面，底层的容器 runtime 比较常见的就是类似像 docker 或者是 containerd，不论是 docker 还是 containerd，它们使用的一个机制都是基于 Linux namespace 的一个方式进行虚拟化和隔离的。</p>

<p>通常情况下 ，并不会在镜像里面带特别多的调试工具，类似像 netstat telnet 等等这些 ，因为这个会造成应用整体非常冗余。那么如果想要调试的时候该怎么做呢？其实这个时候就可以依赖类似于像 kubectl-debug 这样一个工具。</p>

<p>kubectl-debug 这个工具是依赖于 Linux namespace 的方式来去做的，它可以 datash 一个 Linux namespace 到一个额外的 container，然后在这个 container 里面执行任何的 debug 动作，其实和直接去 debug 这个 Linux namespace 是一致的。这里有一个简单的操作，给大家来介绍一下：</p>

<p>这个地方其实已经安装好了 kubectl-debug，它是 kubectl 的一个插件。所以这个时候，你可以直接通过 kubectl-debug 这条命令来去诊断远程的一个 pod。像这个例子里面，当执行 debug 的时候，实际上它首先会先拉取一些镜像，这个镜像里面实际上会默认带一些诊断的工具。当这个镜像启用的时候，它会把这个 debug container 进行启动。与此同时会把这个 container 和相应的你要诊断的这个 container 的 namespace 进行挂靠，也就说此时这个 container 和你是同 namespace 的，类似像网络站，或者是类似像内核的一些参数，其实都可以在这个 debug container 里面实时地进行查看。</p>

<p><img src="assets/FoWjhV4sD3r03T-biKRtroSrJeRN" alt="avatar"></p>

<p>像这个例子里面，去查看类似像 hostname、进程、netstat 等等，这些其实都是和这个需要 debug 的 pod 是在同一个环境里面的，所以你之前这三条命令可以看到里面相关的信息。</p>

<p><img src="assets/FowhZxpEseUMpEWfjTWWOX5QjCjV" alt="avatar"></p>

<p>如果此时进行 logout 的话，相当于会把相应的这个 debug pod 杀掉，然后进行退出，此时对应用实际上是没有任何的影响的。那么通过这种方式可以不介入到容器里面，就可以实现相应的一个诊断。</p>

<p><img src="assets/FgNL87ISOH_7znKE_1kznU0zASHv" alt="avatar"></p>

<p>此外它还支持额外的一些机制，比如说我给设定一些 image，然后类似像这里面安装了的是 htop，然后开发者可以通过这个机制来定义自己需要的这个命令行的工具，并且通过这种 image 的方式设置进来。那么这个时候就可以通过这种机制来调试远程的一个 pod。</p>

<h2>本节总结</h2>

<ul>

<li>关于 Liveness 和 Readiness 的指针。Liveness probe 就是保活指针，它是用来看 pod 是否存活的，而 Readiness probe 是就绪指针，它是判断这个 pod 是否就绪的，如果就绪了，就可以对外提供服务。这个就是 Liveness 和 Readiness 需要记住的部分；</li>

<li>应用诊断的三个步骤：首先 describe 相应的一个状态；然后提供状态来排查具体的一个诊断方向；最后来查看相应对象的一个 event 获取更详细的一个信息；</li>

<li>提供 pod 一个日志来定位应用的自身的一个状态；</li>

<li>远程调试的一个策略，如果想把本地的应用代理到远程集群，此时可以通过 Telepresence 这样的工具来实现，如果想把远程的应用代理到本地，然后在本地进行调用或者是调试，可以用类似像 port-forward 这种机制来实现。</li>

</ul>

</div>

                    </div><div class="book-post">

                        <p id="tip" align="center"></p>

                        <div><h1>12 可观测性-监控与日志（莫源）</h1>

<p><strong>本文主要分为四个部分：</strong></p>

<ol>

<li>在 K8s 中监控和日志的背景信息；</li>

<li>在 K8s 中监控方案的演进，以及常见的监控方案的提供；</li>

<li>日志采集的一些细节以及常见的日志的开源系统；</li>

<li>课程总结，介绍一下阿里云容器服务上面监控和日志的最佳实践。</li>

</ol>

<h2>背景</h2>

<p>监控和日志是大型分布式系统的重要<strong>基础设施</strong>，监控可以帮助开发者查看系统的运行状态，而日志可以协助问题的排查和诊断。</p>

<p>在 Kubernetes 中，监控和日志属于生态的一部分，它并不是核心组件，因此大部分的能力依赖上层的云厂商的适配。Kubernetes 定义了介入的接口标准和规范，任何符合接口标准的组件都可以快速集成。</p>

<h2>监控</h2>

<h3>监控类型</h3>

<p>先看一下监控，从监控类型上划分，在 K8s 中可以分成四个不同的类型：</p>

<ul>

<li>资源监控</li>

</ul>

<p>比较常见的像 CPU、内存、网络这种资源类的一个指标，通常这些指标会以数值、百分比的单位进行统计，是最常见的一个监控方式。这种监控方式在常规的监控里面，类似项目 zabbix telegraph，这些系统都是可以做到的。</p>

<ul>

<li>性能监控</li>

</ul>

<p>性能监控指的就是 APM 监控，也就是说常见的一些应用性能类的监控指标的检查。通常是通过一些 Hook 的机制在虚拟机层、字节码执行层通过隐式调用，或者是在应用层显示注入，获取更深层次的一个监控指标，一般是用来应用的调优和诊断的。比较常见的类似像 jvm 或者 php 的 Zend Engine，通过一些常见的 Hook 机制，拿到类似像 jvm 里面的 GC 的次数，各种内存代的一个分布以及网络连接数的一些指标，通过这种方式来进行应用的性能诊断和调优。</p>

<ul>

<li>安全监控</li>

</ul>

<p>安全监控主要是对安全进行的一系列的监控策略，类似像越权管理、安全漏洞扫描等等。</p>

<ul>

<li>事件监控</li>

</ul>

<p>事件监控是 K8s 中比较另类的一种监控方式。在上一节课中给大家介绍了在 K8s 中的一个设计理念，就是基于状态机的一个状态转换。从正常的状态转换成另一个正常的状态的时候，会发生一个 normal 的事件，而从一个正常状态转换成一个异常状态的时候，会发生一个 warning 的事件。通常情况下，warning 的事件是我们比较关心的，而事件监控就是可以把 normal 的事件或者是 warning 事件离线到一个数据中心，然后通过数据中心的分析以及报警，把相应的一些异常通过像钉钉或者是短信、邮件的方式进行暴露，弥补常规监控的一些缺陷和弊端。</p>

<h3>Kubernetes 的监控演进</h3>

<p>在早期，也就是 1.10 以前的 K8s 版本。大家都会使用类似像 Heapster 这样的组件来去进行监控的采集，Heapster 的设计原理其实也比较简单。</p>

<p><img src="assets/Ft0GfWowV_jtgx4AAg98wg8qKv0M" alt="avatar"></p>

<p>首先，我们在每一个 Kubernetes 上面有一个包裹好的 cadvisor，这个 cadvisor 是负责数据采集的组件。当 cadvisor 把数据采集完成，Kubernetes 会把 cadvisor 采集到的数据进行包裹，暴露成相应的 API。在早期的时候，实际上是有三种不同的 API：</p>

<ul>

<li>第一种是 summary 接口；</li>

<li>第二种是 kubelet 接口；</li>

<li>第三种是 Prometheus 接口。</li>

</ul>

<p>这三种接口，其实对应的数据源都是 cadvisor，只是数据格式有所不同。而在 Heapster 里面，其实支持了 summary 接口和 kubelet 两种数据采集接口，Heapster 会定期去每一个节点拉取数据，在自己的内存里面进行聚合，然后再暴露相应的 service，供上层的消费者进行使用。在 K8s 中比较常见的消费者，类似像 dashboard，或者是 HPA-Controller，它通过调用 service 获取相应的监控数据，来实现相应的弹性伸缩，以及监控数据的一个展示。</p>

<p>这个是以前的一个数据消费链路，这条消费链路看上去很清晰，也没有太多的一个问题，那为什么 Kubernetes 会将 Heapster 放弃掉而转换到 metrics-service 呢？其实这个主要的一个动力来源是由于 Heapster 在做监控数据接口的标准化。为什么要做监控数据接口标准化呢？</p>

<ul>

<li>第一点在于客户的需求是千变万化的，比如说今天用 Heapster 进行了基础数据的一个资源采集，那明天的时候，我想在应用里面暴露在线人数的一个数据接口，放到自己的接口系统里进行数据的一个展现，以及类似像 HPA 的一个数据消费。那这个场景在 Heapster 下能不能做呢？答案是不可以的，所以这就是 Heapster 自身扩展性的弊端；</li>

<li>第二点是 Heapster 里面为了保证数据的离线能力，提供了很多的 sink，而这个 sink 包含了类似像 influxdb、sls、钉钉等等一系列 sink。这个 sink 主要做的是把数据采集下来，并且把这个数据离线走，然后很多客户会用 influxdb 做这个数据离线，在 influxdb 上去接入类似像 grafana 监控数据的一个可视化的软件，来实践监控数据的可视化。</li>

</ul>

<p>但是后来社区发现，这些 sink 很多时候都是没有人来维护的。这也导致整个 Heapster 的项目有很多的 bug，这个 bug 一直存留在社区里面，是<strong>没有人修复</strong>的，这个也是会给社区的项目的活跃度包括项目的稳定性带来了很多的挑战。</p>

<p>基于这两点原因，K8s 把 Heapster 进行了 break 掉，然后做了一个精简版的监控采集组件，叫做 metrics-server。</p>

<p><img src="assets/FhGU1YMHEaTsC9J02JwbptJ1Y7_1" alt="avatar"></p>

<p>上图是 Heapster 内部的一个架构。大家可以发现它分为几个部分，第一个部分是 core 部分，然后上层是有一个通过标准的 http 或者 https 暴露的这个 API。然后中间是 source 的部分，source 部分相当于是采集数据暴露的不同的接口，然后 processor 的部分是进行数据转换以及数据聚合的部分。最后是 sink 部分，sink 部分是负责数据离线的，这个是早期的 Heapster 的一个应用的架构。那到后期的时候呢，K8s 做了这个监控接口的一个标准化，逐渐就把 Heapster 进行了裁剪，转化成了 metrics-server。</p>

<p><img src="assets/Frw_NaylxdgOvTXUQLVFVC32zBBA" alt="avatar"></p>

<p>目前 0.3.1 版本的 metrics-server 大致的一个结构就变成了上图这样，是非常简单的：有一个 core 层、中间的 source 层，以及简单的 API 层，额外增加了 API Registration 这层。这层的作用就是它可以把相应的数据接口注册到 K8s 的 API server 之上，以后客户不再需要通过这个 API 层去访问 metrics-server，而是可以通过这个 API 注册层，通过 API server 访问 API 注册层，再到 metrics-server。这样的话，真正的数据消费方可能感知到的并不是一个 metrics-server，而是说感知到的是实现了这样一个 API 的具体的实现，而这个实现是 metrics-server。这个就是 metrics-server 改动最大的一个地方。</p>

<h3>Kubernetes 的监控接口标准</h3>

<p>在 K8s 里面针对于监控，有三种不同的接口标准。它将监控的数据消费能力进行了标准化和解耦，实现了一个与社区的融合，社区里面主要分为三类。</p>

<h4>第一类 Resource Metrice</h4>

<p>对应的接口是 metrics.k8s.io，主要的实现就是 metrics-server，它提供的是资源的监控，比较常见的是节点级别、pod 级别、namespace 级别、class 级别。这类的监控指标都可以通过 metrics.k8s.io 这个接口获取到。</p>

<h4>第二类 Custom Metrics</h4>

<p>对应的 API 是 custom.metrics.k8s.io，主要的实现是 Prometheus。它提供的是资源监控和自定义监控，资源监控和上面的资源监控其实是有覆盖关系的，而这个自定义监控指的是：比如应用上面想暴露一个类似像在线人数，或者说调用后面的这个数据库的 MySQL 的慢查询。这些其实都是可以在应用层做自己的定义的，然后并通过标准的 Prometheus 的 client，暴露出相应的 metrics，然后再被 Prometheus 进行采集。</p>

<p>而这类的接口一旦采集上来也是可以通过类似像 custom.metrics.k8s.io 这样一个接口的标准来进行数据消费的，也就是说现在如果以这种方式接入的 Prometheus，那你就可以通过 custom.metrics.k8s.io 这个接口来进行 HPA，进行数据消费。</p>

<h4>第三类 External Metrics</h4>

<p>External Metrics 其实是比较特殊的一类，因为我们知道 K8s 现在已经成为了云原生接口的一个实现标准。很多时候在云上打交道的是云服务，比如说在一个应用里面用到了前面的是消息队列，后面的是 RBS 数据库。那有时在进行数据消费的时候，同时需要去消费一些云产品的监控指标，类似像消息队列中消息的数目，或者是接入层 SLB 的 connection 数目，SLB 上层的 200 个请求数目等等，这些监控指标。</p>

<p>那怎么去消费呢？也是在 K8s 里面实现了一个标准，就是 external.metrics.k8s.io。主要的实现厂商就是各个云厂商的 provider，通过这个 provider 可以通过云资源的监控指标。在阿里云上面也实现了阿里巴巴 cloud metrics adapter 用来提供这个标准的 external.metrics.k8s.io 的一个实现。</p>

<h3>Promethues - 开源社区的监控“标准”</h3>

<p>接下来我们来看一个比较常见的开源社区里面的监控方案，就是 Prometheus。Prometheus 为什么说是开源社区的监控标准呢？</p>

<ul>

<li>一是因为首先 Prometheus 是 CNCF 云原生社区的一个毕业项目。然后第二个是现在有越来越多的开源项目都以 Prometheus 作为监控标准，类似说我们比较常见的 Spark、Tensorflow、Flink 这些项目，其实它都有标准的 Prometheus 的采集接口。</li>

<li>第二个是对于类似像比较常见的一些数据库、中间件这类的项目，它都有相应的 Prometheus 采集客户端。类似像 ETCD、zookeeper、MySQL 或者说 PostgreSQL，这些其实都有相应的这个 Prometheus 的接口，如果没有的，社区里面也会有相应的 exporter 进行接口的一个实现。</li>

</ul>

<p>那我们先来看一下 Prometheus 整个的大致一个结构。</p>

<p><img src="assets/FpzNKYxLZcyNX_XNOh-OvJUuSrSb" alt="avatar"></p>

<p>上图是 Prometheus 采集的数据链路，它主要可以分为三种不同的数据采集链路。</p>

<ul>

<li>第一种，是这个 push 的方式，就是通过 pushgateway 进行数据采集，然后数据线到 pushgateway，然后 Prometheus 再通过 pull 的方式去 pushgateway 去拉数据。这种采集方式主要应对的场景就是你的这个任务可能是比较短暂的，比如说我们知道 Prometheus，最常见的采集方式是拉模式，那带来一个问题就是，一旦你的数据声明周期短于数据的采集周期，比如我采集周期是 30s，而我这个任务可能运行 15s 就完了。这种场景之下，可能会造成有些数据漏采。对于这种场景最简单的一个做法就是先通过 pushgateway，先把你的 metrics push下来，然后再通过 pull 的方式从 pushgateway 去拉数据，通过这种方式可以做到，短时间的不丢作业任务。</li>

<li>第二种是标准的 pull 模式，它是直接通过拉模式去对应的数据的任务上面去拉取数据。</li>

<li>第三种是 Prometheus on Prometheus，就是可以通过另一个 Prometheus 来去同步数据到这个 Prometheus。</li>

</ul>

<p>这是三种 Prometheus 中的采集方式。那从数据源上面，除了标准的静态配置，Prometheus 也支持 service discovery。也就是说可以通过一些服务发现的机制，动态地去发现一些采集对象。在 K8s 里面比较常见的是可以有 Kubernetes 的这种动态发现机制，只需要配置一些 annotation，它就可以自动地来配置采集任务来进行数据采集，是非常方便的。</p>

<p>在报警上面，Prometheus 提供了一个外置组件叫 Alentmanager，它可以将相应的报警信息通过邮件或者短信的方式进行数据的一个告警。在数据消费上面，可以通过上层的 API clients，可以通过 web UI，可以通过 Grafana 进行数据的展现和数据的消费。</p>

<p>总结起来 Prometheus 有如下五个特点：</p>

<ul>

<li>第一个特点就是简介强大的接入标准，开发者只需要实现 Prometheus Client 这样一个接口标准，就可以直接实现数据的一个采集；</li>

<li>第二种就是多种的数据采集、离线的方式。可以通过 push 的方式、 pull 的方式、Prometheus on Prometheus的方式来进行数据的采集和离线；</li>

<li>第三种就是和 K8s 的兼容；</li>

<li>第四种就是丰富的插件机制与生态；</li>

<li>第五个是 Prometheus Operator 的一个助力，Prometheus Operator 可能是目前我们见到的所有 Operator 里面做的最复杂的，但是它里面也是把 Prometheus 这种动态能力做到淋漓尽致的一个 Operator，如果在 K8s 里面使用 Prometheus，比较推荐大家使用 Prometheus Operator 的方式来去进行部署和运维。</li>

</ul>

<h3>kube-eventer - Kubernetes 事件离线工具</h3>

<p>最后，我们给大家介绍一个 K8s 中的事件离线工具叫做 kube-eventer。kube-eventer 是阿里云容器服务开源出的一个组件，它可以将 K8s 里面，类似像 pod eventer、node eventer、核心组件的 eventer、crd 的 eventer 等等一系列的 eventer，通过 API sever 的这个 watch 机制离线到类似像 SLS、Dingtalk、kafka、InfluxDB，然后通过这种离线的机制进行一个时间的审计、监控和告警，我们现在已经把这个项目开源到 GitHub 上了，大家有兴趣的话可以来看一下<a href="https://github.com/AliyunContainerService/kube-eventer">这个项目</a>。</p>

<p><img src="assets/FoyA1kdtOCvct2V0DjpuNKIonDie" alt="avatar"></p>

<p>那上面这张图其实就是 Dingtalk 的一个报警图。可以看见里面有一个 warning 的事件，这个事件是在 kube-system namespace 之下，具体的这个 pod，大致的一个原因是这个 pod 重启失败了，然后大致 reason 就是 backoff，然后具体发生事件是什么时间。可以通过这个信息来做到一个 Checkups。</p>

<h2>日志</h2>

<h3>日志的场景</h3>

<p>接下来给大家来介绍一下在 K8s 里面日志的一个部分。首先我们来看一下日志的场景，日志在 K8s 里面主要分为四个大的场景：</p>

<h4>主机内核的日志</h4>

<ul>

<li>第一个是主机内核的日志，主机内核日志可以协助开发者进行一些常见的问题与诊断，比如说网栈的异常，类似像我们的 iptables mark，它可以看到有 controller table 这样的一些 message；</li>

<li>第二个是驱动异常，比较常见的是一些网络方案里面有的时候可能会出现驱动异常，或者说是类似 GPU 的一些场景，驱动异常可能是比较常见的一些错误；</li>

<li>第三个就是文件系统异常，在早期 docker 还不是很成熟的场景之下，overlayfs 或者是 AUFS，实际上是会经常出现问题的。在这些出现问题后，开发者是没有太好的办法来去进行监控和诊断的。这一部分，其实是可以主机内核日志里面来查看到一些异常；</li>

<li>再往下是影响节点的一些异常，比如说内核里面的一些 kernel panic，或者是一些 OOM，这些也会在主机日志里面有相应的一些反映。</li>

</ul>

<h4>Runtime 的日志</h4>

<p>第二个是 runtime 的日志，比较常见的是 Docker 的一些日志，我们可以通过 docker 的日志来排查类似像删除一些 Pod Hang 这一系列的问题。</p>

<h4>核心组件的日志</h4>

<p>第三个是核心组件的日志，在 K8s 里面核心组件包含了类似像一些外置的中间件，类似像 etcd，或者像一些内置的组件，类似像 API server、kube-scheduler、controller-manger、kubelet 等等这一系列的组件。而这些组件的日志可以帮我们来看到整个 K8s 集群里面管控面的一个资源的使用量，然后以及目前运行的一个状态是否有一些异常。</p>

<p>还有的就是类似像一些核心的中间件，如 Ingress 这种网络中间件，它可以帮我们来看到整个的一个接入层的一个流量，通过 Ingress 的日志，可以做到一个很好的接入层的一个应用分析。</p>

<h4>部署应用的日志</h4>

<p>最后是部署应用的日志，可以通过应用的日志来查看业务层的一个状态。比如说可以看业务层有没有 500 的请求？有没有一些 panic？有没有一些异常的错误的访问？那这些其实都可以通过应用日志来进行查看的。</p>

<h3>日志的采集</h3>

<p>首先我们来看一下日志采集，从采集位置是哪个划分，需要支持如下三种：</p>

<p><img src="assets/FkMnAr3Yht5fgO1uiGKRf9TmtIPS" alt="avatar"></p>

<ul>

<li>首先是宿主机文件，这种场景比较常见的是说我的这个容器里面，通过类似像 volume，把日志文件写到了宿主机之上。通过宿主机的日志轮转的策略进行日志的轮转，然后再通过我的宿主机上的这个 agent 进行采集；</li>

<li>第二种是容器内有日志文件，那这种常见方式怎么处理呢，比较常见的一个方式是说我通过一个 Sidecar 的 streaming 的 container，转写到 stdout，通过 stdout 写到相应的 log-file，然后再通过本地的一个日志轮转，然后以及外部的一个 agent 采集；</li>

<li>第三种我们直接写到 stdout，这种比较常见的一个策略，第一种就是直接我拿这个 agent 去采集到远端，第二种我直接通过类似像一些 sls 的标准 API 采集到远端。</li>

</ul>

<p>那社区里面其实比较推荐的是使用 **Fluentd **的一个采集方案，Fluentd 是在每一个节点上面都会起相应的 agent，然后这个 agent 会把数据汇集到一个 Fluentd 的一个 server，这个 server 里面可以将数据离线到相应的类似像 elasticsearch，然后再通过 kibana 做展现；或者是离线到 influxdb，然后通过 Grafana 做展现。这个其实是社区里目前比较推荐的一个做法。</p>

<h2>总结</h2>

<p>最后给大家做一下今天课程的总结，以及给大家介绍一下在阿里云上面监控和日志的最佳实践。在课程开始的时候，给大家介绍了监控和日志并不属于 K8s 里面的核心组件，而大部分是定义了一个标准的一个接口方式，然后通过上层的这个云厂商进行各自的一个适配。</p>

<h3>阿里云容器服务监控体系</h3>

<h4>监控体系组件介绍</h4>

<p>首先，我先给大家来介绍一下在阿里云容器服务里面的监控体系，这张图实际上是监控的一个大图。</p>

<p><img src="assets/FgozeF6IuXmJGPnhv-KeLXP5xlrb" alt="avatar"></p>

<p>右侧的四个产品是和监控日志相关比较紧密的四个产品：</p>

<h5>sls</h5>

<p>第一个是 SLS，就是日志服务，那刚才我们已经提到了在 K8s 里面日志分为很多种不同的采集，比如说有核心组件的日志、接入层的日志、还有应用的日志等等。在阿里云容器服务里面，可以通过 API server 采集到审计的日志，然后可以通过类似像 service mesh 或者 ingress controller 采集到接入层的日志，然后以及相应的应用层采集到应用的日志。</p>

<p>有了这条数据链路之后，其实还不够。因为数据链路只是帮我们做到了一个数据的离线，我们还需要做上层的数据的展现和分析。比如说像审计，可以通过审计日志来看到今天有多少操作、有多少变更、有没有攻击、系统有没有异常。这些都可以通过审计的 Dashboard 来查看。</p>

<h5>ARMS</h5>

<p>第二个就是应用的一个性能监控。性能监控上面，可以通过这个 ARMS 这样的产品来去进行查看。ARMS 目前支持的 JAVA、PHP 两种语言，可以通过 ARMS 来做应用的一个性能诊断和问题的一个调优。</p>

<h5>AHAS</h5>

<p>第三个是比较特殊的叫 AHAS。AHAS 是一个架构感知的监控，我们知道在 K8s 里面，很多时候都是通过一些微服的架构进行部署的。微服带来的问题就是组件会变的非常多，组件的副本处也会变的很多。这会带来一个在拓扑管理上面的一个复杂性。</p>

<p>如果我们想要看一个应用在 K8s 中流量的一个走向，或者是针对流量异常的一个排查，其实没有一个很好的可视化是很复杂的。AHAS 的一个作用就是通过网络栈的一个监控，可以绘制出整个 K8s 中应用的一个拓扑关系，然后以及相应的资源监控和网络的带宽监控、流量的监控，以及异常事件的一个诊断。任何如果有架构拓扑感知的一个层面，来实现另一种的监控解决方案。</p>

<h5>Cloud Monitor</h5>

<p>最后是 Cloud Monitor，也就是基础的云监控。它可以采集标准的 Resource Metrics Monitoring，来进行监控数据的一个展现，可以实现 node、pod 等等监控指标的一个展现和告警。</p>

<h4>阿里云增强的功能</h4>

<p>这一部分是阿里云在开源上做的增强。首先是 metrics-server，文章开始提到了 metrics-server 做了很多的一个精简。但是从客户的角度来讲，这个精简实际上是把一些功能做了一个裁剪，这将会带来很多不便。比如说有很多客户希望将监控数据离线到类似像 SLS 或者是 influxdb，这种能力实际上用社区的版本是没有办法继续来做的，这个地方阿里云继续保留了常见的维护率比较高的 sink，这是第一个增强。</p>

<p>然后是第二个增强，因为在 K8s 里面整合的一个生态的发展并不是以同样的节奏进行演进的。比如说 Dashboard 的发布，并不是和 K8s 的大版本进行匹配的。比如 K8s 发了 1.12，Dashboard 并不会也发 1.12 的版本，而是说它会根据自己的节奏来去发布，这样会造成一个结果就是说以前依赖于 Heapster 的很多的组件在升级到 metrics-server 之后就直接 break 掉，阿里云在 metrics-server 上面做了完整的 Heapster 兼容，也就是说从目前 K8s 1.7 版本一直到 K8s 1.14 版本，都可以使用阿里云的 metrics-server，来做到完整的监控组件的消费的一个兼容。</p>

<p>还有就是 eventer 和 npd，上面提到了 kube-eventer 这个组件。然后在 npd 上面，我们也做了很多额外的增强，类似像增加了很多监控和检测项，类似像 kernel Hang、npd 的一个检测、出入网的监控、snat 的一个检测。然后还有类似像 fd 的 check，这些其实都是在 npd 里面的一些监控项，阿里云做了很多的增强。然后开发者可以直接部署 npd 的一个 check，就可以实现节点诊断的一个告警，然后并通过 eventer 离线上的 kafka 或者是 Dingtalk。</p>

<p>再往上是 Prometheus 生态，Prometheus 生态里面，在存储层可以让开发者对接，阿里云的 HiTSDB 以及 InfluxDB，然后在采集层提供了优化的 node-exporter，以及一些场景化监控的 exporter，类似像 Spark、TensorFlow、Argo 这类场景化的 exporter。还有就是针对于 GPU，阿里云做了很多额外的增强，类似于像支持 GPU 的单卡监控以及 GPU share 的监控。</p>

<p>然后在 Prometheus 上面，我们连同 ARMS 团队推出了托管版的 Prometheus，开发者可以使用开箱即用的 helm chats，不需要部署 Prometheus server，就可以直接体验到 Prometheus 的一个监控采集能力。</p>

<h3>阿里云容器服务日志体系</h3>

<p>在日志上面，阿里云做了哪些增强呢？首先是采集方式上，做到了完整的一个兼容。可以采集 pod log 日志、核心组件日志、docker engine 日志、kernel 日志，以及类似像一些中间件的日志，都收集到 SLS。收集到 SLS 之后，我们可以通过数据离线到 OSS，离线到 Max Compute，做一个数据的离线和归档，以及离线预算。</p>

<p>然后还有是对于一些数据的实时消费，我们可以到 Opensearch、可以到 E-Map、可以到 Flink，来做到一个日志的搜索和上层的一个消费。在日志展现上面，我们可以既对接开源的 Grafana，也可以对接类似像 DataV，去做数据展示，实现一个完整的数据链路的采集和消费。</p>

<h2>本文总结</h2>

<p>今天的课程到这里就结束了，下面为大家进行要点总结：</p>

<ul>

<li>首先主要为大家介绍了监控，其中包括：四种容器场景下的常见的监控方式；Kubernetes 的监控演进和接口标准；两种常用的来源的监控方案；</li>

<li>在日志上我们主要介绍了四种不同的场景，介绍了 Fluentd 的一个采集方案；</li>

<li>最后向大家介绍了一下阿里云日志和监控的一个最佳实践。</li>

</ul>

</div>

                    </div><div class="book-post">

                        <p id="tip" align="center"></p>

                        <div><h1>13 Kubernetes 网络概念及策略控制（叶磊）</h1>

<p>本文将主要分享以下 5 方面的内容：</p>

<ol>

<li>Kubernetes 基本网络模型；</li>

<li>Netns 探秘；</li>

<li>主流网络方案简介；</li>

<li>Network Policy 的用处；</li>

<li>思考时间。</li>

</ol>

<h2>Kubernetes 基本网络模型</h2>

<p>本节来介绍一下 Kubernetes 对网络模型的一些想法。大家知道 Kubernetes 对于网络具体实现方案，没有什么限制，也没有给出特别好的参考案例。Kubernetes 对一个容器网络是否合格做出了限制，也就是 Kubernetes 的容器网络模型。可以把它归结为约法三章和四大目标。</p>

<ul>

<li>约法三章的意思是：在评价一个容器网络或者设计容器网络的时候，它的准入条件。它需要满足哪三条？ 才能认为它是一个合格的网络方案。</li>

<li>四大目标意思是在设计这个网络的拓扑，设计网络的具体功能的实现的时候，要去想清楚，能不能达成连通性等这几大指标。</li>

</ul>

<h3>约法三章</h3>

<p>先来看下约法三章：</p>

<ul>

<li>第一条：任意两个 pod 之间其实是可以直接通信的，无需经过显式地使用 NAT 来接收数据和地址的转换；</li>

<li>第二条：node 与 pod 之间是可以直接通信的，无需使用明显的地址转换；</li>

<li>第三条：pod 看到自己的 IP 跟别人看见它所用的IP是一样的，中间不能经过转换。</li>

</ul>

<p>后文中会讲一下我个人的理解，为什么 Kubernetes 对容器网络会有一些看起来武断的模型和要求。</p>

<h3>四大目标</h3>

<p>四大目标其实是在设计一个 K8s 的系统为外部世界提供服务的时候，从网络的角度要想清楚，外部世界如何一步一步连接到容器内部的应用？</p>

<ul>

<li>**外部世界和 service 之间是怎么通信的？**就是有一个互联网或者是公司外部的一个用户，怎么用到 service？service 特指 K8s 里面的服务概念。</li>

<li><strong>service 如何与它后端的 pod 通讯？</strong></li>

<li><strong>pod 和 pod 之间调用是怎么做到通信的？</strong></li>

<li><strong>最后就是 pod 内部容器与容器之间的通信？</strong></li>

</ul>

<p>最终要达到目标，就是外部世界可以连接到最里面，对容器提供服务。</p>

<h3>对基本约束的解释</h3>

<p>对基本约束，可以做出这样一些解读：因为容器的网络发展复杂性就在于它其实是寄生在 Host 网络之上的。从这个角度讲，可以把容器网络方案大体分为 <strong>Underlay/Overlay</strong> 两大派别：</p>

<ul>

<li>

<p>Underlay 的标准是它与 Host 网络是同层的，从外在可见的一个特征就是它是不是使用了 Host 网络同样的网段、输入输出基础设备、容器的 IP 地址是不是需要与 Host 网络取得协同（来自同一个中心分配或统一划分）。这就是 Underlay；</p>

</li>

<li>

<p>Overlay 不一样的地方就在于它并不需要从 Host 网络的 IPM 的管理的组件去申请IP，一般来说，它只需要跟 Host 网络不冲突，这个 IP 可以自由分配的。</p>

<p><img src="assets/Fhz1xeIxAKpYT30nYTMDPEe2fsKF" alt="avatar"></p>

</li>

</ul>

<p>为什么社区会提出 perPodperIP 这种简单武断的模型呢？我个人是觉得这样为后面的 service 管理一些服务的跟踪性能监控，带来了非常多的好处。因为一个 IP 一贯到底，对 case 或者各种不大的事情都会有很大的好处。</p>

<h2>Netns 探秘</h2>

<h3>Netns 究竟实现了什么</h3>

<p>下面简单讲一下，Network Namespace 里面能网络实现的内核基础。狭义上来说 runC 容器技术是不依赖于任何硬件的，它的执行基础就是它的内核里面，进程的内核代表就是 task，它如果不需要隔离，那么用的是主机的空间（ namespace），并不需要特别设置的空间隔离数据结构（ nsproxy-namespace proxy）。</p>

<p><img src="assets/FoojsS015WB_S-B_daEr2ho1gSa4" alt="avatar"></p>

<p>相反，如果一个独立的网络 proxy，或者 mount proxy，里面就要填上真正的私有数据。它可以看到的数据结构如上图所示。</p>

<p>从感官上来看一个隔离的网络空间，它会拥有自己的网卡或者说是网络设备。网卡可能是虚拟的，也可能是物理网卡，它会拥有自己的 IP 地址、IP 表和路由表、拥有自己的协议栈状态。这里面特指就是 TCP/Ip协议栈，它会有自己的status，会有自己的 iptables、ipvs。</p>

<p>从整个感官上来讲，这就相当于拥有了一个完全独立的网络，它与主机网络是隔离的。当然协议栈的代码还是公用的，只是数据结构不相同。</p>

<h3>Pod 与 Netns 的关系</h3>

<p><img src="assets/Fhtlyi6PUwJHW4e1-BJTtaXuPl6B" alt="avatar"></p>

<p>这张图可以清晰表明 pod 里 Netns 的关系，每个 pod 都有着独立的网络空间，pod net container 会共享这个网络空间。一般 K8s 会推荐选用 Loopback 接口，在 pod net container 之间进行通信，而所有的 container 通过 pod 的 IP 对外提供服务。另外对于宿主机上的 Root Netns，可以把它看做一个特殊的网络空间，只不过它的 Pid 是1。</p>

<h2>主流网络方案简介</h2>

<h3>典型的容器网络实现方案</h3>

<p>接下来简单介绍一下典型的容器网络实现方案。容器网络方案可能是 K8s 里最为百花齐放的一个领域，它有着各种各样的实现。容器网络的复杂性，其实在于它需要跟底层 Iass 层的网络做协调、需要在性能跟 IP 分配的灵活性上做一些选择，这个方案是多种多样的。</p>

<p><img src="assets/FmJ42-sGp6Jlcmht2-KScPPIoos5" alt="avatar"></p>

<p>下面简单介绍几个比较主要的方案：分别是 Flannel、Calico、Canal ，最后是 WeaveNet，中间的大多数方案都是采用了跟 Calico 类似的策略路由的方法。</p>

<ul>

<li><strong>Flannel</strong> 是一个比较大一统的方案，它提供了多种的网络 backend。不同的 backend 实现了不同的拓扑，它可以覆盖多种场景；</li>

<li><strong>Calico</strong> 主要是采用了策略路由，节点之间采用 BGP 的协议，去进行路由的同步。它的特点是功能比较丰富，尤其是对 Network Point 支持比较好，大家都知道 Calico 对底层网络的要求，一般是需要 mac 地址能够直通，不能跨二层域；</li>

<li>当然也有一些社区的同学会把 Flannel 的优点和 Calico 的优点做一些集成。我们称之为嫁接型的创新项目 <strong>Cilium</strong>；</li>

<li>最后讲一下 <strong>WeaveNet</strong>，如果大家在使用中需要对数据做一些加密，可以选择用 WeaveNet，它的动态方案可以实现比较好的加密。</li>

</ul>

<h3>Flannel 方案</h3>

<p><img src="assets/FhssEuWNm9DD-eRNnc51NNkEObNb" alt="avatar"></p>

<p>Flannel 方案是目前使用最为普遍的。如上图所示，可以看到一个典型的容器网方案。它首先要解决的是 container 的包如何到达 Host，这里采用的是加一个 Bridge 的方式。它的 backend 其实是独立的，也就是说这个包如何离开 Host，是采用哪种封装方式，还是不需要封装，都是可选择的。</p>

<p>现在来介绍三种主要的 backend：</p>

<ul>

<li>一种是用户态的 udp，这种是最早期的实现；</li>

<li>然后是内核的 Vxlan，这两种都算是 overlay 的方案。Vxlan 的性能会比较好一点，但是它对内核的版本是有要求的，需要内核支持 Vxlan 的特性功能；</li>

<li>如果你的集群规模不够大，又处于同一个二层域，也可以选择采用 host-gw 的方式。这种方式的 backend 基本上是由一段广播路由规则来启动的，性能比较高。</li>

</ul>

<h2>Network Policy 的用处</h2>

<h3>Network Policy 基本概念</h3>

<p>下面介绍一下 Network Policy 的概念。</p>

<p><img src="assets/FgcMnXjYQ4VCi7tdl6TY-gcIFJBY" alt="avatar"></p>

<p>刚才提到了 Kubernetes 网络的基本模型是需要 pod 之间全互联，这个将带来一些问题：可能在一个 K8s 集群里，有一些调用链之间是不会直接调用的。比如说两个部门之间，那么我希望 A 部门不要去探视到 B 部门的服务，这个时候就可以用到策略的概念。</p>

<p>基本上它的想法是这样的：它采用各种选择器（标签或 namespace），找到一组 pod，或者找到相当于通讯的两端，然后通过流的特征描述来决定它们之间是不是可以联通，可以理解为一个白名单的机制。</p>

<p>在使用 Network Policy 之前，如上图所示要注意 apiserver 需要打开一下这几个开关。另一个更重要的是我们选用的网络插件需要支持 Network Policy 的落地。大家要知道，Network Policy 只是 K8s 提供的一种对象，并没有内置组件做落地实施，需要取决于你选择的容器网络方案对这个标准的支持与否及完备程度，如果你选择 Flannel 之类，它并没有真正去落地这个 Policy，那么你试了这个也没有什么用。</p>

<h3>配置实例</h3>

<p><img src="assets/Fveji-UPz0yJjDG1dsDW3zqBikeT" alt="avatar"></p>

<p>接下来讲一个配置的实例，或者说在设计一个 Network Policy 的时候要做哪些事情？我个人觉得需要决定三件事：</p>

<ul>

<li>第一件事是控制对象，就像这个实例里面 spec 的部分。spec 里面通过 podSelector 或者 namespace 的 selector，可以选择做特定的一组 pod 来接受我们的控制；</li>

<li>第二个就是对流向考虑清楚，需要控制入方向还是出方向？还是两个方向都要控制？</li>

<li>最重要的就是第三部分，如果要对选择出来的方向加上控制对象来对它流进行描述，具体哪一些 stream 可以放进来，或者放出去？类比这个流特征的五元组，可以通过一些选择器来决定哪一些可以作为我的远端，这是对象的选择；也可以通过 IPBlock 这种机制来得到对哪些 IP 是可以放行的；最后就是哪些协议或哪些端口。其实流特征综合起来就是一个五元组，会把特定的能够接受的流选择出来 。</li>

</ul>

<h2>本节课总结</h2>

<p>本节内容到这里就结束了，我们简单总结一下：</p>

<ul>

<li>

<p>在 pod 的容器网络中核心概念就是 IP，IP 就是每个 pod 对外通讯的地址基础，必须内外一致，符合 K8s 的模型特征；</p>

</li>

<li>

<p>那么在介绍网络方案的时候，影响容器网络性能最关键的就是拓扑。要能够理解你的包端到端是怎么联通的，中间怎么从 container 到达 Host，Host 出了 container 是要封装还是解封装？还是通过策略路由？最终到达对端是怎么解出来的？</p>

</li>

<li>

<p>容器网络选择和设计选择。如果你并不清楚你的外部网络，或者你需要一个普适性最强的方案，假设说你对 mac 是否直连不太清楚、对外部路由器的路由表能否控制也不太清楚，那么你可以选择 Flannel 利用 Vxlan 作为 backend 的这种方案。如果你确信你的网络是 2 层可直连的，你可以进行选用 Calico 或者 Flannel-Hostgw 作为一个 backend；</p>

</li>

<li>

<p>最后就是对 Network Policy，在运维和使用的时候，它是一个很强大的工具，可以实现对进出流的精确控制。实现的方法我们也介绍了，要想清楚你要控制谁，然后你的流要怎么去定义。</p>

</li>

</ul>

<h2>思考时间</h2>

<p>最后留一些思考，大家可以想一想：</p>

<ol>

<li>为什么接口标准化 CNI 化了，但是容器网络却没有一个很标准的实现，内置在 K8s 里面？</li>

<li>Network Policy 为什么没有一个标准的 controller 或者一个标准的实现，而是交给这个容器网络的 owner 来提供？</li>

<li>有没有可能完全不用网络设备来实现容器网络呢？考虑到现在有 RDMA 等有别于 TCP/IP 的这种方案。</li>

<li>在运维过程中网络问题比较多、也比较难排查，那么值不值得做一个开源工具，让它可以友好的展示从 container 到 Host 之间、Host 到 Host 之间，或者说封装及解封装之间，各个阶段的网络情况，有没有出现问题，能够快速的定位。据我所知应该现在是没有这样的工具的。</li>

</ol>

<p>以上就是我对 K8s 容器网络的基本概念、以及 Network Policy 的一些介绍，谢谢大家的观看。</p>

</div>

                    </div><div class="book-post">

                        <p id="tip" align="center"></p>

                        <div><h1>14 Kubernetes Service（溪恒）</h1>

<h2>需求来源</h2>

<h3>为什么需要服务发现</h3>

<p>在 K8s 集群里面会通过 pod 去部署应用，与传统的应用部署不同，传统应用部署在给定的机器上面去部署，我们知道怎么去调用别的机器的 IP 地址。但是在 K8s 集群里面应用是通过 pod 去部署的， 而 pod 生命周期是短暂的。在 pod 的生命周期过程中，比如它创建或销毁，它的 IP 地址都会发生变化，这样就不能使用传统的部署方式，不能指定 IP 去访问指定的应用。</p>

<p>另外在 K8s 的应用部署里，之前虽然学习了 deployment 的应用部署模式，但还是需要创建一个 pod 组，然后这些 pod 组需要提供一个统一的访问入口，以及怎么去控制流量负载均衡到这个组里面。比如说测试环境、预发环境和线上环境，其实在部署的过程中需要保持同样的一个部署模板以及访问方式。因为这样就可以用同一套应用的模板在不同的环境中直接发布。</p>

<h3>Service：Kubernetes 中的服务返现与负载均衡</h3>

<p>最后应用服务需要暴露到外部去访问，需要提供给外部的用户去调用的。我们上节了解到 pod 的网络跟机器不是同一个段的网络，那怎么让 pod 网络暴露到去给外部访问呢？这时就需要服务发现。</p>

<p><img src="assets/FsNSc0DcnIAm90gDtlBd-ipYG4aE" alt="avatar"></p>

<p>在 K8s 里面，服务发现与负载均衡就是 K8s Service。上图就是在 K8s 里 Service 的架构，K8s Service 向上提供了外部网络以及 pod 网络的访问，即外部网络可以通过 service 去访问，pod 网络也可以通过 K8s Service 去访问。</p>

<p>向下，K8s 对接了另外一组 pod，即可以通过 K8s Service 的方式去负载均衡到一组 pod 上面去，这样相当于解决了前面所说的复发性问题，或者提供了统一的访问入口去做服务发现，然后又可以给外部网络访问，解决不同的 pod 之间的访问，提供统一的访问地址。</p>

<h2>用例解读</h2>

<p>下面进行实际的一个用例解读，看 pod K8s 的 service 要怎么去声明、怎么去使用？</p>

<h3>Service 语法</h3>

<p><img src="assets/Fl_9VxVULnLmtkexITpiECoDO-v1" alt="avatar"></p>

<p>首先来看 K8s Service 的一个语法，上图实际就是 K8s 的一个声明结构。这个结构里有很多语法，跟之前所介绍的 K8s 的一些标准对象有很多相似之处。比如说标签 label 去做一些选择、selector 去做一些选择、label 去声明它的一些 label 标签等。</p>

<p>这里有一个新的知识点，就是定义了用于 K8s Service 服务发现的一个协议以及端口。继续来看这个模板，声明了一个名叫 my-service 的一个 K8s Service，它有一个 app:my-service 的 label，它选择了 app:MyApp 这样一个 label 的 pod 作为它的后端。</p>

<p>最后是定义的服务发现的协议以及端口，这个示例中我们定义的是 TCP 协议，端口是 80，目的端口是 9376，效果是访问到这个 service 80 端口会被路由到后端的 targetPort，就是只要访问到这个 service 80 端口的都会负载均衡到后端 app：MyApp 这种 label 的 pod 的 9376 端口。</p>

<h3>创建和查看 Service</h3>

<p>如何去创建刚才声明的这个 service 对象，以及它创建之后是什么样的效果呢？通过简单的命令：</p>

<ul>

<li>kubectl apply -f service.yaml</li>

</ul>

<p>或者是</p>

<ul>

<li>kubectl created -f service.yaml</li>

</ul>

<p>上面的命令可以简单地去创建这样一个 service。创建好之后，可以通过：</p>

<ul>

<li>kubectl discribe service</li>

</ul>

<p>去查看 service 创建之后的一个结果。</p>

<p><img src="assets/FjSOqAh-KD96eHj5Cm1lwApCPArZ" alt="avatar"></p>

<p>service 创建好之后，你可以看到它的名字是 my-service。Namespace、Label、Selector 这些都跟我们之前声明的一样，这里声明完之后会生成一个 IP 地址，这个 IP 地址就是 service 的 IP 地址，这个 IP 地址在集群里面可以被其它 pod 所访问，相当于通过这个 IP 地址提供了统一的一个 pod 的访问入口，以及服务发现。</p>

<p>这里还有一个 Endpoints 的属性，就是我们通过 Endpoints 可以看到：通过前面所声明的 selector 去选择了哪些 pod？以及这些 pod 都是什么样一个状态？比如说通过 selector，我们看到它选择了这些 pod 的一个 IP，以及这些 pod 所声明的 targetPort 的一个端口。</p>

<p><img src="assets/FtrSVn8y4xXkzK2W_ypZA-bg6KFX" alt="avatar"></p>

<p>实际的架构如上图所示。在 service 创建之后，它会在集群里面创建一个虚拟的 IP 地址以及端口，在集群里，所有的 pod 和 node 都可以通过这样一个 IP 地址和端口去访问到这个 service。这个 service 会把它选择的 pod 及其 IP 地址都挂载到后端。这样通过 service 的 IP 地址访问时，就可以负载均衡到后端这些 pod 上面去。</p>

<p>当 pod 的生命周期有变化时，比如说其中一个 pod 销毁，service 就会自动从后端摘除这个 pod。这样实现了：就算 pod 的生命周期有变化，它访问的端点是不会发生变化的。</p>

<h3>集群内访问 Service</h3>

<p>在集群里面，其他 pod 要怎么访问到我们所创建的这个 service 呢？有三种方式：</p>

<ul>

<li>首先我们可以通过 service 的虚拟 IP 去访问，比如说刚创建的 my-service 这个服务，通过 kubectl get svc 或者 kubectl discribe service 都可以看到它的虚拟 IP 地址是 172.29.3.27，端口是 80，然后就可以通过这个虚拟 IP 及端口在 pod 里面直接访问到这个 service 的地址。</li>

<li>第二种方式直接访问服务名，依靠 DNS 解析，就是同一个 namespace 里 pod 可以直接通过 service 的名字去访问到刚才所声明的这个 service。不同的 namespace 里面，我们可以通过 service 名字加“.”，然后加 service 所在的哪个 namespace 去访问这个 service，例如我们直接用 curl 去访问，就是 my-service:80 就可以访问到这个 service。</li>

<li>第三种是通过环境变量访问，在同一个 namespace 里的 pod 启动时，K8s 会把 service 的一些 IP 地址、端口，以及一些简单的配置，通过环境变量的方式放到 K8s 的 pod 里面。在 K8s pod 的容器启动之后，通过读取系统的环境变量比读取到 namespace 里面其他 service 配置的一个地址，或者是它的端口号等等。比如在集群的某一个 pod 里面，可以直接通过 curl $ 取到一个环境变量的值，比如取到 MY<em>SERVICE</em>SERVICE<em>HOST 就是它的一个 IP 地址，MY</em>SERVICE 就是刚才我们声明的 MY<em>SERVICE，SERVICE</em>PORT 就是它的端口号，这样也可以请求到集群里面的 MY_SERVICE 这个 service。</li>

</ul>

<h3>Headless Service</h3>

<p>service 有一个特别的形态就是 Headless Service。service 创建的时候可以指定 clusterIP:None，告诉 K8s 说我不需要 clusterIP（就是刚才所说的集群里面的一个虚拟 IP），然后 K8s 就不会分配给这个 service 一个虚拟 IP 地址，它没有虚拟 IP 地址怎么做到负载均衡以及统一的访问入口呢？</p>

<p>它是这样来操作的：pod 可以直接通过 service_name 用 DNS 的方式解析到所有后端 pod 的 IP 地址，通过 DNS 的 A 记录的方式会解析到所有后端的 Pod 的地址，由客户端选择一个后端的 IP 地址，这个 A 记录会随着 pod 的生命周期变化，返回的 A 记录列表也发生变化，这样就要求客户端应用要从 A 记录把所有 DNS 返回到 A 记录的列表里面 IP 地址中，客户端自己去选择一个合适的地址去访问 pod。</p>

<p><img src="assets/FpaWBf0-QsUFdbAv_c76ZeZLSmuO" alt="avatar"></p>

<p>可以从上图看一下跟刚才我们声明的模板的区别，就是在中间加了一个 clusterIP:None，即表明不需要虚拟 IP。实际效果就是集群的 pod 访问 my-service 时，会直接解析到所有的 service 对应 pod 的 IP 地址，返回给 pod，然后 pod 里面自己去选择一个 IP 地址去直接访问。</p>

<h3>向集群外暴露 Service</h3>

<p>前面介绍的都是在集群里面 node 或者 pod 去访问 service，service 怎么去向外暴露呢？怎么把应用实际暴露给公网去访问呢？这里 service 也有两种类型去解决这个问题，一个是 NodePort，一个是 LoadBalancer。</p>

<ul>

<li>

<p>NodePort 的方式就是在集群的 node 上面（即集群的节点的宿主机上面）去暴露节点上的一个端口，这样相当于在节点的一个端口上面访问到之后就会再去做一层转发，转发到虚拟的 IP 地址上面，就是刚刚宿主机上面 service 虚拟 IP 地址。</p>

</li>

<li>

<p>LoadBalancer 类型就是在 NodePort 上面又做了一层转换，刚才所说的 NodePort 其实是集群里面每个节点上面一个端口，LoadBalancer 是在所有的节点前又挂一个负载均衡。比如在阿里云上挂一个 SLB，这个负载均衡会提供一个统一的入口，并把所有它接触到的流量负载均衡到每一个集群节点的 node pod 上面去。然后 node pod 再转化成 ClusterIP，去访问到实际的 pod 上面。</p>

</li>

</ul>

<h2>操作演示</h2>

<p>下面进行实际操作演示，在阿里云的容器服务上面进去体验一下如何使用 K8s Service。</p>

<h3>创建 Service</h3>

<p>我们已经创建好了一个阿里云的容器集群，然后并且配置好本地终端到阿里云容器集群的一个连接。</p>

<p>首先可以通过 kubectl get cs ，可以看到我们已经正常连接到了阿里云容器服务的集群上面去。</p>

<p><img src="assets/FussFCyed8qkgeqLQQqlja_vG2ln" alt="avatar"></p>

<p>今天将通过这些模板实际去体验阿里云服务上面去使用 K8s Service。有三个模板，首先是 client，就是用来模拟通过 service 去访问 K8s 的 service，然后负载均衡到我们的 service 里面去声明的一组 pod 上。</p>

<p><img src="assets/FlkYzXSj2LQOsZ3VP8noupeMgcno" alt="avatar"></p>

<p>K8s Service 的上面，跟刚才介绍一样，我们创建了一个 K8s Service 模板，里面 pod，K8s Service 会通过前端指定的 80 端口负载均衡到后端 pod 的 80 端口上面，然后 selector 选择到 run:nginx 这样标签的一些 pod 去作为它的后端。</p>

<p><img src="assets/FoNi0Bx7CTNHvlsvoORGa7K6CitD" alt="avatar"></p>

<p>然后去创建带有这样标签的一组 pod，通过什么去创建 pod 呢？就是之前所介绍的 K8s deployment，通过 deployment 我们可以轻松创建出一组 pod，然后上面声明 run:nginx 这样一个label，并且它有两个副本，会同时跑出来两个 pod。</p>

<p><img src="assets/Fo9OvQITzJLl-GmHd3_mcwImzs1q" alt="avatar"></p>

<p>先创建一组 pod，就是创建这个 K8s deployment，通过 kubectl create -f service.yaml。这个 deployment 也创建好了，再看一下 pod 有没有创建出来。如下图看到这个 deployment 所创建的两个 pod 都已经在 running 了。通过 kubectl get pod -o wide 可以看到 IP 地址。通过 -l，即 label 去做筛选，run=nginx。如下图所示可以看到，这两个 pod 分别是 10.0.0.135 和 10.0.0.12 这样一个 IP 地址，并且都是带 run=nginx 这个 label 的。</p>

<p><img src="assets/FrO8OaFDt8TTk4TIIcKgrR2JQom5" alt="avatar"></p>

<p>下面我们去创建 K8s service，就是刚才介绍的通过 service 去选择这两个 pod。这个 service 已经创建好了。</p>

<p><img src="assets/Fl1Remwwd4dM5XwQI-dipSmubSCC" alt="avatar"></p>

<p>根据刚才介绍，通过 kubectl describe svc 可以看到这个 service 实际的一个状态。如下图所示，刚才创建的 nginx service，它的选择器是 run=nginx，通过 run=nginx 这个选择器选择到后端的 pod 地址，就是刚才所看到那两个 pod 的地址：10.0.0.12 和 10.0.0.135。这里可以看到 K8s 为它生成了集群里面一个虚拟 IP 地址，通过这个虚拟 IP 地址，它就可以负载均衡到后面的两个 pod 上面去。</p>

<p><img src="assets/FgIJa7up714MeuWzxs03RgCmzO9o" alt="avatar"></p>

<p>现在去创建一个客户端的 pod 实际去感受一下如何去访问这个 K8s Service，我们通过 client.yaml 去创建客户端的 pod，kubectl get pod 可以看到客户端 pod 已经创建好并且已经在运行中了。</p>

<p><img src="assets/FolM0NWD4tOaeJpv94h2Y46NLR2U" alt="avatar"></p>

<p>通过 kubectl exec 到这个 pod 里面，进入这个 pod 去感受一下刚才所说的三种访问方式，首先可以直接去访问这个 K8s 为它生成的这个 ClusterIP，就是虚拟 IP 地址，通过 curl 访问这个 IP 地址，这个 pod 里面没有装 curl。通过 wget 这个 IP 地址，输入进去测试一下。可以看到通过这个去访问到实际的 IP 地址是可以访问到后端的 nginx 上面的，这个虚拟是一个统一的入口。</p>

<p><img src="assets/FljsuZcLh2n6_0wvbhJkgQw0h1hs" alt="avatar"></p>

<p>第二种方式，可以通过直接 service 名字的方式去访问到这个 service。同样通过 wget，访问我们刚才所创建的 service 名 nginx，可以发现跟刚才看到的结果是一样的。</p>

<p><img src="assets/Fi5yxWwlD_JIk5E55H88SN64StcH" alt="avatar"></p>

<p>在不同的 namespace 时，也可以通过加上 namespace 的一个名字去访问到 service，比如这里的 namespace 为 default。 <img src="assets/Fq_yIQCrXUv5Y4m_vILXYKug7P5o" alt="avatar"></p>

<p>最后我们介绍的访问方式里面还可以通过环境变量去访问，在这个 pod 里面直接通过执行 env 命令看一下它实际注入的环境变量的情况。看一下 nginx 的 service 的各种配置已经注册进来了。</p>

<p><img src="assets/FiEdQgHfGb6VXlLk-bcufkLyDjH_" alt="avatar"></p>

<p>可以通过 wget 同样去访问这样一个环境变量，然后可以访问到我们的一个 service。</p>

<p><img src="assets/Fs4FQn_R-mqcrssjuNQfrLJUVS_V" alt="avatar"></p>

<p>介绍完这三种访问方式，再看一下如何通过 service 外部的网络去访问。我们 vim 直接修改一些刚才所创建的 service。</p>

<p><img src="assets/FhyPM4xafUuVVzDvSXbnJfCHM8J4" alt="avatar"></p>

<p>最后我们添加一个 type，就是 LoadBalancer，就是我们前面所介绍的外部访问的方式。</p>

<p><img src="assets/FkST3Et2rFn638rTbcVW1KGbwC90" alt="avatar"></p>

<p>然后通过 kubectl apply，这样就把刚刚修改的内容直接生效在所创建的 service 里面。</p>

<p><img src="assets/Fqea8W8Ogt2-owgI6bXp5DXjgokt" alt="avatar"></p>

<p>现在看一下 service 会有哪些变化呢？通过 kubectl get svc -o wide，我们发现刚刚创建的 nginx service 多了一个 EXTERNAL-IP，就是外部访问的一个 IP 地址，刚才我们所访问的都是 CLUSTER-IP，就是在集群里面的一个虚拟 IP 地址。</p>

<p><img src="assets/Fjbn-BHV2p-H30EvE7la_JAA9apy" alt="avatar"></p>

<p>然后现在实际去访问一下这个外部 IP 地址 39.98.21.187，感受一下如何通过 service 去暴露我们的应用服务，直接在终端里面点一下，这里可以看到我们直接通过这个应用的外部访问端点，可以访问到这个 service，是不是很简单？</p>

<p><img src="assets/Fg_4hT2V3pLzFQv3DwoNJ-NV6PXU" alt="avatar"></p>

<p>我们最后再看一下用 service 去实现了 K8s 的服务发现，就是 service 的访问地址跟 pod 的生命周期没有关系。我们先看一下现在的 service 后面选择的是这两个 pod IP 地址。</p>

<p><img src="assets/Fu5UojYcffLzszDkpSRYtLftgMKf" alt="avatar"></p>

<p>我们现在把其中的一个 pod 删掉，通过 kubectl delete 的方式把前面一个 pod 删掉。</p>

<p><img src="assets/FotfekPF1eKhNRI3-PRJVo2eL5NC" alt="avatar"></p>

<p>我们知道 deployment 会让它自动生成一个新的 pod，现在看 IP 地址已经变成 137。</p>

<p><img src="assets/FgZVDOs5m09juiP1Q9pWuN1Y0qoW" alt="avatar"></p>

<p>现在再去 describe 一下刚才的 service，如下图,看到前面访问端点就是集群的 IP 地址没有发生变化，对外的 LoadBalancer 的 IP 地址也没有发生变化。在所有不影响客户端的访问情况下，后端的一个 pod IP 已经自动放到了 service 后端里面。</p>

<p><img src="assets/Frw3OFsRfAMCivcDWFfUKjnGWPJi" alt="avatar"></p>

<p>这样就相当于在应用的组件调用的时候可以不用关心 pod 在生命周期的一个变化。</p>

<p>以上就是所有演示。</p>

<h2>架构设计</h2>

<p>** **最后是对 K8s 设计的一个简单的分析以及实现的一些原理。</p>

<h3>Kubernetes 服务发现架构</h3>

<p><img src="assets/FrLOYwy8Lgh8Rkeue3Gawhco_F0C" alt="avatar"></p>

<p>如上图所示，K8s 服务发现以及 K8s Service 是这样整体的一个架构。</p>

<p>K8s 分为 master 节点和 worker 节点：</p>

<ul>

<li>master 里面主要是 K8s 管控的内容；</li>

<li>worker 节点里面是实际跑用户应用的一个地方。</li>

</ul>

<p>在 K8s master 节点里面有 APIServer，就是统一管理 K8s 所有对象的地方，所有的组件都会注册到 APIServer 上面去监听这个对象的变化，比如说我们刚才的组件 pod 生命周期发生变化，这些事件。</p>

<p>这里面最关键的有三个组件：</p>

<ul>

<li>一个是 Cloud Controller Manager，负责去配置 LoadBalancer 的一个负载均衡器给外部去访问；</li>

<li>另外一个就是 Coredns，就是通过 Coredns 去观测 APIServer 里面的 service 后端 pod 的一个变化，去配置 service 的 DNS 解析，实现可以通过 service 的名字直接访问到 service 的虚拟 IP，或者是 Headless 类型的 Service 中的 IP 列表的解析；</li>

<li>然后在每个 node 里面会有 kube-proxy 这个组件，它通过监听 service 以及 pod 变化，然后实际去配置集群里面的 node pod 或者是虚拟 IP 地址的一个访问。</li>

</ul>

<p>实际访问链路是什么样的呢？比如说从集群内部的一个 Client Pod3 去访问 Service，就类似于刚才所演示的一个效果。Client Pod3 首先通过 Coredns 这里去解析出 ServiceIP，Coredns 会返回给它 ServiceName 所对应的 service IP 是什么，这个 Client Pod3 就会拿这个 Service IP 去做请求，它的请求到宿主机的网络之后，就会被 kube-proxy 所配置的 iptables 或者 IPVS 去做一层拦截处理，之后去负载均衡到每一个实际的后端 pod 上面去，这样就实现了一个负载均衡以及服务发现。</p>

<p>对于外部的流量，比如说刚才通过公网访问的一个请求。它是通过外部的一个负载均衡器 Cloud Controller Manager 去监听 service 的变化之后，去配置的一个负载均衡器，然后转发到节点上的一个 NodePort 上面去，NodePort 也会经过 kube-proxy 的一个配置的一个 iptables，把 NodePort 的流量转换成 ClusterIP，紧接着转换成后端的一个 pod 的 IP 地址，去做负载均衡以及服务发现。这就是整个 K8s 服务发现以及 K8s Service 整体的结构。</p>

<h3>后续进阶</h3>

<p>后续再进阶部分我们还会更加深入地去讲解 K8s Service 的实现原理，以及在 service 网络出问题之后，如何去诊断以及去修复的技巧。</p>

<h2>本节总结</h2>

<p>本节课的主要内容就到此为止了，这里为大家简单总结一下：</p>

<ol>

<li>为什么云原生的场景需要服务发现和负载均衡，</li>

<li>在 Kubernetes 中如何使用 Kubernetes 的 Service 做服务发现和负载均衡</li>

<li>Kubernetes 集群中 Service 涉及到的组件和大概实现原理</li>

</ol>

<p>相信经过这一节的学习大家能够通过 Kubernetes Service 将复杂的企业级应用快速并标准的编排起来。</p>

</div>

                    </div><div class="book-post">

                        <p id="tip" align="center"></p>

                        <div><h1>15 从 0 开始创作云原生应用（殷达）</h1>

<h2>一、 云原生应用是什么？</h2>

<p>首先我们来思考一个问题：云原生应用是什么？</p>

<p><img src="assets/FlGLRC6CFQW7Xqx5WQmgFXFkPxmh" alt="avatar"></p>

<p>在生活中我们会和各种各样的应用打交道，有时候会在移动端上使用淘宝购物、使用高德导航，在 PC 端使用 word 编辑文稿、使用 Photoshop 处理相片······这些在各类平台上的应用程序对用户而言，大多数都只需要用户点击安装就可以使用了。那么对于云上的应用，或者说在我们今天的云上、在 Kubernetes 上的应用，是什么样子的呢？</p>

<p>想象一下，如果我们要把一个应用程序部署到云上都需要做什么呢？</p>

<p><img src="assets/FgbvI682-snP7lTv8Efj6TBvtRS2" alt="avatar"></p>

<p>首先我们要准备好它所需的环境，打包成一个 docker 镜像，把这个镜像放到 deployment 中。部署服务、应用所需要的账户、权限、匿名空间、秘钥信息，还有可持久化存储，这些 Kubernetes 的资源，简而言之就是要把一堆 yaml 配置文件布置在 Kubernetes 上。</p>

<p><img src="assets/FmTz-frFm2fS1jOGoyssX0z-hdI1" alt="avatar"></p>

<p>虽然应用的开发者可以把这些镜像存放在公共的仓库中，然后把部署所需要的 yaml 的资源文件提供给用户，当然用户仍然需要自己去寻找这些资源文件在哪里，并把它们一一部署起来。倘若用户希望修改开发者提供的默认资源，比如说想使用更多的副本数，或者是修改服务端口，那他还需要自己去查：在这些资源文件中，哪些地方需要相应的修改。同时版本更替和维护，也会给开发者和用户造成很大的麻烦，所以可以看到最原始的这种 Kubernetes 的应用形态并不是非常的便利。</p>

<h2>二、Helm 与 Helm Chart</h2>

<h3>Helm 是什么？</h3>

<p><img src="assets/Fh-OqTnu1PrHRpx5ger_CWtRyIXt" alt="avatar"></p>

<p>我们今天的主角：Helm，就在这样的环境下应用而生。开发者安装 Helm Chart 的格式，将应用所需要的资源文件都包装起来，通过模板化的方法，将一些可变的字段，比如说我们之前提到的要暴露哪一个端口、使用多少副本数量，把这些信息都暴露给用户，最后将封装好的应用包，也就是我们所说的 Helm Chart 集中存放在统一的仓库里面供用户浏览下载。</p>

<p>那么对于用户而言，使用 Helm 一条简单的命令就可以完成应用的安装、卸载和升级，我们可以在安装完成之后使用 kubectl 来查看一下应用安装后的 pod 的运行状态。需要注意的是，我们证明使用的是 Helm v3 的一个命令，与目前相对较为成熟的 Helm v2 是有一定的区别的。我们推荐大家在进行学习尝鲜的过程中使用这个最新的 v3 版本。</p>

<h3>如何去创作一个 Helm 应用</h3>

<p><img src="assets/Fr3tuEfSFfA8Y3CtOI7agIY1zX43" alt="avatar"></p>

<p>站在开发者的角度上，我们应该如何去创作一个 Helm 应用呢？首先我们需要一个准备部署的镜像，这个镜像可以是一个 JAVA 程序、一个 Python 脚本，甚至是一个空的 Linux 镜像，跑几条命令。</p>

<h4>编写 Golang 程序</h4>

<p><img src="assets/FiO1KiSU0QikZMSmUE_W5reBmdqC" alt="avatar"></p>

<p>如上图所示，这里我们是用 Golang 编写一个非常简单的 Hello World 的 http 服务，并且使用 docker 进行一个打包。Golang 的程序大致是长这个样子的，包括从环境变量中读取 pod、username 两个参数，在指定的端口上提取 http 服务，并返回相应的响应信息。</p>

<h4>构建 Docker 镜像</h4>

<p><img src="assets/FlHU3ldIkseL6MfY_cFb6BLlK9A2" alt="avatar"></p>

<p>打包用的 Dockerfile 是长这个样子的。在这里面，我们首先对 Golang 代码进行编译，然后将编译后的程序放到 03：53 的一个镜像中来缩小镜像的体积。我们看到上文所说的两个环境变量只有 port 在这里面进行一个设置，username 将会在后续作为应用的一个参数报告给用户，所以在这里面我们先不做设置，在 docker 构建好镜像之后，我们把这个镜像上传到仓库中，比如说我们可以上传到 Docker Helm，或者是阿里云镜像仓库中。</p>

<h4>创建空白应用</h4>

<p><img src="assets/FoLmRbN6XO1vVBOKvArpgJNh7trC" alt="avatar"></p>

<p>准备工作都做完之后，我们可以开始今天的重头戏，也就是构建这个 Helm Chart 了。首先我们先运行 helm create 的命令，创建一个空白的应用，那么在 create 命令运行完之后，可以看到在这个 Charts 的文件夹下出现了一系列文件和文件夹。</p>

<ul>

<li>Charts.yaml 的文件包含了 Helm Chart 的一些基本信息；</li>

<li>templates 文件夹内则是存放了这个应用所需要的各种 Kubernetes 的资源；</li>

<li>values.yaml 则是提供了一个默认的参数配置。</li>

</ul>

<h4>Chart 配置</h4>

<p><img src="assets/FtR1WQmfbfH6aTzD7oJ5A5v_GC0a" alt="avatar"></p>

<p>接下来一个一个看：</p>

<p>在根目录下这个 Charts.yaml 文件内声明了当前 Chart 的名称和版本和一些基本信息，那么这些信息会在 chart 被放入仓库之后，供用户浏览和检索，比如我们在这里面可以把 chart 的 description 改成 My first hello world helm chart。</p>

<p>在 Charts.yaml 里面，有两个和版本相关的字段，其中 version 指的是我们当前这个 chart 应用包的版本，而另外一个 appVersion 则指的是我们内部所使用的，比如说在这里面就是我们内部所使用的 Golang 这个程序，我们给它打一个 tag 这个版本。</p>

<h4>template 文件夹</h4>

<p><img src="assets/FtiTc_vfrsua9Bp_DAugf1XmlFhp" alt="avatar"></p>

<p>在 templates 这个文件夹下，则是存放各种部署应用所需要的 yaml 文件，比如说我们看到的 deployment 和 service。</p>

<p>我们当前的应用实际上只需要一个 deployment，但是有的应用可能包含多个组件，此时就需要在这个 templates 文件夹下放 deploymentA、deploymentB 等多个 yaml 文件。有时候我们还需要去配置一些 service account secret volume 的内容，也是在这里面去添加相应的内容。</p>

<p>在 templates 文件夹下，这个配置文件可以只是单纯的 Kubernetes.yaml 配置文件，也可以是配置文件的模板，比如说在这看到 deployment.yaml 文件里面，有很多以 {{ }} 包裹起来的变量，这些以 values 或者是 chart 开头的这些变量，都是从根目录下的 chart.yaml 以及 values.yaml 中获取出来的。</p>

<p><img src="assets/FoN7_vx878Ljhs5xukVQM3C8_Ear" alt="avatar"></p>

<p>如上图所示，看到 replicaCount 实际上就是我们所要部署的副本数量，而 repository 则是指定了镜像的位置。我们之前在 docker 镜像构建中并没有设置 username 的环境变量，这里也是通过类似的方式暴露在了 values.yaml 里面。</p>

<p>Helm 在安装应用的时候，实际上会先去渲染 templates 这个文件夹下的模板文件，将所需要的变量都填入进去，然后再使用渲染后的 kubernetes.yaml 文件进行一个部署，而我们在创建这个 Helm Chart 的过程中，实际上并不需要考虑太多如何去渲染，因为 Helm 已经在客户端安装应用的时候帮我们把这些事情都完成了。</p>

<h4>校验与打包</h4>

<p><img src="assets/FjyvOU9o7XaU46Mh5vhJYt_KoZ7v" alt="avatar"></p>

<p>在我们准备好这些应用后，就可以使用 helm lint 命令，来粗略检查一下我们制作的这个 chart 有没有语法上的错误，如果没有问题的话，就可以使用 Helm package 命令，对我们的 chart 文件包进行一个打包。打包之后，我们就可以得到一个 tar 的应用包了，这个就是我们所要发布的一个应用。</p>

<h4>安装测试</h4>

<p><img src="assets/Ftca-nUBwx_Fy1wvKF_eif_R9G23" alt="avatar"></p>

<p>我们可以使用 Helm install 这个命令来尝试安装一下刚刚做好的应用包，然后使用 kubectl 来查看一下 pod 的运行状态，同样可以通过 port-forward 命令来把这个 pod 的端口映射到本地的端口上，这样就可以通过本地的这个 localhost 来访问到刚刚部署好的这个应用了。</p>

<h4>参数覆盖</h4>

<p><img src="assets/Fj0CeyV8ku5wg16mzZwIW--vvqtv" alt="avatar"></p>

<p>有的同学可能会有疑惑：虽然我们应用开发者把这些可配置的信息都暴露在了 values.yanl 里面，用户使用应用的时候，如果想要修改该怎么办呢？这个答案其实也很简单，用户只需要在 install 的时候使用这个 set 参数设置，把想要设置的参数覆盖掉就行了。</p>

<p>同样，如果用户编写自己的 my-values.yaml 文件，也可以把这个文件在 install 的时候设置起来，这样的话，文件中的参数会覆盖掉原有的一些参数。如果用户不想重新去 install 一个新的 app，而是想要升级原来的 app，他也只需要用这个 helm upgrade 的命令把这个 Helm install 这个命令替换掉就可以了。</p>

<h4>修改 NOTES.txt</h4>

<p><img src="assets/FpUjnQVVQV8mzneN7bBeCLsMSRNH" alt="avatar"></p>

<p>细心的同学可能会注意到，之前在执行 Helm install 的命令后，这个提示信息其实是有一些问题的，我们看一下之前所写的 deployment.yaml 这个文件，里面可以看到，两个 label 其实是有一定出入的，这个提示信息其实就是在 templates 的 notes 文件下，所以我们只需要到这个文件里面去把这里面的相应信息修改一下就可以了。</p>

<h4>升级版本</h4>

<p><img src="assets/FsmUV512mtq1-LByVUuUdOEXKwyq" alt="avatar"></p>

<p>接下来我们回到 chart.yaml 的文件中，更新一下这个 version 字段，重新做一个打包，这样我们就可以对原来部署好的应用做这样一个版本升级了。</p>

<h4>应用上传</h4>

<p><img src="assets/FgEMgu2zmJm06MshWXVMz7mwAWaV" alt="avatar"></p>

<p>制作完成的这个应用应该如何和其他人做分享呢？</p>

<p>Helm 官方是通过了 CHARTMUSEUM 这样一个工具，用这个工具，大家可以来构建自己的 chart 仓库，但是自己维护一个 chart 成本会比较高，另外对于使用户而言，如果它使用每一个应用的开发者都有自己的一个仓库的话，那这个用户他就需要去把这些所有的仓库都加入到自己的检索列表里面，这个非常麻烦，非常不利于应用的传播和分享。</p>

<h2>三、开放云原生应用中心</h2>

<h3>应用来源</h3>

<p><img src="assets/FnazvWuaUJi6rkcIhqUNi9VS-ND4" alt="avatar"></p>

<p>我们团队最近推出了一个开放云原生应用中心：**Cloud Native App Hub。**在这里面，我们同步了各种非常流行的应用，同时还提供了一个开发者上传自己应用的一个渠道。</p>

<h3>提交应用</h3>

<p>在开放云原生应用中心，应用主要是来自两个渠道：</p>

<ul>

<li>一方面我们会定期从一些国外知名的 Helm 仓库同步 chart 资源，同时将其内部使用的一些 docker 镜像也一并做这样的替换。</li>

<li>另一方面，我们和 Helm 官方库一样，在 GitHub 上，也接受开发者通过 push request 的形式提交自己的应用。</li>

</ul>

<p><img src="assets/Fgg7QwtE8dAcRZkVKjLnYf9IOPmD" alt="avatar"></p>

<p>感兴趣的同学可以移步我们的云原生应用中心位于 GitHub 上的 chart 仓库，仿照刚才所讲的 chart 制作流程创作自己的 chart，然后提交 push request。</p>

<h2>结束语</h2>

<p>最后欢迎大家使用 Helm 和阿里云来进行云原生应用的开发。如果有问题或者希望深入交流讨论的同学，可以扫码加入我们的 Kubernetes 钉钉技术大群，和大牛们一起探索技术。今天的介绍就到这里，谢谢大家。</p>

</div>

                    </div><div class="book-post">

                        <p id="tip" align="center"></p>

                        <div><h1>16 深入解析 Linux 容器（华敏）</h1>

<p>前两个部分就是资源隔离和限制还有容器镜像的构成，第三部分会以一个业界比较成熟的容器引擎为例去讲解一下容器引擎的构成。</p>

<h2>容器</h2>

<p>容器是一种轻量级的虚拟化技术，因为它跟虚拟机比起来，它少了一层 hypervisor 层。先看一下下面这张图，这张图简单描述了一个容器的启动过程。</p>

<p><img src="assets/FpTs-4dpRPGlwQlH2RgaF3CqcE3-" alt="avatar"></p>

<p>最下面是一个磁盘，容器的镜像是存储在磁盘上面的。上层是一个容器引擎，容器引擎可以是 docker，也可以是其它的容器引擎。引擎向下发一个请求，比如说创建容器，然后这时候它就把磁盘上面的容器镜像，运行成在宿主机上的一个进程。</p>

<p>对于容器来说，最重要的是怎么保证这个进程所用到的资源是被隔离和被限制住的，在 Linux 内核上面是由 cgroup 和 namespace 这两个技术来保证的。接下来以 docker 为例，来详细介绍一下资源隔离和容器镜像这两部分内容。</p>

<h2>一、资源隔离和限制</h2>

<h3>namespace</h3>

<p>namespace 是用来做资源隔离的，在 Linux 内核上有七种 namespace，docker 中用到了前六种。第七种 cgroup namespace 在 docker 本身并没有用到，但是在 runC 实现中实现了 cgroup namespace。</p>

<p><img src="assets/Fp2wdVXaGZhGiPcay0kKTb_LP1_r" alt="avatar"></p>

<p>我们先从头看一下：</p>

<ul>

<li>第一个是 mout namespace。mout namespace 就是保证容器看到的文件系统的视图，是容器镜像提供的一个文件系统，也就是说它看不见宿主机上的其他文件，除了通过 -v 参数 bound 的那种模式，是可以把宿主机上面的一些目录和文件，让它在容器里面可见的。</li>

<li>第二个是 uts namespace，这个 namespace 主要是隔离了 hostname 和 domain。</li>

<li>第三个是 pid namespace，这个 namespace 是保证了容器的 init 进程是以 1 号进程来启动的。</li>

<li>第四个是网络 namespace，除了容器用 host 网络这种模式之外，其他所有的网络模式都有一个自己的 network namespace 的文件。</li>

<li>第五个是 user namespace，这个 namespace 是控制用户 UID 和 GID 在容器内部和宿主机上的一个映射，不过这个 namespace 用的比较少。</li>

<li>第六个是 IPC namespace，这个 namespace 是控制了进程兼通信的一些东西，比方说信号量。</li>

<li>第七个是 cgroup namespace，上图右边有两张示意图，分别是表示开启和关闭 cgroup namespace。用 cgroup namespace 带来的一个好处是容器中看到的 cgroup 视图是以根的形式来呈现的，这样的话就和宿主机上面进程看到的 cgroup namespace 的一个视图方式是相同的。另外一个好处是让容器内部使用 cgroup 会变得更安全。</li>

</ul>

<p>这里我们简单用 unshare 示例一下 namespace 创立的过程。容器中 namespace 的创建其实都是用 unshare 这个系统调用来创建的。</p>

<p><img src="assets/FvWvn8rrjHSdKn3o5jmpUWctABn4" alt="avatar"></p>

<p>上图上半部分是 unshare 使用的一个例子，下半部分是我实际用 unshare 这个命令去创建的一个 pid namespace。可以看到这个 bash 进程已经是在一个新的 pid namespace 里面，然后 ps 看到这个 bash 的 pid 现在是 1，说明它是一个新的 pid namespace。</p>

<h3>cgroup</h3>

<h4>两种 cgroup 驱动</h4>

<p>cgroup 主要是做资源限制的，docker 容器有两种 cgroup 驱动：一种是 systemd 的，另外一种是 cgroupfs 的。</p>

<p><img src="assets/FovwNSZ3NcNu2ns89eo4vZg-9N-J" alt="avatar"></p>

<ul>

<li><strong>cgroupfs</strong> 比较好理解。比如说要限制内存是多少，要用 CPU share 为多少，其实直接把 pid 写入对应的一个 cgroup 文件，然后把对应需要限制的资源也写入相应的 memory cgroup 文件和 CPU 的 cgroup 文件就可以了。</li>

<li>另外一个是 <strong>systemd</strong> 的一个 cgroup 驱动。这个驱动是因为 systemd 本身可以提供一个 cgroup 管理方式。所以如果用 systemd 做 cgroup 驱动的话，所有的写 cgroup 操作都必须通过 systemd 的接口来完成，不能手动更改 cgroup 的文件。</li>

</ul>

<h4>容器中常用的 cgroup</h4>

<p>接下来看一下容器中常用的 cgroup。Linux 内核本身是提供了很多种 cgroup，但是 docker 容器用到的大概只有下面六种：</p>

<p><img src="assets/Fk3LDSkW2qFq9lGSKOgu5VPUyW7h" alt="avatar"></p>

<ul>

<li>第一个是 CPU，CPU 一般会去设置 cpu share 和 cupset，控制 CPU 的使用率。</li>

<li>第二个是 memory，是控制进程内存的使用量。</li>

<li>第三个 device ，device 控制了你可以在容器中看到的 device 设备。</li>

<li>第四个 freezer。它和第三个 cgroup（device）都是为了安全的。当你停止容器的时候，freezer 会把当前的进程全部都写入 cgroup，然后把所有的进程都冻结掉，这样做的目的是，防止你在停止的时候，有进程会去做 fork。这样的话就相当于防止进程逃逸到宿主机上面去，是为安全考虑。</li>

<li>第五个是 blkio，blkio 主要是限制容器用到的磁盘的一些 IOPS 还有 bps 的速率限制。因为 cgroup 不唯一的话，blkio 只能限制同步 io，docker io 是没办法限制的。</li>

<li>第六个是 pid cgroup，pid cgroup 限制的是容器里面可以用到的最大进程数量。</li>

</ul>

<h4>不常用的 cgroup</h4>

<p>也有一部分是 docker 容器没有用到的 cgroup。容器中常用的和不常用的，这个区别是对 docker 来说的，因为对于 runC 来说，除了最下面的 rdma，所有的 cgroup 其实都是在 runC 里面支持的，但是 docker 并没有开启这部分支持，所以说 docker 容器是不支持下图这些 cgroup 的。</p>

<p><img src="assets/FpGy3tVphWtAeloTZRRc6C2Ck6f1" alt="avatar"></p>

<h2>二、容器镜像</h2>

<h3>docker images</h3>

<p>接下来我们讲一下容器镜像，以 docker 镜像为例去讲一下容器镜像的构成。</p>

<p>docker 镜像是基于联合文件系统的。简单描述一下联合文件系统：大概的意思就是说，它允许文件是存放在不同的层级上面的，但是最终是可以通过一个统一的视图，看到这些层级上面的所有文件。</p>

<p><img src="assets/Fk_vifMYZw4dqK1e8GQJRqhtv8o5" alt="avatar"></p>

<p>如上图所示，右边是从 docker 官网拿过来的容器存储的一个结构图。这张图非常形象的表明了 docker 的存储，docker 存储也就是基于联合文件系统，是分层的。每一层是一个 Layer，这些 Layer 由不同的文件组成，它是可以被其他镜像所复用的。可以看一下，当镜像被运行成一个容器的时候，最上层就会是一个容器的读写层。这个容器的读写层也可以通过 commit 把它变成一个镜像顶层最新的一层。</p>

<p>docker 镜像的存储，它的底层是基于不同的文件系统的，所以它的存储驱动也是针对不同的文件系统作为定制的，比如 AUFS、btrfs、devicemapper 还有 overlay。docker 对这些文件系统做了一些相对应的一个 graph driver 的驱动，也就是通过这些驱动把镜像存在磁盘上面。</p>

<h3>以 overlay 为例</h3>

<h4>存储流程</h4>

<p>接下来我们以 overlay 这个文件系统为例，看一下 docker 镜像是怎么在磁盘上进行存储的。先看一下下面这张图，简单地描述了 overlay 文件系统的工作原理 。</p>

<p><img src="assets/Fki0alOPLdln3jhwzrb3z11OLBlB" alt="avatar"></p>

<p>最下层是一个 lower 层，也就是镜像层，它是一个只读层。右上层是一个 upper 层，upper 是容器的读写层，upper 层采用了写实复制的机制，也就是说只有对某些文件需要进行修改的时候才会从 lower 层把这个文件拷贝上来，之后所有的修改操作都会对 upper 层的副本进行修改。</p>

<p>upper 并列的有一个 workdir，它的作用是充当一个中间层的作用。也就是说，当对 upper 层里面的副本进行修改时，会先放到 workdir，然后再从 workdir 移到 upper 里面去，这个是 overlay 的工作机制。</p>

<p>最上面的是 mergedir，是一个统一视图层。从 mergedir 里面可以看到 upper 和 lower 中所有数据的整合，然后我们 docker exec 到容器里面，看到一个文件系统其实就是 mergedir 统一视图层。</p>

<h4>文件操作</h4>

<p>接下来我们讲一下基于 overlay 这种存储，怎么对容器里面的文件进行操作？</p>

<p><img src="assets/Fm191Rh6NVM8l53r55ppgSzGf9cp" alt="avatar"></p>

<p>先看一下读操作，容器刚创建出来的时候，upper 其实是空的。这个时候如果去读的话，所有数据都是从 lower 层读来的。</p>

<p>写操作如刚才所提到的，overlay 的 upper 层有一个写实数据的机制，对一些文件需要进行操作的时候，overlay 会去做一个 copy up 的动作，然后会把文件从 lower 层拷贝上来，之后的一些写修改都会对这个部分进行操作。</p>

<p>然后看一下删除操作，overlay 里面其实是没有真正的删除操作的。它所谓的删除其实是通过对文件进行标记，然后从最上层的统一视图层去看，看到这个文件如果做标记，就会让这个文件显示出来，然后就认为这个文件是被删掉的。这个标记有两种方式：</p>

<ul>

<li>一种是 whiteout 的方式。</li>

<li>第二个就是通过设置目录的一个扩展权限，通过设置扩展参数来做到目录的删除。</li>

</ul>

<h3>操作步骤</h3>

<p>接下来看一下实际用 docker run 去启动 busybox 的容器，它的 overlay 的挂载点是什么样子的？</p>

<p><img src="assets/FjJ916iAyCuUgxWVhsjwdZ93iisS" alt="avatar"></p>

<p>第二张图是 mount，可以看到这个容器 rootfs 的一个挂载，它是一个 overlay 的 type 作为挂载的。里面包括了 upper、lower 还有 workdir 这三个层级。</p>

<p>接下来看一下容器里面新文件的写入。docker exec 去创建一个新文件，diff 这个从上面可以看到，是它的一个 upperdir。再看 upperdir 里面有这个文件，文件里面的内容也是 docker exec 写入的。</p>

<p>最后看一下最下面的是 mergedir，mergedir 里面整合的 upperdir 和 lowerdir 的内容，也可以看到我们写入的数据。</p>

<h2>三、容器引擎</h2>

<h3>containerd 容器架构详解</h3>

<p>接下来讲一下容器引擎，我们基于 CNCF 的一个容器引擎上的 containerd，来讲一下容器引擎大致的构成。下图是从 containerd 官网拿过来的一张架构图，基于这张架构图先简单介绍一下 containerd 的架构。</p>

<p><img src="assets/FtDzADL7AJek1oHX4og4pM2Bw-eZ" alt="avatar"></p>

<p>上图如果把它分成左右两边的话，可以认为 containerd 提供了两大功能。</p>

<p>第一个是对于 runtime，也就是对于容器生命周期的管理，左边 storage 的部分其实是对一个镜像存储的管理。containerd 会负责进行的拉取、镜像的存储。</p>

<p>按照水平层次来看的话:</p>

<ul>

<li>第一层是 GRPC，containerd 对于上层来说是通过 GRPC serve 的形式来对上层提供服务的。Metrics 这个部分主要是提供 cgroup Metrics 的一些内容。</li>

<li>下面这层的左边是容器镜像的一个存储，中线 images、containers 下面是 Metadata，这部分 Matadata 是通过 **bootfs **存储在磁盘上面的。右边的 Tasks 是管理容器的容器结构，Events 是对容器的一些操作都会有一个 Event 向上层发出，然后上层可以去订阅这个 Event，由此知道容器状态发生什么变化。</li>

<li>最下层是 Runtimes 层，这个 Runtimes 可以从类型区分，比如说 runC 或者是安全容器之类的。</li>

</ul>

<h3>shim v1/v2 是什么</h3>

<p>接下来讲一下 containerd 在 runtime 这边的大致架构。下面这张图是从 kata 官网拿过来的，上半部分是原图，下半部分加了一些扩展示例，基于这张图我们来看一下 containerd 在 runtime 这层的架构。</p>

<p><img src="assets/Fp7sISzoH2qWcoVz3JqqoZ4bAMmx" alt="avatar"></p>

<p>如图所示：按照从左往右的一个顺序，从上层到最终 runtime 运行起来的一个流程。</p>

<p>我们先看一下最左边，最左边是一个 CRI Client。一般就是 kubelet 通过 CRI 请求，向 containerd 发送请求。containerd 接收到容器的请求之后，会经过一个 containerd shim。containerd shim 是管理容器生命周期的，它主要负责两方面：</p>

<ul>

<li>第一个是它会对 io 进行转发。</li>

<li>第二是它会对信号进行传递。</li>

</ul>

<p>图的上半部分画的是安全容器，也就是 kata 的一个流程，这个就不具体展开了。下半部分，可以看到有各种各样不同的 shim。下面介绍一下 containerd shim 的架构。</p>

<p>一开始在 containerd 中只有一个 shim，也就是蓝色框框起来的 containerd-shim。这个进程的意思是，不管是 kata 容器也好、runc 容器也好、gvisor 容器也好，上面用的 shim 都是 containerd。</p>

<p>后面针对不同类型的 runtime，containerd 去做了一个扩展。这个扩展是通过 shim-v2 这个 interface 去做的，也就是说只要去实现了这个 shim-v2 的 interface，不同的 runtime 就可以定制不同的自己的一个 shim。比如：runC 可以自己做一个 shim，叫 shim-runc；gvisor 可以自己做一个 shim 叫 shim-gvisor；像上面 kata 也可以自己去做一个 shim-kata 的 shim。这些 shim 可以替换掉上面蓝色框的 containerd-shim。</p>

<p>这样做的好处有很多，举一个比较形象的例子。可以看一下 kata 这张图，它上面原先如果用 shim-v1 的话其实有三个组件，之所以有三个组件的原因是因为 kata 自身的一个限制，但是用了 shim-v2 这个架构后，三个组件可以做成一个二进制，也就是原先三个组件，现在可以变成一个 shim-kata 组件，这个可以体现出 shim-v2 的一个好处。</p>

<h3>containerd 容器架构详解 - 容器流程示例</h3>

<p>接下来我们以两个示例来详细解释一下容器的流程是怎么工作的，下面的两张图是基于 containerd 的架构画的一个容器的工作流程。</p>

<h4>start 流程</h4>

<p>先看一下容器 start 的流程：</p>

<p><img src="assets/FlA6h5Qqhwx_V7SjzpFSu2zSGs2W" alt="avatar"></p>

<p>这张图由三个部分组成：</p>

<ul>

<li>第一个部分是容器引擎部分，容器引擎可以是 docker，也可以是其它的。</li>

<li>两个虚线框框起来的 containerd 和 containerd-shim，它们两个是属于 containerd 架构的部分。</li>

<li>最下面就是 container 的部分，这个部分是通过一个 runtime 去拉起的，可以认为是 shim 去操作 runC 命令创建的一个容器。</li>

</ul>

<p>先看一下这个流程是怎么工作的，图里面也标明了 1、2、3、4。这个 1、2、3、4 就是 containerd 怎么去创建一个容器的流程。</p>

<p>首先它会去创建一个 matadata，然后会去发请求给 task service 说要去创建容器。通过中间一系列的组件，最终把请求下发到一个 shim。containerd 和 shim 的交互其实也是通过 GRPC 来做交互的，containerd 把创建请求发给 shim 之后，shim 会去调用 runtime 创建一个容器出来，以上就是容器 start 的一个示例。</p>

<h4>exec 流程</h4>

<p>接下来看下面这张图，是怎么去 exec 一个容器的。和 start 流程非常相似，结构也大概相同，不同的部分其实就是 containerd 怎么去处理这部分流程。和上面的图一样，我也在图中标明了 1、2、3、4，这些步骤就代表了 containerd 去做 exec 的一个先后顺序。</p>

<p><img src="assets/FnZ76tbS2WpIQuGmyO_6i3DJ1BwN" alt="avatar"></p>

<p>由上图可以看到，exec 的操作还是发给 containerd-shim 的。对容器来说，去 start 一个容器和去 exec 一个容器，其实并没有本质的区别。</p>

<p>最终的一个区别无非就是，是否对容器中跑的进程做一个 namespace 的创建：</p>

<ul>

<li>exec 的时候，需要把这个进程加入到一个已有的 namespace 里面；</li>

<li>start 的时候，容器进程的 namespace 是需要去专门创建。</li>

</ul>

<h2>本节总结</h2>

<p>最后希望各位同学看完本节后，能够对 Linux 容器有更深刻的了解。这里为大家简单总结一下：</p>

<ol>

<li>容器如何用 namespace 做资源隔离以及 cgroup 做资源限制；</li>

<li>简单介绍了基于 overlay 文件系统的容器镜像存储；</li>

<li>以 docker+containerd 为例介绍了容器引擎如何工作的。</li>

</ol>

</div>

                    </div>

                </div>

            </div>
        </div>
    </div>
</div>

<a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>
<script defer src="https://static.cloudflareinsights.com/beacon.min.js/v652eace1692a40cfa3763df669d7439c1639079717194" integrity="sha512-Gi7xpJR8tSkrpF7aordPZQlW2DLtzUlZcumS8dMQjwDHEnw9I7ZLyiOj/6tZStRBGtGgN6ceN6cMH8z7etPGlw==" data-cf-beacon='{"rayId":"709ba3a57eaefbdc","version":"2021.12.0","r":1,"token":"1f5d475227ce4f0089a7cff1ab17c0f5","si":100}' crossorigin="anonymous"></script>
</body>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NPSEEVD756"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }

    gtag('js', new Date());
    gtag('config', 'G-NPSEEVD756');
    var path = window.location.pathname
    var cookie = getCookie("lastPath");
    console.log(path)
    if (path.replace("/", "") === "") {
        if (cookie.replace("/", "") !== "") {
            console.log(cookie)
            document.getElementById("tip").innerHTML = "<a href='" + cookie + "'>跳转到上次进度</a>"
        }
    } else {
        setCookie("lastPath", path)
    }

    function setCookie(cname, cvalue) {
        var d = new Date();
        d.setTime(d.getTime() + (180 * 24 * 60 * 60 * 1000));
        var expires = "expires=" + d.toGMTString();
        document.cookie = cname + "=" + cvalue + "; " + expires + ";path = /";
    }

    function getCookie(cname) {
        var name = cname + "=";
        var ca = document.cookie.split(';');
        for (var i = 0; i < ca.length; i++) {
            var c = ca[i].trim();
            if (c.indexOf(name) === 0) return c.substring(name.length, c.length);
        }
        return "";
    }

</script>

</html>
